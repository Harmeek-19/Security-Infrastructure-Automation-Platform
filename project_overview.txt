"""
Django settings for security_automation project.

Generated by 'django-admin startproject' using Django 4.2.11.

For more information on this file, see
https://docs.djangoproject.com/en/4.2/topics/settings/

For the full list of settings and their values, see
https://docs.djangoproject.com/en/4.2/ref/settings/
"""

from pathlib import Path

# Build paths inside the project like this: BASE_DIR / 'subdir'.
BASE_DIR = Path(__file__).resolve().parent.parent


# Quick-start development settings - unsuitable for production
# See https://docs.djangoproject.com/en/4.2/howto/deployment/checklist/

# SECURITY WARNING: keep the secret key used in production secret!
SECRET_KEY = 'django-insecure-7u1-7x*e*z3&3d#1f1ktuo#zuu6f($58^=yuz=b4n7(n)6w4kl'

# SECURITY WARNING: don't run with debug turned on in production!
DEBUG = True

ALLOWED_HOSTS = ['localhost', '127.0.0.1']


# Application definition

INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'network_visualization',
    'automation',
    'reconnaissance.apps.ReconnaissanceConfig',
    'vulnerability.apps.VulnerabilityConfig',
    'reporting.apps.ReportingConfig',
    
]

# Update DATABASES in settings.py
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': BASE_DIR / 'db.sqlite3',
        'TIMEOUT': 20,  # Seconds
        'OPTIONS': {
            'timeout': 20,
            'check_same_thread': False,
        }
    }
}

MIDDLEWARE = [
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
    'automation.middleware.AutomationProcessorMiddleware'

]
# OpenVAS Configuration
# In security_automation/settings.py
# Update the OpenVAS configuration section





# Security Settings
SECURE_BROWSER_XSS_FILTER = True
SECURE_CONTENT_TYPE_NOSNIFF = True
X_FRAME_OPTIONS = 'DENY'
SECURE_HSTS_SECONDS = 31536000
SECURE_HSTS_INCLUDE_SUBDOMAINS = True
SECURE_HSTS_PRELOAD = True

# Session Security
SESSION_COOKIE_SECURE = True
CSRF_COOKIE_SECURE = True
SESSION_COOKIE_HTTPONLY = True
CSRF_COOKIE_HTTPONLY = True

# Rate Limiting
RATELIMIT_ENABLE = True
RATELIMIT_USE_CACHE = 'default'
RATELIMIT_DEFAULT_RATES = ['100/h']  # Default rate limit

# Logging Configuration
# Add this to your settings.py



# Logging Configuration

# Logging Configuration
import os
from pathlib import Path

# Build paths inside the project
BASE_DIR = Path(__file__).resolve().parent.parent

# Create logs directory
LOGS_DIR = os.path.join(BASE_DIR, 'logs')
os.makedirs(LOGS_DIR, exist_ok=True)

# ZAP Scanner Configuration
# These settings control the behavior of the OWASP ZAP integration
ZAP_SETTINGS = {
    'API_KEY': 'change_me_please',  # Change this in production
    'HOST': 'localhost',
    'PORT': 8080,
    'TIMEOUT': 300,  # 5 minutes timeout for ZAP operations
    'DEBUG': DEBUG,  # Tie ZAP debug mode to Django's debug setting
    'MAX_RETRIES': 3,  # Maximum number of connection retry attempts
    'RETRY_DELAY': 5,  # Seconds to wait between retries
}

# Logging Configuration
# This defines how the application handles different types of logs
LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'verbose': {
            'format': '[{asctime}] {levelname} {module} {process:d} {thread:d} {message}',
            'style': '{',
            'datefmt': '%Y-%m-%d %H:%M:%S'
        },
        'simple': {
            'format': '[{asctime}] {levelname} {message}',
            'style': '{',
            'datefmt': '%Y-%m-%d %H:%M:%S'
        },
    },
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',
            'formatter': 'simple',
            'level': 'DEBUG' if DEBUG else 'INFO',
        },
        'file': {
            'class': 'logging.FileHandler',
            'filename': os.path.join(LOGS_DIR, 'debug.log'),
            'formatter': 'verbose',
            'level': 'DEBUG',
        },
        'service_file': {
            'class': 'logging.FileHandler',
            'filename': os.path.join(LOGS_DIR, 'services.log'),
            'formatter': 'verbose',
            'level': 'INFO',
        },
        'error_file': {
            'class': 'logging.FileHandler',
            'filename': os.path.join(LOGS_DIR, 'error.log'),
            'formatter': 'verbose',
            'level': 'ERROR',
        },
        'security_file': {
            'class': 'logging.FileHandler',
            'filename': os.path.join(LOGS_DIR, 'security.log'),
            'formatter': 'verbose',
            'level': 'INFO',
        }
    },
    'loggers': {
        'django': {
            'handlers': ['console', 'file'],
            'level': 'INFO',
            'propagate': True,
        },
        'reconnaissance': {
            'handlers': ['console', 'service_file', 'error_file'],
            'level': 'INFO',
            'propagate': True,
        },
        'vulnerability': {
            'handlers': ['console', 'service_file', 'error_file', 'security_file'],
            'level': 'INFO',
            'propagate': True,
        },
        'zap': {
            'handlers': ['console', 'service_file', 'error_file', 'security_file'],
            'level': 'DEBUG' if DEBUG else 'INFO',
            'propagate': True,
        },
        'scanner': {
            'handlers': ['console', 'service_file', 'error_file', 'security_file'],
            'level': 'DEBUG' if DEBUG else 'INFO',
            'propagate': True,
        }
    },
}

ROOT_URLCONF = 'security_automation.urls'

import os

# Build paths inside the project
BASE_DIR = Path(__file__).resolve().parent.parent

TEMPLATES = [
    {
        'BACKEND': 'django.template.backends.django.DjangoTemplates',
        'DIRS': [os.path.join(BASE_DIR, 'templates')],  # Add this line
        'APP_DIRS': True,
        'OPTIONS': {
            'context_processors': [
                'django.template.context_processors.debug',
                'django.template.context_processors.request',
                'django.contrib.auth.context_processors.auth',
                'django.contrib.messages.context_processors.messages',
            ],
        },
    },
]

WSGI_APPLICATION = 'security_automation.wsgi.application'


# Database
# https://docs.djangoproject.com/en/4.2/ref/settings/#databases



# Password validation
# https://docs.djangoproject.com/en/4.2/ref/settings/#auth-password-validators

AUTH_PASSWORD_VALIDATORS = [
    {
        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',
    },
]


# Internationalization
# https://docs.djangoproject.com/en/4.2/topics/i18n/

LANGUAGE_CODE = 'en-us'

TIME_ZONE = 'UTC'

USE_I18N = True

USE_TZ = True

SECRET_KEY = 'p6IOStJ0i0azQfySy0mSCIht7VouYg9RGggv6iDgI4MQUSgADoTcy7SG2Z4kjDboM6Q'  # Remember to use environment variables in production
DEBUG = True 

# Static files (CSS, JavaScript, Images)
# https://docs.djangoproject.com/en/4.2/howto/static-files/

STATIC_URL = 'static/'

# Default primary key field type
# https://docs.djangoproject.com/en/4.2/ref/settings/#default-auto-field

DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'


# ZAP Settings
ZAP_SETTINGS = {
    'API_KEY': 'change_me_please',
    'HOST': 'localhost',
    'PORT': 8080,
    'TIMEOUT': 300,
    'MAX_RETRIES': 3,
    'SPIDER_TIMEOUT': 600,
    'ACTIVE_SCAN_TIMEOUT': 1200
}
# Nuclei Settings
NUCLEI_SETTINGS = {
    'TEMPLATES_DIR': 'nuclei-templates',
    'RESULTS_DIR': 'nuclei-results',
    'DEFAULT_SEVERITY': 'critical,high,medium',
    'RATE_LIMIT': 150,
    'TIMEOUT': 300,
    'GO_PATH': '/usr/local/go/bin',  # Adjust this based on your Go installation
    'BINARY_PATH': str(Path.home() / "go" / "bin" / "nuclei"),  # Default Go installation path
}

# Email Settings
# Email Settings
EMAIL_BACKEND = 'django.core.mail.backends.smtp.EmailBackend'
EMAIL_HOST = 'smtp.gmail.com'  # Changed from example.com to Gmail's SMTP server
EMAIL_PORT = 587
EMAIL_USE_TLS = True
EMAIL_HOST_USER = 'harmeeksingh729@gmail.com' 
EMAIL_HOST_PASSWORD = 'lyel vzou qbuh fvcl'  # App password or regular password
DEFAULT_FROM_EMAIL = 'harmeeksingh729@gmail.com'  # Should match EMAIL_HOST_USER

AUTOMATION_AUTOSTART = True  # Automatically start the processor on application startup
AUTOMATION_PROCESSING_INTERVAL = 60  # Seconds between processing cycles# Updated portion of security_automation/urls.py

from django.contrib import admin
from django.urls import path, include

urlpatterns = [
    path('admin/', admin.site.urls),
    path('recon/', include('reconnaissance.urls')),
    path('vulnerability/', include('vulnerability.urls')),
    path('reporting/', include('reporting.urls')),
    path('network/', include('network_visualization.urls')),
    path('automation/', include('automation.urls')),  # Add this line
]asgiref==3.8.1
bcrypt==4.2.1
Brotli==1.1.0
certifi==2025.1.31
cffi==1.17.1
charset-normalizer==3.4.1
croniter==6.0.0
cryptography==44.0.1
cssselect2==0.8.0
decorator==5.1.1
defusedxml==0.7.1
Django==5.1.6
django-ratelimit==4.1.0
djangorestframework==3.15.2
djangorestframework_simplejwt==5.4.0
dnspython==2.7.0
docker==7.1.0
fonttools==4.56.0
greenlet==3.1.1
gvm-tools==25.2.0
idna==3.10
Jinja2==3.1.6
lxml==4.9.4
MarkupSafe==3.0.2
mysql-connector-python==9.2.0
paramiko==2.12.0
pdfkit==1.0.0
pillow==11.1.0
py==1.11.0
pycparser==2.22
pydyf==0.11.0
PyJWT==2.10.1
PyNaCl==1.5.0
pyphen==0.17.2
python-dateutil==2.9.0.post0
python-gvm==23.4.2
python-nmap==0.7.1
python-owasp-zap-v2.4==0.0.20
python3-nmap==1.9.1
pytz==2025.1
PyYAML==6.0.2
requests==2.32.3
responses==0.25.6
retry==0.9.2
scapy==2.6.1
simplejson==3.19.3
six==1.17.0
SQLAlchemy==2.0.38
sqlparse==0.5.3
tinycss2==1.4.0
tinyhtml5==2.0.0
typing_extensions==4.12.2
urllib3==2.3.0
weasyprint==64.1
webencodings==0.5.1
zopfli==0.2.3.post1from django.db import models

from django.db import models

class Subdomain(models.Model):
    domain = models.CharField(max_length=255)
    subdomain = models.CharField(max_length=255)
    ip_address = models.GenericIPAddressField(null=True)
    discovered_date = models.DateTimeField(auto_now_add=True)
    is_active = models.BooleanField(default=True)

    class Meta:
        ordering = ['-discovered_date']
        unique_together = ['domain', 'subdomain']
        indexes = [
            models.Index(fields=['domain']),
            models.Index(fields=['subdomain']),
        ]

    def __str__(self):
        return self.subdomain

    def save(self, *args, **kwargs):
        # Update instead of error on duplicate
        try:
            super().save(*args, **kwargs)
        except:
            existing = Subdomain.objects.get(domain=self.domain, subdomain=self.subdomain)
            existing.ip_address = self.ip_address
            existing.is_active = self.is_active
            existing.save()

class Service(models.Model):
    RISK_LEVELS = [
        ('LOW', 'Low'),
        ('MEDIUM', 'Medium'),
        ('HIGH', 'High'),
    ]

    CATEGORIES = [
        ('web', 'Web Services'),
        ('database', 'Database Services'),
        ('mail', 'Mail Services'),
        ('file_transfer', 'File Transfer'),
        ('remote_access', 'Remote Access'),
        ('domain_services', 'Domain Services'),
        ('monitoring', 'Monitoring'),
        ('security', 'Security Services'),
        ('other', 'Other'),
    ]

    PROTOCOLS = [
        ('tcp', 'TCP'),
        ('udp', 'UDP'),
        ('sctp', 'SCTP'),
    ]

    host = models.CharField(max_length=255)
    port = models.IntegerField()
    protocol = models.CharField(max_length=10, choices=PROTOCOLS, default='tcp')
    name = models.CharField(max_length=100)
    product = models.CharField(max_length=100, blank=True)
    version = models.CharField(max_length=100, blank=True)
    extra_info = models.TextField(blank=True)
    category = models.CharField(max_length=50, choices=CATEGORIES)
    risk_level = models.CharField(max_length=10, choices=RISK_LEVELS)
    cpe = models.JSONField(default=list)
    scan_date = models.DateTimeField(auto_now_add=True)
    last_seen = models.DateTimeField(auto_now=True)
    is_active = models.BooleanField(default=True)

    class Meta:
        unique_together = ('host', 'port', 'protocol')
        ordering = ['-scan_date']
        indexes = [
            models.Index(fields=['host', 'port']),
            models.Index(fields=['category']),
            models.Index(fields=['risk_level']),
        ]

    def __str__(self):
        return f"{self.host}:{self.port} - {self.name}"

class PortScan(models.Model):
    STATES = [
        ('pending', 'Pending'),
        ('in_progress', 'In Progress'), 
        ('open', 'Open'),
        ('closed', 'Closed'),
        ('filtered', 'Filtered'),
        ('unfiltered', 'Unfiltered'),
        ('error', 'Error'),
        ('completed', 'Completed')
    ]

    SCAN_STATUS = [
        ('pending', 'Pending'),
        ('in_progress', 'In Progress'),
        ('completed', 'Completed'),
        ('error', 'Error')
    ]

    host = models.CharField(max_length=255)
    port = models.IntegerField()
    service = models.CharField(max_length=100)
    state = models.CharField(max_length=50, choices=STATES)
    scan_status = models.CharField(max_length=50, choices=SCAN_STATUS, default='pending')
    scan_date = models.DateTimeField(auto_now_add=True)
    protocol = models.CharField(max_length=10, choices=Service.PROTOCOLS, default='tcp')
    banner = models.TextField(blank=True)
    notes = models.TextField(blank=True)
    scan_type = models.CharField(max_length=50, default='quick')
    error_message = models.TextField(blank=True)

    class Meta:
        ordering = ['-scan_date']
        indexes = [
            models.Index(fields=['host', 'port']),
            models.Index(fields=['state']),
            models.Index(fields=['scan_date']),
            models.Index(fields=['scan_status'])
        ]

    def __str__(self):
        return f"{self.host}:{self.port} - {self.state}"
class SystemLogEntry(models.Model):
    """Model for system logs admin interface"""
    class Meta:
        managed = False
        verbose_name_plural = 'System Logs'
        default_permissions = ('view',)import nmap
from typing import Dict, Any, List
from enum import Enum
import os
import logging
import time
import threading
import socket

logger = logging.getLogger(__name__)

class ScanType(Enum):
    QUICK = "quick"       # Fast scan of most common ports
    PARTIAL = "partial"   # Standard scan with version detection
    COMPLETE = "complete" # Comprehensive scan of all ports with version detection
    FULL = "full"        # Intensive scan with all possible features

class PortScanner:
    def __init__(self):
        self.scanner = nmap.PortScanner()
        # Check if running as root
        self.is_root = os.geteuid() == 0 if hasattr(os, 'geteuid') else False
        # Set default timeout
        self.default_timeout = 300  # 5 minutes default
        
    def get_scan_config(self, scan_type: str) -> Dict[str, str]:
        # Base configurations without OS detection
        base_configs = {
            ScanType.QUICK.value: {
                'ports': '21-23,25,80,443,3306,8080',
                'arguments': '-sV -T4 --version-intensity 0',  # Fast scan
                'timeout': 120  # 2 minutes timeout
            },
            ScanType.PARTIAL.value: {
                'ports': '1-1000',
                'arguments': '-sV -T4 -sC --version-intensity 5',  # Standard scan
                'timeout': 300  # 5 minutes timeout
            },
            ScanType.COMPLETE.value: {
                'ports': '1-65535',
                'arguments': '-sV -T4 -sC --version-intensity 7',  # All ports
                'timeout': 600  # 10 minutes timeout
            },
            ScanType.FULL.value: {
                'ports': '1-10000',  # Reduced port range for full scan
                'arguments': '-sV -T4 --version-intensity 9',  # Simplified for full scan
                'timeout': 900  # 15 minutes timeout
            }
        }
        
        # Add OS detection flags only if running as root
        if self.is_root:
            base_configs[ScanType.COMPLETE.value]['arguments'] += ' -O'
            base_configs[ScanType.FULL.value]['arguments'] += ' -O -A'
        
        config = base_configs.get(scan_type, base_configs[ScanType.QUICK.value])
        logger.info(f"Scan configuration for {scan_type}: {config}")
        return config

    def scan(self, target: str, scan_type: str = "quick") -> Dict[str, Any]:
        try:
            config = self.get_scan_config(scan_type)
            logger.info(f"Starting {scan_type} scan for {target} with args: {config['arguments']}")
            
            # Always do a manual port check first - this is more reliable
            responsive_ports = self._check_responsive_ports(target)
            
            # If we found ports in our manual check, create a manual result immediately
            if responsive_ports:
                logger.info(f"Manual check found open ports on {target}: {responsive_ports}")
                manual_result = self._create_manual_result(target, responsive_ports)
                
                # Only try nmap if it's not a full scan (which seems problematic)
                if scan_type != 'full':
                    try:
                        # Still run nmap for better service detection
                        scan_result = self.scanner.scan(target, config['ports'], config['arguments'])
                        
                        # Check if nmap found any open ports
                        nmap_ports = self._extract_open_ports_from_nmap(scan_result)
                        
                        if nmap_ports:
                            logger.info(f"Nmap found {len(nmap_ports)} open ports")
                            # Process the nmap results normally
                            return self._process_nmap_results(scan_result, scan_type)
                    except Exception as e:
                        logger.warning(f"Nmap scan failed: {str(e)}, using manual results")
                
                # Return our manual results
                return manual_result
            
            # If no ports were found by manual check, still try nmap
            try:
                scan_result = self.scanner.scan(target, config['ports'], config['arguments'])
                # Process the nmap results
                return self._process_nmap_results(scan_result, scan_type)
            except Exception as e:
                logger.error(f"Nmap scan error: {str(e)}")
                return {
                    'status': 'error',
                    'error': str(e)
                }
            
        except Exception as e:
            logger.error(f"Scan error: {str(e)}")
            return {
                'status': 'error',
                'error': str(e)
            }
    
# File: reconnaissance/scanner.py
# Modify the _process_nmap_results method to ensure consistent state reporting

    def _process_nmap_results(self, scan_result, scan_type):
        """Process nmap scan results into our standard format"""
        scan_results = []
        scan_info = {
            'scan_type': scan_type,
            'command_line': self.scanner.command_line(),
            'scan_time': self.scanner.scanstats().get('elapsed', ''),
            'total_hosts': len(self.scanner.all_hosts())
        }
        
        for host in self.scanner.all_hosts():
            host_data = {
                'host': host,
                'state': self.scanner[host].state(),
                'ports': []
            }
            
            # Flag to check if we found any open ports
            found_open_ports = False
            
            for proto in self.scanner[host].all_protocols():
                ports = self.scanner[host][proto].keys()
                for port in ports:
                    port_info = self.scanner[host][proto][port]
                    # Normalize state to 'open', 'closed', or 'filtered'
                    port_state = port_info['state']
                    
                    if port_state == 'open':
                        found_open_ports = True
                        
                    port_data = {
                        'port': port,
                        'state': port_state,
                        'service': port_info.get('name', ''),
                        'version': port_info.get('version', ''),
                        'product': port_info.get('product', ''),
                        'extrainfo': port_info.get('extrainfo', ''),
                        'reason': port_info.get('reason', ''),
                        'cpe': port_info.get('cpe', '')
                    }
                    host_data['ports'].append(port_data)
            
            # Add a flag to indicate if any open ports were found
            host_data['has_open_ports'] = found_open_ports
            
            # Only try to include OS data if it exists
            if 'osmatch' in self.scanner[host]:
                host_data['os_matches'] = self.scanner[host]['osmatch']
            
            scan_results.append(host_data)
        
        return {
            'status': 'success',
            'scan_info': scan_info,
            'results': scan_results,
            'open_ports_found': any(host.get('has_open_ports', False) for host in scan_results)
        }
    
    def _extract_open_ports_from_nmap(self, scan_result) -> List[int]:
        """Extract list of open ports from nmap scan result"""
        open_ports = []
        for host in self.scanner.all_hosts():
            for proto in self.scanner[host].all_protocols():
                ports = self.scanner[host][proto].keys()
                for port in ports:
                    port_info = self.scanner[host][proto][port]
                    if port_info['state'] == 'open':
                        open_ports.append(int(port))
        return open_ports
        
    def _check_responsive_ports(self, target: str) -> List[int]:
        """Check if common ports are responsive and return list of open ports"""
        common_ports = [80, 443, 22, 21, 8080, 8443, 3306, 3389, 7001, 8081, 8000]
        open_ports = []
        
        for port in common_ports:
            try:
                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                    sock.settimeout(1)
                    result = sock.connect_ex((target, port))
                    if result == 0:
                        open_ports.append(port)
                        logger.info(f"Target {target} is responsive on port {port}")
            except:
                pass
        
        return open_ports
    
    def _create_manual_result(self, target: str, open_ports: List[int]) -> Dict[str, Any]:
        """Create scan result dictionary from manually detected open ports"""
        ports_data = []
        
        for port in open_ports:
            service_name = self._guess_service_name(port)
            ports_data.append({
                'port': port,
                'state': 'open',
                'service': service_name,
                'version': '',
                'product': '',
                'extrainfo': 'Detected by manual scan',
                'reason': 'syn-ack',
                'cpe': ''
            })
        
        return {
            'status': 'success',
            'scan_info': {
                'scan_type': 'manual',
                'command_line': 'Manual socket scan',
                'scan_time': '0',
                'total_hosts': 1
            },
            'results': [{
                'host': target,
                'state': 'up',
                'ports': ports_data
            }],
            'manual_detected': True
        }
    
    def _guess_service_name(self, port: int) -> str:
        """Guess service name based on common port numbers"""
        service_map = {
            21: 'ftp',
            22: 'ssh',
            23: 'telnet',
            25: 'smtp',
            53: 'domain',
            80: 'http',
            110: 'pop3',
            139: 'netbios-ssn',
            143: 'imap',
            443: 'https', 
            445: 'microsoft-ds',
            993: 'imaps',
            995: 'pop3s',
            1723: 'pptp',
            3306: 'mysql',
            3389: 'ms-wbt-server',
            5900: 'vnc',
            7001: 'weblogic',
            8000: 'http-alt',
            8080: 'http-proxy',
            8081: 'http-alt',
            8443: 'https-alt'
        }
        return service_map.get(port, 'unknown')

    def get_available_scan_types(self) -> Dict[str, str]:
        return {
            ScanType.QUICK.value: "Fast scan of most common ports (21-23,25,80,443,3306,8080)",
            ScanType.PARTIAL.value: "Standard scan of first 1000 ports with version detection",
            ScanType.COMPLETE.value: "Comprehensive scan of all ports with version detection",
            ScanType.FULL.value: "Intensive scan with all features including vulnerability detection"
        }import dns.resolver
import dns.zone
import socket
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
from typing import List, Dict
import requests
import re

class SubdomainEnumerator:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.resolver = dns.resolver.Resolver()
        self.resolver.timeout = 1
        self.resolver.lifetime = 1
        
        # Common subdomain prefixes
        self.common_subdomains = [
            'www', 'mail', 'ftp', 'smtp', 'pop', 'ns1', 'ns2', 'dns1', 'dns2',
            'webmail', 'admin', 'secure', 'vpn', 'remote', 'test', 'dev', 'host',
            'support', 'api', 'dev', 'staging', 'app', 'portal', 'beta'
        ]

# File: reconnaissance/subdomain_enumerator.py
# Updates needed in enumerate_subdomains method

    def enumerate_subdomains(self, target: str) -> List[Dict]:
        """Main subdomain enumeration method combining multiple techniques"""
        # Clean target - remove protocol and path to get just the domain
        domain = self._extract_domain(target)
        if not domain:
            self.logger.error(f"Invalid domain provided: {target}")
            return []
            
        self.logger.info(f"Starting subdomain enumeration for domain: {domain}")
        
        # Add a basic check to ensure the domain is valid
        try:
            socket.gethostbyname(domain)
        except socket.gaierror:
            # Add the domain itself as a subdomain if we can't resolve it
            # This allows the workflow to continue
            self.logger.warning(f"Domain {domain} could not be resolved, but continuing enumeration")
        
        discovered_subdomains = set()
        results = []

        # Always add the domain itself to results
        try:
            main_domain_ip = socket.gethostbyname(domain)
            discovered_subdomains.add(domain)
            results.append({
                'subdomain': domain,
                'ip_address': main_domain_ip,
                'is_http': True,  # Assume main domain has HTTP
                'http_status': None,
                'status': 'active'
            })
        except Exception as e:
            self.logger.warning(f"Couldn't resolve main domain {domain}: {str(e)}")
        
        # 1. DNS enumeration
        dns_results = self._dns_enumeration(domain)
        for subdomain in dns_results:
            discovered_subdomains.add(subdomain)

        # 2. Brute force common subdomains
        brute_results = self._brute_force_subdomains(domain)
        for subdomain in brute_results:
            discovered_subdomains.add(subdomain)

        # Process and validate all discovered subdomains
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_subdomain = {
                executor.submit(self._validate_subdomain, subdomain): subdomain 
                for subdomain in discovered_subdomains
            }
            
            for future in as_completed(future_to_subdomain):
                subdomain = future_to_subdomain[future]
                try:
                    result = future.result()
                    if result:
                        results.append(result)
                except Exception as e:
                    self.logger.error(f"Error validating {subdomain}: {str(e)}")

        # Ensure we have at least the main domain in results
        if not results and domain:
            results.append({
                'subdomain': domain,
                'ip_address': None,
                'is_http': None,
                'http_status': None,
                'status': 'unknown'
            })

        return results

    def _extract_domain(self, url: str) -> str:
        """Extract root domain from a URL or domain string"""
        # Remove protocol if present
        if '://' in url:
            url = url.split('://', 1)[1]
            
        # Remove path, query params, and fragment
        url = url.split('/', 1)[0]
        url = url.split('?', 1)[0]
        url = url.split('#', 1)[0]
        
        # Remove port if present
        url = url.split(':', 1)[0]
        
        # Validate domain format
        domain_pattern = r'^([a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]{2,}$'
        if re.match(domain_pattern, url):
            return url
        
        return None

    def _dns_enumeration(self, domain: str) -> set:
        """Enumerate subdomains using DNS queries"""
        discovered = set()
        
        try:
            # Try zone transfer first
            try:
                ns_records = self.resolver.resolve(domain, 'NS')
                for ns in ns_records:
                    try:
                        zone = dns.zone.from_xfr(dns.query.xfr(str(ns), domain))
                        for name, _ in zone.nodes.items():
                            subdomain = str(name) + '.' + domain
                            discovered.add(subdomain)
                    except Exception as zone_error:
                        # Zone transfers often fail due to security restrictions, this is expected
                        continue
            except dns.resolver.NoAnswer:
                self.logger.info(f"No NS records found for {domain} - this is normal for many domains")
            except dns.resolver.NXDOMAIN:
                self.logger.info(f"Domain {domain} does not exist in DNS")
            except Exception as e:
                self.logger.info(f"NS record query failed for {domain}: {str(e)}")

            # Try to get common DNS records
            for record_type in ['A', 'AAAA', 'CNAME', 'MX', 'NS', 'TXT']:
                try:
                    answers = self.resolver.resolve(domain, record_type)
                    for rdata in answers:
                        if record_type == 'MX':
                            discovered.add(str(rdata.exchange).rstrip('.'))
                        elif record_type == 'NS':
                            discovered.add(str(rdata).rstrip('.'))
                        elif record_type == 'CNAME':
                            discovered.add(str(rdata.target).rstrip('.'))
                except dns.resolver.NoAnswer:
                    # This is normal - not all record types exist for all domains
                    continue
                except Exception:
                    # Other DNS errors are also common and shouldn't stop enumeration
                    continue

        except Exception as e:
            # Only log as warning since this is one of multiple enumeration techniques
            self.logger.warning(f"DNS enumeration had issues for {domain}: {str(e)}")

        return discovered

    def _brute_force_subdomains(self, domain: str) -> set:
        """Brute force subdomains using common prefixes"""
        discovered = set()
        
        with ThreadPoolExecutor(max_workers=20) as executor:
            future_to_subdomain = {
                executor.submit(self._check_subdomain, f"{prefix}.{domain}"): prefix 
                for prefix in self.common_subdomains
            }
            
            for future in as_completed(future_to_subdomain):
                try:
                    result = future.result()
                    if result:
                        discovered.add(result)
                except Exception as e:
                    continue

        return discovered

    def _check_subdomain(self, subdomain: str) -> str:
        """Check if a subdomain exists"""
        try:
            self.resolver.resolve(subdomain, 'A')
            return subdomain
        except:
            return None

    def _validate_subdomain(self, subdomain: str) -> Dict:
        """Validate and get information about a subdomain"""
        try:
            ip_address = socket.gethostbyname(subdomain)
            
            # Basic HTTP check
            is_http = False
            http_status = None
            try:
                response = requests.get(f"http://{subdomain}", timeout=3)
                is_http = True
                http_status = response.status_code
            except:
                try:
                    response = requests.get(f"https://{subdomain}", timeout=3)
                    is_http = True
                    http_status = response.status_code
                except:
                    pass

            return {
                'subdomain': subdomain,
                'ip_address': ip_address,
                'is_http': is_http,
                'http_status': http_status,
                'status': 'active'
            }
        except Exception as e:
            return Noneimport nmap
from typing import Dict, List, Optional
import logging
from datetime import datetime
import socket
import requests
from urllib.parse import urlparse
import concurrent.futures
import threading
import time
from django.conf import settings
import os

class ServiceIdentifier:
    def __init__(self):
        self.scanner = nmap.PortScanner()
        self.logger = logging.getLogger(__name__)
        self.timeout = getattr(settings, 'SERVICE_SCAN_TIMEOUT', 180)  # 3 minutes default
        self._stop_event = threading.Event()
        # Use shorter timeouts for various operations
        self.discovery_timeout = 60  # 1 minute for port discovery
        self.connection_timeout = 2  # 2 seconds for individual connections

    def identify_services(self, target: str, scan_type: str = 'standard') -> Dict:
        """Comprehensive service identification with improved timeout handling"""
        try:
            # Parse target
            parsed_target = urlparse(target)
            target_host = parsed_target.netloc or parsed_target.path
            target_host = target_host.split(':')[0]

            scan_config = self._get_scan_config(scan_type)
            self.logger.info(f"Starting {scan_type} service scan for {target_host}")

            # Use a simpler approach with manual timeout handling
            self._stop_event.clear()
            
            # Start a timer to enforce overall timeout
            start_time = time.time()
            
            # Set timeout based on scan type
            effective_timeout = self.timeout
            if scan_type == 'quick':
                effective_timeout = min(self.timeout, 120)  # 2 minutes max for quick
            elif scan_type == 'full':
                effective_timeout = self.timeout  # Full timeout for full scan
            
            self.logger.info(f"Using timeout of {effective_timeout} seconds for {scan_type} scan")
            
            # Discover open ports with a shorter timeout
            open_ports = self._discover_ports_with_timeout(target_host, scan_config, 
                                                          timeout=self.discovery_timeout)
            
            # Check if we should stop
            elapsed = time.time() - start_time
            if elapsed > effective_timeout * 0.8:  # 80% of timeout used
                self.logger.warning(f"Port discovery took {elapsed:.1f}s, approaching timeout")
                return {
                    'status': 'success',
                    'services': [],
                    'total_services': 0,
                    'scan_stats': {'open_ports': len(open_ports)}
                }
            
            if not open_ports:
                return {
                    'status': 'success',
                    'services': [],
                    'total_services': 0,
                    'scan_stats': {'open_ports': 0}
                }

            # Perform limited service detection directly without nmap
            self.logger.info(f"Starting basic service detection on {len(open_ports)} ports")
            services = self._perform_basic_service_detection(target_host, open_ports)
            
            # Only do nmap service detection if we have time and found less than 10 ports
            time_left = effective_timeout - (time.time() - start_time)
            if time_left > 60 and len(open_ports) < 10 and not self._stop_event.is_set():
                try:
                    # Try nmap service detection with a short timeout
                    self.logger.info(f"Starting nmap service detection (time left: {time_left:.1f}s)")
                    nmap_services = self._run_nmap_service_detection(
                        target_host, open_ports, scan_config, timeout=min(60, time_left)
                    )
                    
                    # Add any services found by nmap
                    if nmap_services:
                        services.extend(nmap_services)
                except Exception as e:
                    self.logger.error(f"Nmap service detection failed: {str(e)}")
                    # Continue with basic services
            
            # Remove duplicates by port
            service_dict = {}
            for service in services:
                port = service['port']
                # Keep the one with more information
                if port not in service_dict or len(str(service)) > len(str(service_dict[port])):
                    service_dict[port] = service
            
            unique_services = list(service_dict.values())
            
            return {
                'status': 'success',
                'target': target,
                'timestamp': datetime.now().isoformat(),
                'services': unique_services,
                'total_services': len(unique_services),
                'scan_stats': {
                    'open_ports': len(open_ports),
                    'scan_time': f"{time.time() - start_time:.1f}s"
                }
            }

        except Exception as e:
            self.logger.error(f"Service scan failed for {target}: {str(e)}")
            return {
                'status': 'error',
                'error': str(e),
                'details': 'Service scan failed'
            }
    
    def _discover_ports_with_timeout(self, target: str, config: Dict, timeout: int = 60) -> List[int]:
        """Discover open ports with a strict timeout"""
        self.logger.info(f"Starting port discovery with {timeout}s timeout")
        open_ports = set()
        
        # Parse ports to scan
        ports_to_scan = self._parse_ports(config['ports'])
        
        # Use a smaller subset of ports for quicker scanning
        if len(ports_to_scan) > 100:
            # If many ports, prioritize common ones
            common_ports = [21, 22, 23, 25, 53, 80, 110, 139, 143, 443, 445, 993, 995, 1723, 3306, 3389, 5900, 8080, 8443]
            subset = [p for p in ports_to_scan if p in common_ports]
            subset.extend(sorted(list(set(ports_to_scan) - set(subset)))[:100-len(subset)])
            self.logger.info(f"Scanning subset of {len(subset)} ports out of {len(ports_to_scan)}")
            ports_to_scan = subset
        
        start_time = time.time()
        
        # Check ports in parallel with a maximum time limit
        with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
            future_to_port = {
                executor.submit(self._check_port, target, port, 1): port 
                for port in ports_to_scan
            }
            
            for future in concurrent.futures.as_completed(future_to_port):
                if time.time() - start_time > timeout:
                    self.logger.warning(f"Port discovery reached timeout of {timeout}s")
                    break
                    
                if self._stop_event.is_set():
                    break
                    
                try:
                    is_open = future.result()
                    if is_open:
                        port = future_to_port[future]
                        open_ports.add(port)
                        self.logger.info(f"Found open port {port} on {target}")
                except Exception as e:
                    self.logger.debug(f"Error checking port: {str(e)}")
                    continue

        self.logger.info(f"Port discovery completed in {time.time() - start_time:.1f}s, found {len(open_ports)} open ports")
        return sorted(list(open_ports))
    
    def _perform_basic_service_detection(self, target: str, open_ports: List[int]) -> List[Dict]:
        """Perform basic service detection without nmap"""
        services = []
        
        # Common service port mappings
        common_services = {
            21: {'name': 'ftp', 'category': 'file_transfer', 'risk_level': 'HIGH'},
            22: {'name': 'ssh', 'category': 'remote_access', 'risk_level': 'LOW'},
            23: {'name': 'telnet', 'category': 'remote_access', 'risk_level': 'HIGH'},
            25: {'name': 'smtp', 'category': 'mail', 'risk_level': 'MEDIUM'},
            53: {'name': 'domain', 'category': 'dns', 'risk_level': 'LOW'},
            80: {'name': 'http', 'category': 'web', 'risk_level': 'MEDIUM'},
            110: {'name': 'pop3', 'category': 'mail', 'risk_level': 'MEDIUM'},
            139: {'name': 'netbios-ssn', 'category': 'file_transfer', 'risk_level': 'HIGH'},
            143: {'name': 'imap', 'category': 'mail', 'risk_level': 'MEDIUM'},
            443: {'name': 'https', 'category': 'web', 'risk_level': 'LOW'},
            445: {'name': 'microsoft-ds', 'category': 'file_transfer', 'risk_level': 'HIGH'},
            993: {'name': 'imaps', 'category': 'mail', 'risk_level': 'LOW'},
            995: {'name': 'pop3s', 'category': 'mail', 'risk_level': 'LOW'},
            1723: {'name': 'pptp', 'category': 'vpn', 'risk_level': 'MEDIUM'},
            3306: {'name': 'mysql', 'category': 'database', 'risk_level': 'HIGH'},
            3389: {'name': 'ms-wbt-server', 'category': 'remote_access', 'risk_level': 'HIGH'},
            5900: {'name': 'vnc', 'category': 'remote_access', 'risk_level': 'HIGH'},
            8080: {'name': 'http-proxy', 'category': 'web', 'risk_level': 'MEDIUM'},
            8443: {'name': 'https-alt', 'category': 'web', 'risk_level': 'LOW'}
        }
        
        for port in open_ports:
            # Start with defaults
            service_info = common_services.get(port, {
                'name': 'unknown',
                'category': 'other',
                'risk_level': 'MEDIUM'
            })
            
            # Try basic banner grabbing with short timeout
            banner = self._grab_banner(target, port)
            
            # For HTTP ports, try to get more info
            if port in [80, 443, 8080, 8443] or banner and ('HTTP' in banner or 'html' in banner.lower()):
                http_info = self._get_http_info(target, port)
                if http_info:
                    service_detail = {
                        'port': port,
                        'protocol': 'tcp',
                        'state': 'open',
                        'service': {
                            'name': 'http' if port != 443 and port != 8443 else 'https',
                            'product': http_info.get('server', ''),
                            'version': '',
                            'extrainfo': '',
                            'banner': banner
                        },
                        'category': 'web',
                        'risk_level': 'MEDIUM',
                        'http_info': http_info
                    }
                    services.append(service_detail)
                    continue
            
            # For other ports, use the basic info
            service_detail = {
                'port': port,
                'protocol': 'tcp',
                'state': 'open',
                'service': {
                    'name': service_info.get('name', 'unknown'),
                    'product': '',
                    'version': '',
                    'extrainfo': '',
                    'banner': banner
                },
                'category': service_info.get('category', 'other'),
                'risk_level': service_info.get('risk_level', 'MEDIUM')
            }
            services.append(service_detail)
        
        return services
    
    def _grab_banner(self, target: str, port: int) -> str:
        """Grab service banner with timeout"""
        banner = ""
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                sock.settimeout(self.connection_timeout)
                sock.connect((target, port))
                sock.send(b"HEAD / HTTP/1.0\r\n\r\n")
                banner = sock.recv(1024).decode('utf-8', errors='ignore')
        except:
            # Try one more time with no data sent (for non-HTTP services)
            try:
                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                    sock.settimeout(self.connection_timeout)
                    sock.connect((target, port))
                    banner = sock.recv(1024).decode('utf-8', errors='ignore')
            except:
                pass
        return banner
    
    def _get_http_info(self, target: str, port: int) -> Dict:
        """Get HTTP service information"""
        protocol = 'https' if port == 443 or port == 8443 else 'http'
        url = f"{protocol}://{target}:{port}"
        try:
            response = requests.get(
                url, 
                timeout=self.connection_timeout,
                verify=False,
                headers={'User-Agent': 'Mozilla/5.0 SecurityScan'}
            )
            return {
                'status': response.status_code,
                'server': response.headers.get('Server', ''),
                'title': self._extract_title(response.text),
                'headers': dict(response.headers)
            }
        except:
            return None
    
    def _extract_title(self, html: str) -> str:
        """Extract title from HTML"""
        import re
        title_match = re.search(r'<title>(.*?)</title>', html, re.IGNORECASE | re.DOTALL)
        if title_match:
            return title_match.group(1).strip()
        return ""
    
    def _run_nmap_service_detection(self, target: str, open_ports: List[int], config: Dict, timeout: int = 60) -> List[Dict]:
        """Run nmap service detection with timeout"""
        if not open_ports:
            return []
        
        # Create a minimal scan config
        args = "-sV -T4 --version-all"
        ports_str = ','.join(map(str, open_ports))
        
        try:
            # Check if nmap is available and working
            try:
                self.scanner.scan('127.0.0.1', '22', "-sV -T5")
            except:
                self.logger.warning("Nmap test scan failed, skipping nmap service detection")
                return []
            
            # Run the scan with manual timeout using threading
            result_holder = []
            
            def run_scan():
                try:
                    scan_result = self.scanner.scan(target, ports_str, args)
                    result_holder.append(scan_result)
                except Exception as e:
                    self.logger.error(f"Nmap scan error in thread: {str(e)}")
            
            # Start thread
            scan_thread = threading.Thread(target=run_scan)
            scan_thread.daemon = True
            scan_thread.start()
            
            # Wait with timeout
            scan_thread.join(timeout)
            
            if not result_holder:
                self.logger.warning(f"Nmap scan timed out or failed after {timeout}s")
                return []
            
            # Process results
            services = []
            for host in self.scanner.all_hosts():
                for proto in self.scanner[host].all_protocols():
                    for port in self.scanner[host][proto].keys():
                        service_info = self.scanner[host][proto][port]
                        if service_info['state'] == 'open':
                            service_detail = {
                                'port': port,
                                'protocol': proto,
                                'state': service_info['state'],
                                'service': {
                                    'name': service_info.get('name', 'unknown'),
                                    'product': service_info.get('product', ''),
                                    'version': service_info.get('version', ''),
                                    'extrainfo': service_info.get('extrainfo', ''),
                                    'cpe': service_info.get('cpe', [])
                                },
                                'category': self._categorize_service(service_info),
                                'risk_level': self._assess_risk_level(service_info)
                            }
                            services.append(service_detail)
            
            return services
        except Exception as e:
            self.logger.error(f"Error in nmap service detection: {str(e)}")
            return []
            
    def _get_scan_config(self, scan_type: str) -> Dict:
        """Get scan configuration based on scan type"""
        configs = {
            'quick': {
                'ports': '21-23,25,80,443,3306,8080',
                'arguments': '-sV -sT -Pn -T4 --version-light'  # Fast scan
            },
            'standard': {
                'ports': '1-1000',
                'arguments': '-sV -sT -Pn -T4 --version-all'  # Standard scan
            },
            'full': {
                'ports': '1-65535',
                'arguments': '-sV -sT -Pn -T4 --version-all'  # Full scan
            },
            'stealth': {
                'ports': '1-1000',
                'arguments': '-sV -sS -Pn -T2 --version-all'  # Stealth scan
            }
        }
        return configs.get(scan_type, configs['standard'])

    def _check_port(self, target: str, port: int, timeout: int = 1) -> bool:
        """Check if a port is open with timeout"""
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                sock.settimeout(timeout)
                result = sock.connect_ex((target, port))
                return result == 0
        except:
            return False

    def _parse_ports(self, ports_str: str) -> List[int]:
        """Parse ports string into list of port numbers"""
        ports = set()
        for part in ports_str.split(','):
            if '-' in part:
                start, end = map(int, part.split('-'))
                ports.update(range(start, end + 1))
            else:
                ports.add(int(part))
        return list(ports)

    def _categorize_service(self, service_info: Dict) -> str:
        """Categorize service based on name and product"""
        service_categories = {
            'web': ['http', 'https', 'nginx', 'apache'],
            'database': ['mysql', 'postgresql', 'mongodb'],
            'mail': ['smtp', 'pop3', 'imap'],
            'file_transfer': ['ftp', 'sftp'],
            'remote_access': ['ssh', 'telnet', 'rdp'],
            'dns': ['dns', 'domain']
        }

        service_name = service_info.get('name', '').lower()
        
        for category, services in service_categories.items():
            if any(s in service_name for s in services):
                return category
        return 'other'

    def _assess_risk_level(self, service_info: Dict) -> str:
        """Basic risk assessment of services"""
        high_risk = ['telnet', 'ftp']
        medium_risk = ['smtp', 'pop3']
        
        service_name = service_info.get('name', '').lower()
        
        if any(service in service_name for service in high_risk):
            return 'HIGH'
        elif any(service in service_name for service in medium_risk):
            return 'MEDIUM'
        return 'LOW'from django.db import models
from django.core.validators import MinValueValidator, MaxValueValidator
import logging

logger = logging.getLogger(__name__)

class Vulnerability(models.Model):
    SEVERITY_CHOICES = [
        ('LOW', 'Low'),
        ('MEDIUM', 'Medium'),
        ('HIGH', 'High'),
        ('CRITICAL', 'Critical'),
    ]

    # Updated source choices to include more options
    SOURCE_CHOICES = [
        ('internal', 'Internal Scanner'),
        ('zap', 'OWASP ZAP'),
        ('nuclei', 'Nuclei Scanner'),
        ('openvas', 'OpenVAS Scanner'),
        ('manual', 'Manual Entry'),
        ('multiple', 'Multiple Sources')  # For correlated findings
    ]

    # Basic Information
    target = models.CharField(max_length=255, db_index=True)
    name = models.CharField(max_length=255)
    description = models.TextField()
    severity = models.CharField(max_length=10, choices=SEVERITY_CHOICES, db_index=True)
    vuln_type = models.CharField(max_length=50, db_index=True)
    
    # Evidence and Details
    evidence = models.TextField()
    solution = models.TextField(blank=True)
    references = models.JSONField(default=list)
    
    # Source and Confidence - increased max_length to accommodate combined sources
    source = models.CharField(max_length=100)  # Removed choices constraint and increased length
    confidence = models.CharField(max_length=50, default='medium')
    
    # Status and Tracking
    discovery_date = models.DateTimeField(auto_now_add=True)
    is_fixed = models.BooleanField(default=False, db_index=True)
    fix_date = models.DateTimeField(null=True, blank=True)
    notes = models.TextField(blank=True)
    
    # Additional Metadata
    cwe = models.CharField(max_length=50, blank=True)
    cvss_score = models.FloatField(
        null=True, 
        blank=True,
        validators=[MinValueValidator(0.0), MaxValueValidator(10.0)]
    )
    metadata = models.JSONField(default=dict)  # Added metadata field
    
    def __str__(self):
        return f"{self.target} - {self.name} ({self.severity})"

    def save(self, *args, **kwargs):
        # Ensure severity is uppercase
        if self.severity:
            self.severity = self.severity.upper()
        
        # Set fix_date when vulnerability is marked as fixed
        if self.is_fixed and not self.fix_date:
            from django.utils import timezone
            self.fix_date = timezone.now()
        
        # Handle source field for multiple sources
        if ',' in self.source:
            # Store original sources in metadata for reference
            if not self.metadata:
                self.metadata = {}
            self.metadata['original_sources'] = self.source.split(',')
            
        super().save(*args, **kwargs)

    @classmethod
    def deduplicate_vulnerabilities(cls, target):
        """
        Deduplicate vulnerabilities for a target by merging duplicates
        
        Args:
            target: The target hostname/domain
            
        Returns:
            dict: Statistics about deduplication
        """
        from django.db.models import Count
        from django.db import transaction
        
        # Keep track of statistics
        stats = {
            'original_count': cls.objects.filter(target=target).count(),
            'merged_count': 0,
            'final_count': 0
        }
        
        try:
            with transaction.atomic():
                # Find groups of vulnerabilities with the same name, type, and severity
                # These are candidates for deduplication
                duplicate_groups = cls.objects.filter(target=target).values(
                    'name', 'vuln_type', 'severity'
                ).annotate(
                    count=Count('id')
                ).filter(count__gt=1)
                
                # Process each group
                for group in duplicate_groups:
                    duplicates = cls.objects.filter(
                        target=target,
                        name=group['name'],
                        vuln_type=group['vuln_type'],
                        severity=group['severity']
                    ).order_by('discovery_date')
                    
                    # Keep the first (oldest) vulnerability as the canonical one
                    if duplicates.count() > 1:
                        primary_vuln = duplicates.first()
                        
                        # Process other duplicates
                        for dup in duplicates[1:]:
                            # Combine sources if different
                            sources = set(primary_vuln.source.split(','))
                            for source in dup.source.split(','):
                                sources.add(source)
                            primary_vuln.source = ','.join(sorted(sources))
                            
                            # Use the higher CVSS score if available
                            if dup.cvss_score and (not primary_vuln.cvss_score or dup.cvss_score > primary_vuln.cvss_score):
                                primary_vuln.cvss_score = dup.cvss_score
                            
                            # Take the most recent evidence if available
                            if dup.evidence and len(dup.evidence) > len(primary_vuln.evidence):
                                primary_vuln.evidence = dup.evidence
                                
                            # Merge references
                            primary_refs = set(primary_vuln.references)
                            for ref in dup.references:
                                primary_refs.add(ref)
                            primary_vuln.references = list(primary_refs)
                            
                            # Track the merged duplicate in metadata
                            if not primary_vuln.metadata:
                                primary_vuln.metadata = {}
                            if 'merged_duplicates' not in primary_vuln.metadata:
                                primary_vuln.metadata['merged_duplicates'] = []
                            primary_vuln.metadata['merged_duplicates'].append({
                                'id': dup.id,
                                'discovery_date': dup.discovery_date.isoformat(),
                                'source': dup.source
                            })
                            
                            # Delete the duplicate
                            dup.delete()
                            stats['merged_count'] += 1
                        
                        # Save the updated primary vulnerability
                        primary_vuln.save()
            
            # Calculate final counts
            stats['final_count'] = cls.objects.filter(target=target).count()
            return stats
            
        except Exception as e:
            logger.error(f"Error deduplicating vulnerabilities: {str(e)}")
            return {'error': str(e)}
    
    @property
    def age_in_days(self):
        from django.utils import timezone
        return (timezone.now() - self.discovery_date).days

    @property
    def risk_score(self):
        """Calculate risk score based on CVSS and age"""
        base_score = self.cvss_score if self.cvss_score else {
            'CRITICAL': 9.0,
            'HIGH': 7.0,
            'MEDIUM': 5.0,
            'LOW': 3.0
        }.get(self.severity, 1.0)
        
        # Age factor: 1.0 - 2.0 based on age (caps at 90 days)
        age_factor = min(1 + (self.age_in_days / 90), 2.0)
        
        return base_score * age_factor
    
    @property
    def source_list(self):
        """Return the source as a list for easier filtering"""
        return self.source.split(',')

class NucleiFinding(models.Model):
    SEVERITY_CHOICES = [
        ('CRITICAL', 'Critical'),
        ('HIGH', 'High'),
        ('MEDIUM', 'Medium'),
        ('LOW', 'Low'),
        ('INFO', 'Info'),
    ]

    template_id = models.CharField(max_length=255)
    name = models.CharField(max_length=255)
    severity = models.CharField(max_length=10, choices=SEVERITY_CHOICES)
    finding_type = models.CharField(max_length=50)
    host = models.CharField(max_length=255)
    matched_at = models.URLField(max_length=500)
    description = models.TextField()
    tags = models.JSONField(default=list)
    references = models.JSONField(default=list)
    cwe = models.CharField(max_length=50, blank=True, null=True)
    cvss_score = models.FloatField(null=True, blank=True)
    discovery_date = models.DateTimeField(auto_now_add=True)
    scan_id = models.CharField(max_length=100)
    target = models.CharField(max_length=255)
    
    class Meta:
        indexes = [
            models.Index(fields=['template_id']),
            models.Index(fields=['severity']),
            models.Index(fields=['discovery_date']),
            models.Index(fields=['target']),
        ]import requests
import socket
import ssl
from typing import List, Dict
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse
import warnings
from datetime import datetime
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

class VulnerabilityScanner:
    # Define valid scan types and their descriptions
    VALID_SCAN_TYPES = {
        'quick': 'Fast scan of common ports/vulnerabilities',
        'standard': 'Standard comprehensive scan',
        'full': 'Full detailed scan with all checks'
    }

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # Available vulnerability checks
        self.checks = {
            'http': self._check_http_vulnerabilities,
            'ssl': self._check_ssl_vulnerabilities,
            'ports': self._check_port_vulnerabilities,
            'headers': self._check_header_vulnerabilities
        }
        
        # Configure HTTP session with retry logic
        self.session = self._create_session()
        warnings.filterwarnings('ignore', message='Unverified HTTPS request')

    def _create_session(self):
        """Create requests session with retry logic"""
        session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=0.5,
            status_forcelist=[500, 502, 503, 504]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        return session

    def validate_scan_type(self, scan_type: str) -> str:
        """Validate and return correct scan type"""
        if not scan_type or scan_type.lower() not in self.VALID_SCAN_TYPES:
            raise ValueError(
                f"Invalid scan type: {scan_type}. Valid types are: {list(self.VALID_SCAN_TYPES.keys())}"
            )
        return scan_type.lower()

    def get_available_scan_types(self) -> Dict[str, str]:
        """Return available scan types and their descriptions"""
        return self.VALID_SCAN_TYPES

    def _is_port_open(self, host: str, port: int) -> bool:
        """Check if a port is open with proper error handling"""
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(2)
            result = sock.connect_ex((host, port))
            sock.close()
            return result == 0
        except socket.gaierror:
            self.logger.warning(f"Hostname {host} could not be resolved")
            return False
        except socket.error as e:
            self.logger.warning(f"Error checking port {port}: {str(e)}")
            return False

    def scan_target(self, target: str, scan_type: str = 'standard', checks: List[str] = None) -> Dict:
        """Perform vulnerability scan on target"""
        try:
            # Validate scan type
            validated_scan_type = self.validate_scan_type(scan_type)
            
            if not checks:
                checks = list(self.checks.keys())

            results = {
                'target': target,
                'scan_start': datetime.now().isoformat(),
                'scan_type': validated_scan_type,
                'vulnerabilities': [],
                'summary': {
                    'high': 0,
                    'medium': 0,
                    'low': 0
                }
            }

            # Parse target
            parsed_target = urlparse(target)
            target_host = parsed_target.netloc or parsed_target.path
            target_host = target_host.split(':')[0]

            # Check basic connectivity first
            if not self._is_port_open(target_host, 80) and not self._is_port_open(target_host, 443):
                self.logger.warning(f"Target {target_host} appears to be unreachable")

            # Adjust checks based on scan type
            if validated_scan_type == 'quick':
                checks = ['http', 'ports']  # Limited checks for quick scan
            elif validated_scan_type == 'full':
                checks = list(self.checks.keys())  # All checks for full scan

            # Run vulnerability checks
            with ThreadPoolExecutor(max_workers=5) as executor:
                futures = []
                
                for check in checks:
                    if check in self.checks:
                        check_func = self.checks[check]
                        futures.append(executor.submit(check_func, target_host))

                vulnerabilities = []
                for future in as_completed(futures):
                    try:
                        check_results = future.result()
                        if check_results:
                            vulnerabilities.extend(check_results)
                    except Exception as e:
                        self.logger.error(f"Check failed: {str(e)}")

            # Process vulnerabilities
            processed_vulns = self._process_vulnerabilities(vulnerabilities)
            results['vulnerabilities'] = processed_vulns
            
            # Update summary
            for vuln in processed_vulns:
                severity = vuln['severity'].lower()
                if severity in results['summary']:
                    results['summary'][severity] += 1

            results['scan_end'] = datetime.now().isoformat()
            return results

        except ValueError as e:
            self.logger.error(f"Validation error: {str(e)}")
            return {
                'status': 'error',
                'error': str(e)
            }
        except Exception as e:
            self.logger.error(f"Scan failed for {target}: {str(e)}")
            return {
                'status': 'error',
                'error': str(e)
            }

    def _check_http_vulnerabilities(self, target: str) -> List[Dict]:
        """Check for common HTTP vulnerabilities"""
        vulnerabilities = []
        
        # Check HTTP connectivity
        http_accessible = False
        https_accessible = False
        has_error_response = False
        
        try:
            # Try HTTP
            response = self.session.get(
                f"http://{target}", 
                timeout=10, 
                allow_redirects=False, 
                verify=False
            )
            http_accessible = True
            
            # Check if it's an error response (4xx-5xx)
            if 400 <= response.status_code < 600:
                has_error_response = True
                vulnerabilities.append({
                    'type': 'http_error',
                    'name': f'HTTP Error Response: {response.status_code}',
                    'description': f'Server returned an error status code: {response.status_code}',
                    'severity': 'MEDIUM' if 500 <= response.status_code < 600 else 'LOW',
                    'evidence': f"HTTP Status Code: {response.status_code}"
                })
            elif response.status_code not in [301, 308]:
                vulnerabilities.append({
                    'type': 'http_redirect',
                    'name': 'Missing HTTP to HTTPS Redirect',
                    'description': 'Server does not redirect HTTP to HTTPS',
                    'severity': 'MEDIUM',
                    'evidence': f"HTTP Status Code: {response.status_code}"
                })
        except requests.RequestException as e:
            self.logger.warning(f"HTTP check failed: {str(e)}")

        try:
            # Try HTTPS
            response = self.session.get(
                f"https://{target}", 
                timeout=10, 
                verify=False
            )
            https_accessible = True
            
            # Check if it's an error response
            if 400 <= response.status_code < 600:
                has_error_response = True
            
            # Check security headers
            headers = response.headers
            security_headers = {
                'Strict-Transport-Security': 'HSTS not enabled',
                'X-Content-Type-Options': 'Missing protection against MIME-type sniffing',
                'X-Frame-Options': 'Missing clickjacking protection'
            }

            for header, issue in security_headers.items():
                if header not in headers:
                    vulnerabilities.append({
                        'type': 'missing_header',
                        'name': f'Missing {header}',
                        'description': issue,
                        'severity': 'MEDIUM',
                        'evidence': 'Header not present in response'
                    })
        except requests.RequestException as e:
            self.logger.warning(f"HTTPS check failed: {str(e)}")

        # Only flag as inaccessible if we couldn't connect at all (not just error responses)
        if not http_accessible and not https_accessible:
            if has_error_response:
                vulnerabilities.append({
                    'type': 'web_error',
                    'name': 'Web Server Error Response',
                    'description': 'Web server is accessible but returns error responses on standard ports (80/443)',
                    'severity': 'MEDIUM',
                    'evidence': 'Error response received'
                })
            else:
                vulnerabilities.append({
                    'type': 'web_access',
                    'name': 'Web Server Inaccessible',
                    'description': 'Unable to access web server on standard ports (80/443)',
                    'severity': 'HIGH',
                    'evidence': 'Connection attempts failed'
                })

        return vulnerabilities

    def _check_ssl_vulnerabilities(self, target: str) -> List[Dict]:
        """Check for SSL/TLS vulnerabilities"""
        vulnerabilities = []
        try:
            context = ssl.create_default_context()
            context.check_hostname = False
            context.verify_mode = ssl.CERT_NONE
            
            with socket.create_connection((target, 443), timeout=10) as sock:
                with context.wrap_socket(sock, server_hostname=target) as ssock:
                    version = ssock.version()
                    if version in ['TLSv1', 'TLSv1.1', 'SSLv2', 'SSLv3']:
                        vulnerabilities.append({
                            'type': 'weak_ssl',
                            'name': 'Weak SSL/TLS Version',
                            'description': f'Server supports outdated {version}',
                            'severity': 'HIGH',
                            'evidence': version
                        })

        except (socket.timeout, ssl.SSLError, ConnectionRefusedError) as e:
            self.logger.warning(f"SSL check failed: {str(e)}")
        except Exception as e:
            self.logger.warning(f"Unexpected SSL check error: {str(e)}")

        return vulnerabilities

    def _check_port_vulnerabilities(self, target: str) -> List[Dict]:
        """Check for common port-related vulnerabilities"""
        vulnerabilities = []
        risky_ports = {
            21: 'FTP',
            23: 'Telnet',
            445: 'SMB',
            3389: 'RDP'
        }

        for port, service in risky_ports.items():
            if self._is_port_open(target, port):
                vulnerabilities.append({
                    'type': 'open_port',
                    'name': f'Potentially Risky Port Open: {port}',
                    'description': f'Port {port} ({service}) is open and could pose a security risk',
                    'severity': 'HIGH',
                    'evidence': f'Port {port} is accessible'
                })

        return vulnerabilities

    def _check_header_vulnerabilities(self, target: str) -> List[Dict]:
        """Check for header-based vulnerabilities"""
        vulnerabilities = []
        try:
            response = self.session.get(
                f"https://{target}", 
                timeout=10, 
                verify=False
            )
            headers = response.headers

            sensitive_headers = ['Server', 'X-Powered-By', 'X-AspNet-Version']
            for header in sensitive_headers:
                if header in headers:
                    vulnerabilities.append({
                        'type': 'info_disclosure',
                        'name': f'Information Disclosure in Headers',
                        'description': f'Header {header} reveals server information',
                        'severity': 'LOW',
                        'evidence': f'{header}: {headers[header]}'
                    })

        except requests.RequestException as e:
            self.logger.warning(f"Header check failed: {str(e)}")

        return vulnerabilities

    def _process_vulnerabilities(self, vulnerabilities: List[Dict]) -> List[Dict]:
        """Process and enhance vulnerability information"""
        processed = []
        for vuln in vulnerabilities:
            if vuln is None:
                continue
                
            vuln.setdefault('severity', 'low')
            vuln.setdefault('confidence', 'medium')
            vuln.setdefault('references', [])
            
            if 'cvss' not in vuln:
                vuln['cvss'] = self._calculate_cvss(vuln)
                
            processed.append(vuln)
            
        return sorted(processed, key=lambda x: x['severity'], reverse=True)

    def _calculate_cvss(self, vuln: Dict) -> float:
        """Calculate CVSS score based on vulnerability characteristics"""
        if vuln['severity'].lower() == 'high':
            return 7.5
        elif vuln['severity'].lower() == 'medium':
            return 5.0
        return 2.5from typing import Dict, List
import logging
from datetime import datetime
from .scanner import VulnerabilityScanner
from .zap_manager import ZAPManager
from .models import Vulnerability, NucleiFinding
from .nuclei_scanner import NucleiScanner
from .correlation import VulnerabilityCorrelator  # Import the enhanced correlator

class UnifiedVulnerabilityScanner:
    # Define valid scan types
    VALID_SCAN_TYPES = {
        'quick': 'Fast scan with basic checks',
        'standard': 'Standard comprehensive scan',
        'full': 'Full detailed scan with all checks'
    }

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.internal_scanner = VulnerabilityScanner()
        self.zap_manager = ZAPManager()
        self.correlator = VulnerabilityCorrelator()  # Initialize the correlator
        try:
            self.nuclei_scanner = NucleiScanner()
            self.logger.info("Successfully initialized Nuclei scanner")
        except Exception as e:
            self.logger.error(f"Failed to initialize Nuclei scanner: {str(e)}")
            self.nuclei_scanner = None
            # Don't raise the exception, just log it and continue with other scanners

    def validate_scan_type(self, scan_type: str) -> str:
        """Validate scan type and return normalized version"""
        if not scan_type:
            return 'standard'  # Default scan type
            
        normalized_type = scan_type.lower().strip()
        if normalized_type not in self.VALID_SCAN_TYPES:
            raise ValueError(
                f"Invalid scan type: '{scan_type}'. Valid types are: {list(self.VALID_SCAN_TYPES.keys())}"
            )
        return normalized_type

    def get_scan_types(self) -> Dict[str, str]:
        """Return available scan types and their descriptions"""
        return self.VALID_SCAN_TYPES

    def scan_target(self, target: str, scan_type: str = 'standard', 
                   include_zap: bool = True, include_nuclei: bool = True,
                   nuclei_scan_type: str = 'basic',
                   use_advanced_correlation: bool = True) -> Dict:
        """
        Perform a comprehensive scan using multiple scanners based on parameters
        """
        try:
            # Validate scan type first
            validated_scan_type = self.validate_scan_type(scan_type)
            
            results = {
                'target': target,
                'scan_start': datetime.now().isoformat(),
                'vulnerabilities': [],
                'scanners_used': ['internal'],
                'summary': {
                    'high': 0,
                    'medium': 0,
                    'low': 0,
                    'total': 0
                },
                'correlation': {}
            }

            # Track findings from each scanner for correlation
            internal_results = []
            zap_results = []
            nuclei_results = []

            # Run internal scanner with validated scan type
            self.logger.info(f"Starting internal scanner with scan type: {validated_scan_type}")
            internal_scan = self.internal_scanner.scan_target(target, validated_scan_type)
            
            if internal_scan.get('vulnerabilities'):
                internal_results = internal_scan['vulnerabilities']
                results['scanners_used'].append('internal')
                self.logger.info(f"Internal scanner found {len(internal_results)} vulnerabilities")

            # Run ZAP scan if requested
            if include_zap:
                self.logger.info("Starting ZAP scanner")
                if self.zap_manager.ensure_zap_running():
                    results['scanners_used'].append('zap')
                    zap_scan = self.zap_manager.run_scan(target)
                    if zap_scan.get('status') == 'success' and zap_scan.get('alerts'):
                        zap_results = zap_scan['alerts']
                        self.logger.info(f"ZAP scanner found {len(zap_results)} vulnerabilities")
                else:
                    self.logger.warning("ZAP scanner not available")

            # Run Nuclei scan if requested
            # In the scan_target method of UnifiedVulnerabilityScanner class:
            # Run Nuclei scan if requested
            if include_nuclei:
                if self.nuclei_scanner:
                    try:
                        self.logger.info(f"Starting Nuclei scan with type: {nuclei_scan_type}")
                        if nuclei_scan_type.lower() == 'advanced':
                            nuclei_scan = self.nuclei_scanner.run_advanced_scan(target)
                        else:
                            nuclei_scan = self.nuclei_scanner.run_basic_scan(target)
                        
                        if nuclei_scan.get('status') == 'success' and nuclei_scan.get('findings'):
                            results['scanners_used'].append('nuclei')
                            nuclei_results = nuclei_scan['findings']
                            self.logger.info(f"Nuclei scanner found {len(nuclei_results)} vulnerabilities")
                        else:
                            self.logger.warning(f"Nuclei scan completed but returned no findings or had an error: {nuclei_scan.get('error', 'No error specified')}")
                    except Exception as e:
                        self.logger.error(f"Nuclei scan failed: {str(e)}")
                        self.logger.error("Continuing with other scanners")
                else:
                    self.logger.warning("Nuclei scanner not available or failed to initialize")

            # Use enhanced correlation if enabled
            if use_advanced_correlation:
                self.logger.info("Using advanced correlation for findings")
                correlation_result = self.correlator.correlate_findings(
                    internal_results=internal_results,
                    zap_results=zap_results,
                    nuclei_results=nuclei_results,
                    target=target
                )
                
                if correlation_result.get('status') == 'success':
                    results['vulnerabilities'] = correlation_result.get('findings', [])
                    results['correlation'] = {
                        'original_count': correlation_result.get('original_count', 0),
                        'correlated_count': correlation_result.get('correlated_count', 0),
                        'reduction_percentage': correlation_result.get('statistics', {}).get('reduction_percentage', 0),
                        'stats': correlation_result.get('statistics', {})
                    }
                    self.logger.info(f"Correlation reduced findings from {correlation_result.get('original_count', 0)} to {correlation_result.get('correlated_count', 0)}")
                else:
                    # Fallback to basic processing if correlation failed
                    self.logger.warning("Advanced correlation failed, falling back to basic processing")
                    self._process_scanner_results(results, internal_results, zap_results, nuclei_results)
            else:
                # Use basic processing
                self.logger.info("Using basic processing for findings")
                self._process_scanner_results(results, internal_results, zap_results, nuclei_results)
            
            # Update summary
            self._update_summary(results)
            
            # Add scan configuration to results
            results['scan_config'] = {
                'scan_type': validated_scan_type,
                'scanners': {
                    'internal': True,
                    'zap': include_zap,
                    'nuclei': {
                        'enabled': include_nuclei,
                        'type': nuclei_scan_type if include_nuclei else 'disabled'
                    }
                },
                'correlation': {
                    'advanced': use_advanced_correlation
                }
            }
            
            results['scan_end'] = datetime.now().isoformat()
            results['status'] = 'success'
            
            # Deduplicate vulnerabilities
            deduplication_stats = Vulnerability.deduplicate_vulnerabilities(target)
            
            # Add deduplication stats to results
            results['deduplication_stats'] = deduplication_stats
            
            return results

        except ValueError as e:
            # Handle validation errors
            error_msg = str(e)
            self.logger.error(f"Validation error: {error_msg}")
            return {
                'status': 'error',
                'error': error_msg,
                'valid_scan_types': list(self.VALID_SCAN_TYPES.keys())
            }
        except Exception as e:
            self.logger.error(f"Unified scan failed: {str(e)}")
            return {
                'status': 'error',
                'error': str(e)
            }
            
    def _process_scanner_results(self, results: Dict, internal_results: List, 
                                zap_results: List, nuclei_results: List) -> None:
        """Process results from different scanners using basic approach"""
        # Process internal scanner results
        for vuln in internal_results:
            results['vulnerabilities'].append({
                'source': 'internal',
                'name': vuln.get('name', ''),
                'description': vuln.get('description', ''),
                'severity': vuln.get('severity', 'LOW'),
                'type': vuln.get('type', ''),
                'evidence': vuln.get('evidence', ''),
                'confidence': vuln.get('confidence', 'medium'),
                'cvss_score': vuln.get('cvss', 0.0)
            })

        # Process ZAP scanner results
        for alert in zap_results:
            results['vulnerabilities'].append({
                'source': 'zap',
                'name': alert.get('name', ''),
                'description': alert.get('description', ''),
                'severity': self._normalize_severity(alert.get('risk')),
                'type': 'web',
                'evidence': alert.get('evidence', ''),
                'confidence': alert.get('confidence', 'medium'),
                'solution': alert.get('solution', ''),
                'cwe': alert.get('cweid', ''),
                'metadata': {
                    'url': alert.get('url', ''),
                    'parameter': alert.get('parameter', '')
                }
            })

        # Process Nuclei scanner results
        for finding in nuclei_results:
            results['vulnerabilities'].append({
                'source': 'nuclei',
                'name': finding.get('name', ''),
                'description': finding.get('description', ''),
                'severity': finding.get('severity', 'LOW'),
                'type': finding.get('type', 'nuclei'),
                'evidence': finding.get('evidence', ''),
                'confidence': 'high',
                'cvss_score': finding.get('cvss_score', 0.0),
                'cwe': finding.get('cwe', ''),
                'references': finding.get('references', []),
                'metadata': {
                    'template_id': finding.get('template_id', ''),
                    'tags': finding.get('tags', []),
                    'matched_at': finding.get('matched', ''),
                    'host': finding.get('host', '')
                }
            })
        
        # Basic deduplication by name and severity
        results['vulnerabilities'] = self._basic_deduplicate(results['vulnerabilities'])
        
    def _basic_deduplicate(self, vulnerabilities: List[Dict]) -> List[Dict]:
        """Simple deduplication of vulnerabilities by name and severity"""
        unique_vulns = {}
        
        for vuln in vulnerabilities:
            # Create a key for deduplication
            key = f"{vuln['name']}_{vuln['severity']}"
            
            if key in unique_vulns:
                existing = unique_vulns[key]
                # Merge sources
                sources = set([existing['source']])
                sources.add(vuln['source'])
                existing['source'] = ','.join(sources)
                # Take highest confidence
                if vuln.get('confidence') == 'high':
                    existing['confidence'] = 'high'
            else:
                unique_vulns[key] = vuln
                
        return list(unique_vulns.values())

    def _normalize_severity(self, severity: str) -> str:
        """Normalize severity ratings across different scanners"""
        severity = str(severity).lower()
        
        if severity in ['critical', 'high', '3', '4']:
            return 'HIGH'
        elif severity in ['medium', 'warning', '2']:
            return 'MEDIUM'
        elif severity in ['low', 'info', '1']:
            return 'LOW'
        return 'INFO'

    def _update_summary(self, results: Dict) -> None:
        """Update the summary counts"""
        summary = {'high': 0, 'medium': 0, 'low': 0, 'total': 0}
        
        for vuln in results['vulnerabilities']:
            severity = vuln['severity'].lower()
            if severity in summary:
                summary[severity] += 1
            summary['total'] += 1

        results['summary'] = summary

    def get_scanner_status(self) -> Dict:
        """Get status of all scanners"""
        status = {
            'internal': {
                'status': 'available',
                'checks': list(self.internal_scanner.checks.keys())
            },
            'zap': self.zap_manager.get_status()
        }

        # Add Nuclei status if initialized
        if self.nuclei_scanner:
            try:
                nuclei_info = self.nuclei_scanner.get_template_info()
                status['nuclei'] = {
                    'status': 'available',
                    'templates': nuclei_info.get('total_templates', 0),
                    'template_types': nuclei_info.get('template_types', {})
                }
            except Exception as e:
                status['nuclei'] = {
                    'status': 'error',
                    'error': str(e)
                }

        return status

    def _save_findings(self, vulnerabilities: List[Dict], target: str) -> None:
        """Save findings to database"""
        for vuln in vulnerabilities:
            # Extract metadata
            metadata = {
                'url': vuln.get('url'),
                'parameter': vuln.get('parameter'),
                'extra_info': vuln.get('metadata', {})
            }
            
            try:
                Vulnerability.objects.create(
                    target=target,
                    name=vuln['name'],
                    description=vuln.get('description', ''),
                    severity=vuln['severity'],
                    vuln_type=vuln.get('type', 'unknown'),
                    evidence=vuln.get('evidence', ''),
                    source=vuln['source'],
                    confidence=vuln['confidence'],
                    solution=vuln.get('solution', ''),
                    cwe=vuln.get('cwe', ''),
                    cvss_score=vuln.get('cvss_score'),
                    references=vuln.get('references', []),
                    metadata=metadata
                )
            except Exception as e:
                self.logger.error(f"Error saving vulnerability: {str(e)}")
                self.logger.error(f"Vulnerability data: {vuln}")from django.db import models

# Create your models here.
class Report(models.Model):
    title = models.CharField(max_length=255)
    creation_date = models.DateTimeField(auto_now_add=True)
    content = models.TextField()
    report_type = models.CharField(max_length=50)import logging
from datetime import datetime
import json
from django.core.serializers.json import DjangoJSONEncoder
from django.forms.models import model_to_dict
from .models import Report
from reconnaissance.models import Subdomain, PortScan
from vulnerability.models import Vulnerability

class ReportGenerator:
    def __init__(self):
        # Add logger initialization
        self.logger = logging.getLogger(__name__)
        
    def generate_report(self, report_type: str, target: str, output_format: str = 'json', scan_results: dict = None) -> Report:
        """
        Generate a security report
        
        Args:
            report_type: Type of report ('basic', 'detailed', 'executive')
            target: Target hostname/domain
            output_format: Format to generate ('json', 'html', 'pdf')
            scan_results: Optional dictionary containing scan results to include
            
        Returns:
            Report: The generated report object
        """
        # Clean target string
        target = target.strip()
        
        if report_type not in ['basic', 'detailed', 'executive']:
            report_type = 'basic'
        
        # Generate report content based on type
        try:
            if report_type == 'detailed':
                content = self.generate_detailed_report(target, scan_results)
            elif report_type == 'executive':
                content = self.generate_executive_report(target, scan_results)
            else:
                content = self.generate_basic_report(target, scan_results)
            
            # Make sure severity counts are properly calculated
            self._update_severity_counts(content)
            
            # If output format is PDF, ensure proper formatting
            if output_format == 'pdf':
                content = self._format_for_pdf(content)
            
            # Properly serialize content to JSON string
            json_content = json.dumps(content, cls=DjangoJSONEncoder)
            
            # Create and save the report
            report = Report.objects.create(
                title=f"{report_type.capitalize()} Security Report - {target}",
                content=json_content,
                report_type=f"{report_type}_{output_format}"
            )
            
            self.logger.info(f"Generated {report_type} report for {target} with ID {report.id}")
            return report
                
        except Exception as e:
            self.logger.error(f"Report generation failed: {str(e)}")
            raise

    def _update_severity_counts(self, content: dict) -> None:
        """
        Ensure vulnerability severity counts are accurate
        """
        if 'vulnerabilities' not in content:
            return
        
        # Reset counters
        severity_counts = {
            'critical': 0,
            'high': 0,
            'medium': 0,
            'low': 0,
            'info': 0
        }
        
        # Count each vulnerability by severity
        for vuln in content['vulnerabilities']:
            severity = vuln.get('severity', '').lower()
            if severity in severity_counts:
                severity_counts[severity] += 1
        
        # Update summary with accurate counts
        if 'summary' in content:
            for severity, count in severity_counts.items():
                content['summary'][severity] = count
        
        return

    def _format_for_pdf(self, content: dict) -> dict:
        """
        Format report content specifically for PDF output
        """
        # Create a copy to avoid modifying the original
        pdf_content = content.copy()
        
        # Ensure proper table formatting for vulnerabilities
        if 'vulnerabilities' in pdf_content:
            for vuln in pdf_content['vulnerabilities']:
                # Ensure descriptions are properly formatted
                if 'description' in vuln:
                    vuln['description'] = self._clean_description(vuln['description'])
                
                # Ensure evidence is properly formatted
                if 'evidence' in vuln:
                    vuln['evidence'] = self._clean_evidence(vuln['evidence'])
        
        # Ensure network visualization data is properly formatted
        if 'network_data' in pdf_content:
            pdf_content['network_data'] = self._format_network_data_for_pdf(pdf_content['network_data'])
        
        return pdf_content

    def _format_network_data_for_pdf(self, network_data: dict) -> dict:
        """Format network visualization data for PDF output"""
        # Simplify network data to avoid rendering issues
        if not network_data:
            return {}
            
        # Limit number of nodes and links to avoid overcrowding
        if 'nodes' in network_data and len(network_data['nodes']) > 20:
            # Keep only the most important 20 nodes
            network_data['nodes'] = sorted(
                network_data['nodes'], 
                key=lambda x: self._get_node_importance(x)
            )[:20]
            
            # Keep only links between these nodes
            node_ids = {node['id'] for node in network_data['nodes']}
            network_data['links'] = [
                link for link in network_data.get('links', [])
                if link['source'] in node_ids and link['target'] in node_ids
            ]
        
        return network_data

    def _get_node_importance(self, node: dict) -> int:
        """Calculate node importance for filtering"""
        # Host nodes are most important
        if node.get('type') == 'host':
            return 100
            
        # Next subdomains
        if node.get('type') == 'subdomain':
            return 90
            
        # Important service nodes
        if node.get('type') == 'service':
            # Web services are more important
            if 'http' in node.get('name', '').lower():
                return 80
            return 70
            
        # High and critical vulnerabilities
        if node.get('type') == 'vulnerability':
            if 'critical' in node.get('name', '').lower():
                return 85
            if 'high' in node.get('name', '').lower():
                return 75
                
        # Default importance
        return 0

    # Existing methods below
    
    def generate_basic_report(self, target: str, scan_results: dict = None) -> dict:
        """Generate a basic security report"""
        subdomains = Subdomain.objects.filter(domain=target)
        port_scans = PortScan.objects.filter(host=target)
        vulnerabilities = Vulnerability.objects.filter(target=target, is_fixed=False)
        open_ports = self._get_open_ports(target)

        # Use provided scan_results if available
        if scan_results and 'vulnerabilities' in scan_results:
            # Combine DB findings with scan results if available
            vuln_count = len(scan_results['vulnerabilities'])
            self.logger.info(f"Incorporating {vuln_count} vulnerabilities from scan results")
            
            # Process each vulnerability to ensure it's in the database
            for vuln_data in scan_results['vulnerabilities']:
                try:
                    # Create or update vulnerability in the database
                    vuln_name = vuln_data.get('name', 'Unknown Vulnerability')
                    vuln_severity = vuln_data.get('severity', 'LOW')
                    
                    # Normalize severity
                    if isinstance(vuln_severity, str):
                        vuln_severity = vuln_severity.upper()
                    
                    Vulnerability.objects.update_or_create(
                        target=target,
                        name=vuln_name,
                        defaults={
                            'description': vuln_data.get('description', ''),
                            'severity': vuln_severity,
                            'vuln_type': vuln_data.get('type', 'unknown'),
                            'evidence': vuln_data.get('evidence', ''),
                            'source': vuln_data.get('source', 'scan'),
                            'confidence': vuln_data.get('confidence', 'medium'),
                            'cvss_score': vuln_data.get('cvss_score', 0.0),
                            'is_fixed': False
                        }
                    )
                except Exception as e:
                    self.logger.error(f"Error saving vulnerability from scan results: {str(e)}")
            
            # Refresh vulnerabilities from database to include the ones we just added
            vulnerabilities = Vulnerability.objects.filter(target=target, is_fixed=False)

        # Create the report structure
        report = {
            'target': target,
            'scan_date': datetime.now().isoformat(),
            'summary': {
                'total_subdomains': subdomains.count(),
                'total_ports_scanned': port_scans.count(),
                'total_vulnerabilities': vulnerabilities.count(),
                'open_ports_count': len(open_ports)
            },
            'subdomains': [model_to_dict(sub, exclude=['id']) for sub in subdomains],
            'open_ports': open_ports,
            'port_scan_summary': {
                'total_scanned': port_scans.count(),
                'open': port_scans.filter(state='open').count(),
                'closed': port_scans.filter(state='closed').count(),
                'filtered': port_scans.filter(state='filtered').count()
            },
            'vulnerabilities': [self._serialize_vulnerability(vuln) for vuln in vulnerabilities]
        }
        
        # Log key metrics
        self.logger.info(f"Generated basic report for {target} with {vulnerabilities.count()} vulnerabilities")
        
        return report

    def generate_detailed_report(self, target: str, scan_results: dict = None) -> dict:
        """Generate a detailed security report"""
        basic_report = self.generate_basic_report(target, scan_results)
        vulnerabilities = Vulnerability.objects.filter(target=target, is_fixed=False)
        
        # Enhanced port analysis
        open_ports = self._get_open_ports(target)
        port_risks = self._analyze_port_risks(open_ports)
        
        # Calculate vulnerability severity counts
        vulnerability_severity = {
            'critical': vulnerabilities.filter(severity='CRITICAL').count(),
            'high': vulnerabilities.filter(severity='HIGH').count(),
            'medium': vulnerabilities.filter(severity='MEDIUM').count(),
            'low': vulnerabilities.filter(severity='LOW').count()
        }
        
        # Calculate source counts
        vulnerability_sources = {
            'internal': vulnerabilities.filter(source='internal').count(),
            'zap': vulnerabilities.filter(source='zap').count(),
            'nuclei': vulnerabilities.filter(source='nuclei').count(),
            'multiple': vulnerabilities.exclude(source__in=['internal', 'zap', 'nuclei']).count()
        }
        
        detailed_info = {
            'port_analysis': {
                'high_risk_ports': port_risks['high_risk'],
                'medium_risk_ports': port_risks['medium_risk'],
                'low_risk_ports': port_risks['low_risk']
            },
            'vulnerability_severity': vulnerability_severity,
            'vulnerability_sources': vulnerability_sources
        }
        
        # Log key metrics
        self.logger.info(f"Vulnerability severity counts: {vulnerability_severity}")
        
        return {**basic_report, 'detailed_info': detailed_info}

    def generate_executive_report(self, target: str, scan_results: dict = None) -> dict:
        """Generate an executive summary report"""
        basic_report = self.generate_basic_report(target, scan_results)
        vulnerabilities = Vulnerability.objects.filter(target=target, is_fixed=False)
        high_vulns = vulnerabilities.filter(severity='HIGH')
        critical_vulns = vulnerabilities.filter(severity='CRITICAL')
        
        # Enhanced metrics
        risk_metrics = {
            'critical_severity_vulns': critical_vulns.count(),
            'high_severity_vulns': high_vulns.count(),
            'open_ports': len(basic_report['open_ports']),
            'total_vulnerabilities': vulnerabilities.count(),
            'high_risk_ports': len(self._analyze_port_risks(basic_report['open_ports'])['high_risk'])
        }
        
        # Combine critical and high vulnerabilities in findings
        top_findings = []
        for vuln in critical_vulns:
            top_findings.append(self._serialize_vulnerability(vuln))
        if len(top_findings) < 5:  # Limit to 5 findings total
            for vuln in high_vulns[:5-len(top_findings)]:
                top_findings.append(self._serialize_vulnerability(vuln))
        
        executive_summary = {
            'risk_level': self._calculate_risk_level(risk_metrics),
            'critical_findings': top_findings,
            'risk_metrics': risk_metrics,
            'recommendations': self._generate_recommendations(risk_metrics, basic_report)
        }
        
        # Log key metrics
        self.logger.info(f"Executive report metrics: Critical={critical_vulns.count()}, High={high_vulns.count()}, Total={vulnerabilities.count()}")
        
        return {**basic_report, 'executive_summary': executive_summary}

    def _serialize_port_scan(self, port_scan):
        """Serialize port scan data"""
        return {
            'port': port_scan.port,
            'service': port_scan.service,
            'state': port_scan.state,
            'protocol': port_scan.protocol,
            'banner': port_scan.banner if port_scan.banner else '',
            'scan_date': port_scan.scan_date.isoformat()
        }

    def _get_open_ports(self, target: str) -> list:
        """Get all open ports with details"""
        open_ports = PortScan.objects.filter(
            host=target,
            state='open'
        ).order_by('port')
        return [self._serialize_port_scan(port) for port in open_ports]

    def _analyze_port_risks(self, open_ports: list) -> dict:
        """Analyze risks associated with open ports"""
        high_risk_ports = [21, 23, 445, 3389]  # FTP, Telnet, SMB, RDP
        medium_risk_ports = [22, 25, 110, 143]  # SSH, SMTP, POP3, IMAP
        
        port_risks = {
            'high_risk': [],
            'medium_risk': [],
            'low_risk': []
        }
        
        for port_data in open_ports:
            port = port_data['port']
            if port in high_risk_ports:
                port_risks['high_risk'].append(port_data)
            elif port in medium_risk_ports:
                port_risks['medium_risk'].append(port_data)
            else:
                port_risks['low_risk'].append(port_data)
                
        return port_risks

    def _calculate_risk_level(self, metrics):
        """Calculate overall risk level based on metrics"""
        if metrics['critical_severity_vulns'] > 0:
            return 'CRITICAL'
        elif metrics['high_severity_vulns'] > 2:
            return 'HIGH'
        elif metrics['high_severity_vulns'] > 0 or metrics['high_risk_ports'] > 0:
            return 'MEDIUM'
        else:
            return 'LOW'
    
    def _generate_recommendations(self, metrics, basic_report):
        """Generate security recommendations based on findings"""
        recommendations = []
        
        # Add recommendations based on findings
        if metrics['critical_severity_vulns'] > 0 or metrics['high_severity_vulns'] > 0:
            recommendations.append({
                'title': 'Address High and Critical Vulnerabilities',
                'description': 'Fix identified critical and high vulnerabilities as a priority.'
            })
            
        if metrics['high_risk_ports'] > 0:
            recommendations.append({
                'title': 'Secure High Risk Ports',
                'description': 'Restrict access to high risk services or replace with more secure alternatives.'
            })
            
        # Always add general recommendations
        recommendations.append({
            'title': 'Regular Security Testing',
            'description': 'Perform regular security assessments to identify new vulnerabilities.'
        })
        
        return recommendations
    
    def _serialize_vulnerability(self, vuln):
        """Properly serialize a vulnerability instance with improved formatting"""
        
        # Clean up description to remove repetition
        description = self._clean_description(vuln.description)
        
        # Clean up evidence to remove repetition
        evidence = self._clean_evidence(vuln.evidence)
        
        return {
            'id': vuln.id,
            'target': vuln.target,
            'name': vuln.name,
            'description': description,
            'severity': vuln.severity,
            'vuln_type': vuln.vuln_type,
            'type': vuln.vuln_type,  # Add duplicate field for template compatibility
            'evidence': evidence,
            'discovery_date': vuln.discovery_date.isoformat(),
            'is_fixed': vuln.is_fixed,
            'fix_date': vuln.fix_date.isoformat() if vuln.fix_date else None,
            'source': vuln.source,
            'confidence': vuln.confidence,
            'cvss_score': vuln.cvss_score,
            'solution': vuln.solution if vuln.solution else '',
            'references': list(vuln.references) if vuln.references else [],
            'cwe': vuln.cwe if vuln.cwe else '',
            'metadata': dict(vuln.metadata) if vuln.metadata else {}
        }

# File: reporting/report_generator.py

    def _clean_description(self, description):
        """Clean up repetitive content in descriptions"""
        if not description:
            return ""
        
        # Remove prefixes like "zap:" that appear at the beginning
        if description.startswith("zap:"):
            description = description[4:].strip()
        
        # Split by paragraphs first (handles cases like the CSP text)
        paragraphs = description.split('\n\n')
        
        # Store unique paragraphs preserving order
        unique_paragraphs = []
        seen_paragraphs = set()
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            # Use the first 100 chars as a fingerprint to identify similar paragraphs
            fingerprint = paragraph[:100].lower()
            
            if fingerprint not in seen_paragraphs:
                unique_paragraphs.append(paragraph)
                seen_paragraphs.add(fingerprint)
        
        # For each paragraph, clean duplicate sentences
        cleaned_paragraphs = []
        for paragraph in unique_paragraphs:
            # Split by sentences
            sentences = paragraph.split('. ')
            
            # Remove duplicate sentences
            unique_sentences = []
            seen_sentences = set()
            
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                
                # Create a fingerprint for the sentence
                fingerprint = sentence.lower()
                
                # Only add if not already seen
                if fingerprint not in seen_sentences:
                    unique_sentences.append(sentence)
                    seen_sentences.add(fingerprint)
            
            # Rejoin sentences
            cleaned_paragraph = '. '.join(unique_sentences)
            if not cleaned_paragraph.endswith('.'):
                cleaned_paragraph += '.'
            
            cleaned_paragraphs.append(cleaned_paragraph)
        
        # Join the cleaned paragraphs, limit to a reasonable length
        result = '\n\n'.join(cleaned_paragraphs)
        
        # If still too long, truncate with an indicator
        if len(result) > 2000:
            result = result[:1997] + '...'
            
        return result

    def _clean_evidence(self, evidence):
        """Clean up repetitive content in evidence"""
        if not evidence:
            return ""
        
        # Split by newlines
        lines = evidence.split('\n')
        
        # Store unique evidence items with counts
        unique_lines = []
        seen_patterns = {}
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Extract the pattern (e.g., "nginx/1.19.0" from "zap: nginx/1.19.0")
            if ': ' in line:
                prefix, pattern = line.split(': ', 1)
                processed_line = f"{prefix}: {pattern}"
            else:
                prefix, pattern = "", line
                processed_line = line
            
            # Use lowercase for matching but preserve original case for display
            pattern_key = pattern.lower()
            
            # Track this pattern
            if pattern_key in seen_patterns:
                seen_patterns[pattern_key]['count'] += 1
                
                # Only keep a maximum of 2 examples per pattern
                if seen_patterns[pattern_key]['count'] <= 2:
                    unique_lines.append(processed_line)
            else:
                seen_patterns[pattern_key] = {
                    'count': 1,
                    'line': processed_line
                }
                unique_lines.append(processed_line)
        
        # Add counts for patterns with more than 2 occurrences
        result_lines = []
        
        # First add all the unique lines we want to keep
        for line in unique_lines:
            result_lines.append(line)
        
        # Then add summary counts for patterns with more occurrences
        for pattern, info in seen_patterns.items():
            if info['count'] > 2:
                extra_count = info['count'] - 2
                if extra_count > 0:
                    # Extract prefix from the line to maintain consistency
                    if ': ' in info['line']:
                        prefix = info['line'].split(': ', 1)[0]
                        result_lines.append(f"{prefix}: ... and {extra_count} more similar items")
                    else:
                        result_lines.append(f"... and {extra_count} more similar items")
        
        # Rejoin lines, limit overall size
        result = '\n'.join(result_lines)
        
        # If still too long, truncate
        if len(result) > 500:
            result = result[:497] + '...'
            
        return result
        
    def _similarity(self, str1, str2):
        """Calculate similarity between two strings"""
        # Simple similarity check based on word overlap
        words1 = set(str1.lower().split())
        words2 = set(str2.lower().split())
        
        if not words1 or not words2:
            return 0.0
            
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)# automation/models.py

from django.db import models
from django.utils import timezone

class ScanWorkflow(models.Model):
    """
    Represents a complete security scanning workflow
    """
    STATUS_CHOICES = [
        ('pending', 'Pending'),
        ('scheduled', 'Scheduled'),
        ('in_progress', 'In Progress'),
        ('completed', 'Completed'),
        ('failed', 'Failed'),
        ('canceled', 'Canceled')
    ]
    
    PROFILE_CHOICES = [
        ('quick', 'Quick Scan'),
        ('standard', 'Standard Scan'),
        ('full', 'Full Scan')
    ]
    
    name = models.CharField(max_length=255)
    target = models.CharField(max_length=255)
    scan_profile = models.CharField(max_length=20, choices=PROFILE_CHOICES, default='standard')
    status = models.CharField(max_length=20, choices=STATUS_CHOICES, default='pending')
    
    # Timing information
    created_at = models.DateTimeField(auto_now_add=True)
    scheduled_time = models.DateTimeField(null=True, blank=True)
    start_time = models.DateTimeField(null=True, blank=True)
    end_time = models.DateTimeField(null=True, blank=True)
    
    # Notification settings
    notification_email = models.EmailField(null=True, blank=True)
    
    # Additional metadata
    metadata = models.JSONField(default=dict, blank=True)
    
    class Meta:
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['status']),
            models.Index(fields=['target']),
            models.Index(fields=['created_at']),
        ]
    
    def __str__(self):
        return f"{self.name} - {self.target} ({self.status})"
    
    @property
    def duration(self):
        """Calculate workflow duration in seconds"""
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return None
    
    @property
    def is_active(self):
        """Check if workflow is active (not in terminal state)"""
        return self.status in ['pending', 'scheduled', 'in_progress']
    
    @property
    def is_scheduled(self):
        """Check if workflow is scheduled for future execution"""
        return self.status == 'scheduled' and self.scheduled_time and self.scheduled_time > timezone.now()


class ScanTask(models.Model):
    """
    Represents an individual task within a scanning workflow
    """
    STATUS_CHOICES = [
        ('pending', 'Pending'),
        ('in_progress', 'In Progress'),
        ('completed', 'Completed'),
        ('failed', 'Failed'),
        ('skipped', 'Skipped'),
        ('canceled', 'Canceled')
    ]
    
    TASK_TYPE_CHOICES = [
        ('subdomain_enumeration', 'Subdomain Enumeration'),
        ('port_scanning', 'Port Scanning'),
        ('service_identification', 'Service Identification'),
        ('vulnerability_scanning', 'Vulnerability Scanning'),
        ('network_mapping', 'Network Mapping'),
        ('report_generation', 'Report Generation')
    ]
    
    workflow = models.ForeignKey(ScanWorkflow, on_delete=models.CASCADE, related_name='tasks')
    task_type = models.CharField(max_length=50, choices=TASK_TYPE_CHOICES)
    name = models.CharField(max_length=255)
    status = models.CharField(max_length=20, choices=STATUS_CHOICES, default='pending')
    
    # Task dependencies
    dependencies = models.ManyToManyField('self', symmetrical=False, related_name='dependents', blank=True)
    order = models.IntegerField(default=0)
    
    # Timing information
    created_at = models.DateTimeField(auto_now_add=True)
    start_time = models.DateTimeField(null=True, blank=True)
    end_time = models.DateTimeField(null=True, blank=True)
    
    # Results
    result = models.TextField(null=True, blank=True)
    
    class Meta:
        ordering = ['workflow', 'order']
        indexes = [
            models.Index(fields=['workflow', 'status']),
            models.Index(fields=['task_type']),
        ]
    
    def __str__(self):
        return f"{self.name} ({self.status})"
    
    @property
    def duration(self):
        """Calculate task duration in seconds"""
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return None
    
    @property
    def has_dependencies(self):
        """Check if task has dependencies"""
        return self.dependencies.exists()
    
    @property
    def is_blocked(self):
        """Check if any dependencies are not completed"""
        return self.dependencies.exclude(status='completed').exists()


class Notification(models.Model):
    """
    Represents a notification sent to a user
    """
    NOTIFICATION_TYPE_CHOICES = [
        ('workflow_scheduled', 'Workflow Scheduled'),
        ('workflow_started', 'Workflow Started'),
        ('workflow_completed', 'Workflow Completed'),
        ('workflow_failed', 'Workflow Failed'),
        ('workflow_canceled', 'Workflow Canceled'),
        ('task_failed', 'Task Failed'),
        ('critical_vulnerabilities', 'Critical Vulnerabilities'),
        ('report_ready', 'Report Ready')
    ]
    
    workflow = models.ForeignKey(ScanWorkflow, on_delete=models.CASCADE, related_name='notifications')
    notification_type = models.CharField(max_length=50, choices=NOTIFICATION_TYPE_CHOICES)
    recipient = models.EmailField()
    subject = models.CharField(max_length=255)
    message = models.TextField()
    
    created_at = models.DateTimeField(auto_now_add=True)
    sent = models.BooleanField(default=False)
    sent_time = models.DateTimeField(null=True, blank=True)
    
    # For tracking email opens, clicks, etc.
    metadata = models.JSONField(default=dict, blank=True)
    
    class Meta:
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['notification_type']),
            models.Index(fields=['created_at']),
            models.Index(fields=['sent']),
        ]
    
    def __str__(self):
        return f"{self.notification_type} for {self.workflow.name}"


class ScheduledTask(models.Model):
    """
    Represents a recurring scheduled task/scan
    """
    FREQUENCY_CHOICES = [
        ('daily', 'Daily'),
        ('weekly', 'Weekly'),
        ('monthly', 'Monthly'),
        ('custom', 'Custom')
    ]
    
    name = models.CharField(max_length=255)
    target = models.CharField(max_length=255)
    scan_profile = models.CharField(max_length=20, choices=ScanWorkflow.PROFILE_CHOICES, default='standard')
    
    # Schedule
    frequency = models.CharField(max_length=20, choices=FREQUENCY_CHOICES)
    cron_expression = models.CharField(max_length=100, null=True, blank=True)
    start_date = models.DateField()
    end_date = models.DateField(null=True, blank=True)
    
    # Active flag
    is_active = models.BooleanField(default=True)
    
    # Notification settings
    notification_email = models.EmailField(null=True, blank=True)
    
    # Created by
    created_at = models.DateTimeField(auto_now_add=True)
    created_by = models.CharField(max_length=100, null=True, blank=True)
    
    # Last execution
    last_execution = models.DateTimeField(null=True, blank=True)
    last_status = models.CharField(max_length=20, null=True, blank=True)
    last_workflow = models.ForeignKey(ScanWorkflow, on_delete=models.SET_NULL, null=True, blank=True, related_name='schedule')
    
    class Meta:
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['is_active']),
            models.Index(fields=['frequency']),
            models.Index(fields=['target']),
        ]
    
    def __str__(self):
        return f"{self.name} - {self.target} ({self.frequency})"# automation/processor.py

import threading
import time
import logging
from django.conf import settings
from django.db import connection

from .workflow_orchestrator import WorkflowOrchestrator
from .scheduler import ScanScheduler

logger = logging.getLogger(__name__)

class AutomationProcessor:
    """
    Process automation workflows and scheduled tasks in the background.
    Implemented as a singleton to ensure only one instance is running.
    """
    _instance = None
    _lock = threading.Lock()
    
    @classmethod
    def get_instance(cls):
        """Get or create the singleton instance"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = cls()
        return cls._instance
    
    def __init__(self):
        self.orchestrator = WorkflowOrchestrator()
        self.scheduler = ScanScheduler()
        self.stop_flag = threading.Event()
        self.processing_thread = None
        self.interval = getattr(settings, 'AUTOMATION_PROCESSING_INTERVAL', 60)  # seconds
    
    def start(self):
        """Start the background processing thread if not already running"""
        if self.processing_thread and self.processing_thread.is_alive():
            logger.warning("Automation processor already running")
            return False
            
        self.stop_flag.clear()
        self.processing_thread = threading.Thread(
            target=self._processing_loop,
            daemon=True
        )
        self.processing_thread.start()
        logger.info("Automation processor started")
        return True
    
    def stop(self):
        """Stop the background processing thread"""
        if not self.processing_thread or not self.processing_thread.is_alive():
            logger.warning("Automation processor not running")
            return False
            
        self.stop_flag.set()
        self.processing_thread.join(timeout=10)
        logger.info("Automation processor stopped")
        return True
    
    def is_running(self):
        """Check if the processor is running"""
        return self.processing_thread is not None and self.processing_thread.is_alive()
    
    def _processing_loop(self):
        """Main processing loop for automation tasks"""
        logger.info("Automation processing loop started")
        
        while not self.stop_flag.is_set():
            try:
                # Process scheduled tasks
                scheduled_count = self.scheduler.process_scheduled_tasks()
                if scheduled_count > 0:
                    logger.info(f"Processed {scheduled_count} scheduled tasks")
                
                # Start pending workflows
                started_count = self.orchestrator.check_pending_workflows()
                if started_count > 0:
                    logger.info(f"Started {started_count} pending workflows")
                
                # Process workflow queue
                processed_count = self.orchestrator.process_workflow_queue()
                if processed_count > 0:
                    logger.info(f"Processed {processed_count} workflow tasks")
                
            except Exception as e:
                logger.error(f"Error in automation processing: {str(e)}")
            finally:
                # Close database connections to prevent connection leaks
                connection.close()
            
            # Sleep until next interval
            self.stop_flag.wait(self.interval)
        
        logger.info("Automation processing loop stopped")
    
    @classmethod
    def run_once(cls):
        """
        Run a single cycle of the processor, useful for cron jobs or manual triggers
        """
        processor = cls()
        
        try:
            # Process scheduled tasks
            scheduled_count = processor.scheduler.process_scheduled_tasks()
            
            # Start pending workflows
            started_count = processor.orchestrator.check_pending_workflows()
            
            # Process workflow queue
            processed_count = processor.orchestrator.process_workflow_queue()
            
            return {
                'scheduled_tasks_processed': scheduled_count,
                'workflows_started': started_count,
                'tasks_processed': processed_count
            }
            
        except Exception as e:
            logger.error(f"Error in one-time automation processing: {str(e)}")
            return {
                'error': str(e)
            }
        finally:
            # Close database connections
            connection.close()# automation/workflow_orchestrator.py

import logging
import json
import socket
import time
from datetime import datetime, timedelta
from typing import Dict, List
from django.utils import timezone
from django.db import transaction
from django.conf import settings
from urllib.parse import urlparse

from reconnaissance.subdomain_enumerator import SubdomainEnumerator
from reconnaissance.scanner import PortScanner
from reconnaissance.service_identifier import ServiceIdentifier
from vulnerability.unified_scanner import UnifiedVulnerabilityScanner
from network_visualization.topology_mapper import TopologyMapper
from reporting.report_generator import ReportGenerator
from .models import ScanWorkflow, ScanTask, Notification
from .notification_manager import NotificationManager

logger = logging.getLogger(__name__)

class WorkflowOrchestrator:
    """
    Orchestrates the complete scanning workflow including reconnaissance,
    vulnerability scanning, network visualization, and report generation.
    
    Supports automatic scheduling, task dependencies, and failure handling.
    """
    
    # Workflow task types
    TASK_TYPES = {
        'subdomain_enumeration': 'Subdomain Enumeration',
        'port_scanning': 'Port Scanning',
        'service_identification': 'Service Identification',
        'vulnerability_scanning': 'Vulnerability Scanning',
        'network_mapping': 'Network Mapping',
        'report_generation': 'Report Generation'
    }
    
    # Task dependencies - keys depend on values
    TASK_DEPENDENCIES = {
        'port_scanning': ['subdomain_enumeration'],
        'service_identification': ['port_scanning'],
        'vulnerability_scanning': ['service_identification'],
        'network_mapping': ['service_identification'],
        'report_generation': ['vulnerability_scanning', 'network_mapping']
    }
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.subdomain_enumerator = SubdomainEnumerator()
        self.port_scanner = PortScanner()
        self.service_identifier = ServiceIdentifier()
        self.vulnerability_scanner = UnifiedVulnerabilityScanner()
        self.topology_mapper = TopologyMapper()
        self.report_generator = ReportGenerator()
        self.notification_manager = NotificationManager()
    
    def parse_target_url(self, url: str) -> str:
        """
        Parse a target URL and extract just the hostname.
        Handles various URL formats including those with protocols, paths, query strings, etc.
        
        Args:
            url: The target URL to parse
            
        Returns:
            str: The clean hostname
        """
        # Handle empty values
        if not url:
            return ""
        
        try:
            # Remove protocol if present by using urlparse
            parsed = urlparse(url)
            
            # If netloc is empty, the URL might not have a protocol
            if not parsed.netloc:
                # Try adding a protocol and parsing again
                parsed = urlparse(f"http://{url}")
            
            # Extract just the hostname (netloc without port)
            hostname = parsed.netloc
            if ':' in hostname:
                hostname = hostname.split(':', 1)[0]
                
            # If still empty, return the original url as a last resort
            if not hostname:
                return url
                
            return hostname
        except Exception as e:
            self.logger.error(f"Error parsing URL {url}: {str(e)}")
            # Return the original URL if parsing fails
            return url
    
    def setup_workflow(self, workflow, target: str, scan_profile: str = 'standard'):
        """
        Set up tasks for an existing workflow
        
        Args:
            workflow: The existing ScanWorkflow object
            target: The domain or IP to scan
            scan_profile: Scan intensity level (quick, standard, full)
            
        Returns:
            Updated workflow
        """
        with transaction.atomic():
            # Create tasks with proper dependencies
            task_ids = {}
            
            # Create all tasks first
            for task_type, task_name in self.TASK_TYPES.items():
                task = ScanTask.objects.create(
                    workflow=workflow,
                    task_type=task_type,
                    name=f"{task_name} - {target}",
                    status='pending',
                    order=list(self.TASK_TYPES.keys()).index(task_type)
                )
                task_ids[task_type] = task.id
            
            # Set dependencies
            for task_type, dependencies in self.TASK_DEPENDENCIES.items():
                task = ScanTask.objects.get(id=task_ids[task_type])
                for dependency in dependencies:
                    dependency_task = ScanTask.objects.get(id=task_ids[dependency])
                    task.dependencies.add(dependency_task)
                task.save()
                
            # If scheduled for the future, create a notification 
            if workflow.scheduled_time and workflow.notification_email:
                Notification.objects.create(
                    workflow=workflow,
                    notification_type='workflow_scheduled',
                    recipient=workflow.notification_email,
                    subject=f"Scan scheduled: {workflow.name}",
                    message=f"A security scan for {target} has been scheduled to start at {workflow.scheduled_time}."
                )
                
            return workflow
        
    def _complete_workflow(self, workflow: ScanWorkflow) -> None:
        """Mark workflow as completed and send notifications"""
        workflow.status = 'completed'
        workflow.end_time = timezone.now()
        workflow.save()
        
        self.logger.info(f"Workflow {workflow.id} for {workflow.target} completed successfully")
        
        # Send completion notification
        if workflow.notification_email:
            self.notification_manager.send_workflow_completion_notification(workflow)
                
    def create_workflow(self, target: str, name: str = None, scan_profile: str = 'standard',
                      scheduled_time: datetime = None, notify_email: str = None) -> ScanWorkflow:
        """
        Create a new scanning workflow for a target
        
        Args:
            target: The domain or IP to scan
            name: Optional name for this workflow
            scan_profile: Scan intensity level (quick, standard, full)
            scheduled_time: When to start the scan (None = immediate)
            notify_email: Email to notify when scan completes
            
        Returns:
            ScanWorkflow object
        """
        if not name:
            name = f"Scan {target} - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
            
        with transaction.atomic():
            # Create the workflow
            workflow = ScanWorkflow.objects.create(
                name=name,
                target=target,
                scan_profile=scan_profile,
                scheduled_time=scheduled_time,
                status='scheduled' if scheduled_time else 'pending',
                notification_email=notify_email
            )
            
            # Create tasks with proper dependencies
            task_ids = {}
            
            # Create all tasks first
            for task_type, task_name in self.TASK_TYPES.items():
                task = ScanTask.objects.create(
                    workflow=workflow,
                    task_type=task_type,
                    name=f"{task_name} - {target}",
                    status='pending',
                    order=list(self.TASK_TYPES.keys()).index(task_type)
                )
                task_ids[task_type] = task.id
            
            # Set dependencies
            for task_type, dependencies in self.TASK_DEPENDENCIES.items():
                task = ScanTask.objects.get(id=task_ids[task_type])
                for dependency in dependencies:
                    dependency_task = ScanTask.objects.get(id=task_ids[dependency])
                    task.dependencies.add(dependency_task)
                task.save()
                
            # If scheduled for the future, create a notification 
            if scheduled_time and notify_email:
                Notification.objects.create(
                    workflow=workflow,
                    notification_type='workflow_scheduled',
                    recipient=notify_email,
                    subject=f"Scan scheduled: {name}",
                    message=f"A security scan for {target} has been scheduled to start at {scheduled_time}."
                )
                
            return workflow
    
    def start_workflow(self, workflow_id: int) -> bool:
        """
        Start a workflow by ID
        
        Args:
            workflow_id: ID of the workflow to start
            
        Returns:
            bool: True if successfully started
        """
        try:
            workflow = ScanWorkflow.objects.get(id=workflow_id)
            
            # Check if it's time to start a scheduled workflow
            if workflow.status == 'scheduled' and workflow.scheduled_time:
                now = timezone.now()
                if now < workflow.scheduled_time:
                    self.logger.info(f"Workflow {workflow_id} is scheduled for {workflow.scheduled_time}, not starting yet")
                    return False
            
            # Update workflow status
            workflow.status = 'in_progress'
            workflow.start_time = timezone.now()
            workflow.save()
            
            self.logger.info(f"Starting workflow {workflow_id} for target {workflow.target}")
            
            # Get tasks with no dependencies (entry points)
            entry_tasks = ScanTask.objects.filter(
                workflow=workflow, 
                dependencies__isnull=True
            ).order_by('order')
            
            # Start entry tasks
            for task in entry_tasks:
                self._execute_task(task)
                
            return True
            
        except ScanWorkflow.DoesNotExist:
            self.logger.error(f"Workflow {workflow_id} not found")
            return False
        except Exception as e:
            self.logger.error(f"Error starting workflow {workflow_id}: {str(e)}")
            return False
    
    def check_pending_workflows(self) -> int:
        """
        Check for scheduled workflows that should be started
        
        Returns:
            int: Number of workflows started
        """
        now = timezone.now()
        
        # Find scheduled workflows that should start now
        scheduled_workflows = ScanWorkflow.objects.filter(
            status='scheduled',
            scheduled_time__lte=now
        )
        
        count = 0
        for workflow in scheduled_workflows:
            if self.start_workflow(workflow.id):
                count += 1
                
        return count
    
    def process_workflow_queue(self) -> int:
        """
        Process the workflow queue - check for tasks that can be started
        
        Returns:
            int: Number of tasks started
        """
        # Find in-progress workflows
        active_workflows = ScanWorkflow.objects.filter(
            status='in_progress'
        )
        
        tasks_started = 0
        
        for workflow in active_workflows:
            # Get pending tasks for this workflow
            pending_tasks = ScanTask.objects.filter(
                workflow=workflow,
                status='pending'
            )
            
            for task in pending_tasks:
                # Check if all dependencies are completed
                dependencies = task.dependencies.all()
                all_completed = all(dep.status == 'completed' for dep in dependencies)
                
                if all_completed:
                    self._execute_task(task)
                    tasks_started += 1
            
            # Check if workflow is complete
            if not ScanTask.objects.filter(workflow=workflow).exclude(status='completed').exists():
                self._complete_workflow(workflow)
                
        return tasks_started
    
    def _execute_task(self, task: ScanTask) -> None:
        """
        Execute a specific workflow task
        
        Args:
            task: The task to execute
        """
        try:
            # Update task status
            task.status = 'in_progress'
            task.start_time = timezone.now()
            task.save()
            
            self.logger.info(f"Executing task {task.id} ({task.task_type}) for workflow {task.workflow.id}")
            
            # Execute appropriate task type
            if task.task_type == 'subdomain_enumeration':
                result = self._run_subdomain_enumeration(task)
            elif task.task_type == 'port_scanning':
                result = self._run_port_scanning(task)
            elif task.task_type == 'service_identification':
                result = self._run_service_identification(task)
            elif task.task_type == 'vulnerability_scanning':
                result = self._run_vulnerability_scanning(task)
            elif task.task_type == 'network_mapping':
                result = self._run_network_mapping(task)
            elif task.task_type == 'report_generation':
                result = self._run_report_generation(task)
            else:
                raise ValueError(f"Unknown task type: {task.task_type}")
            
            # Update task status based on result
            if result.get('status') == 'success':
                task.status = 'completed'
                task.result = json.dumps(result)
            else:
                task.status = 'failed'
                task.result = json.dumps({'error': result.get('error', 'Unknown error')})
                
                # Create error notification
                if task.workflow.notification_email:
                    self.notification_manager.send_task_failure_notification(
                        task, result.get('error', 'Unknown error')
                    )
            
            task.end_time = timezone.now()
            task.save()
            
            # Check for critical failures that should stop the workflow
            if task.status == 'failed' and task.task_type in ['subdomain_enumeration', 'port_scanning']:
                self._fail_workflow(task.workflow, f"Critical task {task.task_type} failed")
                
        except Exception as e:
            self.logger.error(f"Error executing task {task.id}: {str(e)}")
            task.status = 'failed'
            task.result = json.dumps({'error': str(e)})
            task.end_time = timezone.now()
            task.save()
            
            # Create error notification
            if task.workflow.notification_email:
                self.notification_manager.send_task_failure_notification(task, str(e))
    
# File: automation/workflow_orchestrator.py
# Update the _run_subdomain_enumeration method to handle subdomain enumeration issues

    def _run_subdomain_enumeration(self, task: ScanTask) -> dict:
        """Run subdomain enumeration task with improved URL handling and reliability"""
        # Get the original target from the task
        original_target = task.workflow.target
        
        # Clean the target URL to get just the domain
        target_url = self.parse_target_url(original_target)
        
        # Remove 'www.' prefix if present for better subdomain enumeration
        if target_url.startswith('www.'):
            search_domain = target_url[4:]  # Remove www. prefix
            self.logger.info(f"Removing www prefix for enumeration, using: {search_domain}")
        else:
            search_domain = target_url
            
        try:
            self.logger.info(f"Starting subdomain enumeration for {search_domain} (original: {original_target})")
            results = self.subdomain_enumerator.enumerate_subdomains(search_domain)
            
            # If no results returned, add the main domain as a fallback
            if not results:
                self.logger.warning(f"No subdomains found for {search_domain}, adding main domain as fallback")
                try:
                    main_ip = socket.gethostbyname(search_domain)
                    results = [{
                        'subdomain': search_domain,
                        'ip_address': main_ip,
                        'is_http': True,
                        'http_status': None,
                        'status': 'active'
                    }]
                except Exception as e:
                    self.logger.error(f"Failed to resolve main domain as fallback: {str(e)}")
                    results = [{
                        'subdomain': search_domain,
                        'ip_address': None,
                        'is_http': None,
                        'http_status': None,
                        'status': 'unknown'
                    }]
            
            # Save results to database to ensure they're available for later steps
            saved_count = 0
            from reconnaissance.models import Subdomain  # Import the model explicitly
            
            for subdomain_data in results:
                # Skip entries without subdomain
                if not subdomain_data.get('subdomain'):
                    continue
                    
                try:
                    sub_obj, created = Subdomain.objects.update_or_create(
                        domain=search_domain,
                        subdomain=subdomain_data['subdomain'],
                        defaults={
                            'ip_address': subdomain_data.get('ip_address'),
                            'is_active': True
                        }
                    )
                    saved_count += 1
                except Exception as save_error:
                    self.logger.error(f"Error saving subdomain: {str(save_error)}")
            
            self.logger.info(f"Saved {saved_count} subdomains to database")
            
            return {
                'status': 'success',
                'target': original_target,  # Return original target for consistency
                'target_domain': search_domain,  # Use the domain without www for lookup
                'subdomains_found': len(results),
                'subdomains': results
            }
        except Exception as e:
            self.logger.error(f"Subdomain enumeration failed: {str(e)}")
            # Create a fallback result with just the main domain
            try:
                # Add the main domain as a subdomain in the database
                from reconnaissance.models import Subdomain  # Import the model explicitly
                
                sub_obj, created = Subdomain.objects.update_or_create(
                    domain=search_domain,
                    subdomain=search_domain,
                    defaults={
                        'ip_address': socket.gethostbyname(search_domain),
                        'is_active': True
                    }
                )
                
                return {
                    'status': 'success',  # Return success to continue workflow
                    'target': original_target,
                    'target_domain': search_domain,
                    'subdomains_found': 1, 
                    'subdomains': [{
                        'subdomain': search_domain,
                        'ip_address': sub_obj.ip_address,
                        'is_http': None,
                        'http_status': None,
                        'status': 'active'
                    }],
                    'warning': f"Error during subdomain scan: {str(e)}. Using main domain only."
                }
            except Exception as fallback_error:
                # Absolute last resort - return minimal data to allow workflow to continue
                return {
                    'status': 'success',  # Return success to continue workflow
                    'target': original_target,
                    'target_domain': search_domain,
                    'subdomains_found': 1,
                    'subdomains': [{
                        'subdomain': search_domain, 
                        'ip_address': None,
                        'is_http': None,
                        'http_status': None,
                        'status': 'unknown'
                    }],
                    'warning': f"Subdomain enumeration failed: {str(e)}. Using main domain as fallback."
                }
    
    def _run_port_scanning(self, task: ScanTask) -> dict:
        """Run port scanning task with improved URL handling and database storage"""
        # Get the original target from the task
        original_target = task.workflow.target
        scan_profile = task.workflow.scan_profile
        
        # Clean the target URL to get just the hostname
        target_url = self.parse_target_url(original_target)
        
        # Map scan profile to scan type
        scan_type = {
            'quick': 'quick',
            'standard': 'partial',
            'full': 'full'
        }.get(scan_profile, 'partial')
        
        self.logger.info(f"Starting port scan for {target_url} (original: {original_target}) with profile: {scan_profile}, type: {scan_type}")
        
        try:
            # Validate target before scanning
            import socket
            try:
                # Try to resolve hostname to ensure it's valid
                socket.gethostbyname(target_url)
            except socket.gaierror:
                self.logger.error(f"Unable to resolve target: {target_url}")
                return {
                    'status': 'error',
                    'error': f"Unable to resolve target: {target_url}. Please check the domain name."
                }
            
            # First run a quick check to see if common ports are open
            # This is more reliable than waiting for nmap
            common_ports = [80, 443, 8080, 8443, 22, 21]
            manual_check_ports = []
            
            for port in common_ports:
                try:
                    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                        sock.settimeout(1)
                        result = sock.connect_ex((target_url, port))
                        if result == 0:
                            manual_check_ports.append(port)
                            self.logger.info(f"Manual port check found open port {port} on {target_url}")
                except Exception as e:
                    self.logger.debug(f"Socket error checking port {port}: {str(e)}")
            
            # Run the actual scan
            results = self.port_scanner.scan(target_url, scan_type)
            
            # Check for success
            if results.get('status') == 'success':
                # Check if the scan found any ports
                ports_found = False
                for host in results.get('results', []):
                    if host.get('ports') and len(host.get('ports', [])) > 0:
                        ports_found = True
                        break
                
                # If we have manual ports but no scan ports, add them to the results
                if not ports_found and manual_check_ports and 'manual_detected' not in results:
                    self.logger.info(f"Adding manually detected ports to scan results: {manual_check_ports}")
                    
                    # Create port data for each manual port
                    manual_ports = []
                    for port in manual_check_ports:
                        service_name = 'https' if port in [443, 8443] else 'http' if port in [80, 8080] else 'unknown'
                        manual_ports.append({
                            'port': port,
                            'state': 'open',
                            'service': service_name,
                            'reason': 'manual check'
                        })
                    
                    # Update the results with our manual ports
                    if len(results.get('results', [])) > 0:
                        results['results'][0]['ports'] = manual_ports
                    else:
                        results['results'] = [{
                            'host': target_url,
                            'state': 'up',
                            'ports': manual_ports
                        }]
                    
                    results['manual_added'] = True
                
                # Save scan results to the database
                from reconnaissance.models import PortScan
                
                # Track which ports we've saved to avoid duplicates
                saved_ports = set()
                saved_count = 0
                
                # Process and save all port results
                for host in results.get('results', []):
                    for port_data in host.get('ports', []):
                        port = port_data.get('port')
                        state = port_data.get('state')
                        service = port_data.get('service', '')
                        
                        # Skip if we already saved this port or if port is invalid
                        if not port or (target_url, port) in saved_ports:
                            continue
                        
                        try:
                            # Create or update port scan record
                            port_scan, created = PortScan.objects.update_or_create(
                                host=target_url,
                                port=port,
                                defaults={
                                    'service': service,
                                    'state': state,
                                    'protocol': 'tcp',
                                    'scan_status': 'completed',
                                    'scan_type': scan_type,
                                    'banner': port_data.get('extrainfo', ''),
                                    'notes': f"Version: {port_data.get('version', 'unknown')}"
                                }
                            )
                            
                            saved_ports.add((target_url, port))
                            saved_count += 1
                            
                            # Check if this port should be flagged as a vulnerability
                            if state == 'open' and service in ['ftp', 'telnet', 'rsh', 'rlogin']:
                                from vulnerability.models import Vulnerability
                                
                                # Create a vulnerability entry for high-risk open ports
                                Vulnerability.objects.get_or_create(
                                    target=target_url,
                                    name=f"Open {service.upper()} Port ({port})",
                                    defaults={
                                        'description': f"Port {port} is open and running {service}, which is potentially insecure.",
                                        'severity': 'HIGH',
                                        'vuln_type': 'open_port',
                                        'evidence': f"Port {port} is open and accessible.",
                                        'source': 'port_scan',
                                        'confidence': 'high',
                                        'cvss_score': 7.5,
                                        'is_fixed': False
                                    }
                                )
                            
                        except Exception as save_error:
                            self.logger.error(f"Error saving port scan result: {str(save_error)}")
                
                self.logger.info(f"Saved {saved_count} port scan results to database")
                
                # Add database save info to results
                results['database_saved'] = {
                    'saved_count': saved_count,
                    'target': target_url
                }
                
                return results
            else:
                error_msg = results.get('error', 'Port scanning failed without specific error')
                
                # If we have manually detected ports, return those instead
                if manual_check_ports:
                    self.logger.info(f"Using manually detected ports after scan error: {manual_check_ports}")
                    
                    # Create port data for each manual port
                    manual_ports = []
                    for port in manual_check_ports:
                        service_name = 'https' if port in [443, 8443] else 'http' if port in [80, 8080] else 'unknown'
                        manual_ports.append({
                            'port': port,
                            'state': 'open',
                            'service': service_name,
                            'reason': 'manual check'
                        })
                    
                    # Save manual results to database
                    from reconnaissance.models import PortScan
                    saved_count = 0
                    
                    for port_data in manual_ports:
                        try:
                            port_scan, created = PortScan.objects.update_or_create(
                                host=target_url,
                                port=port_data['port'],
                                defaults={
                                    'service': port_data['service'],
                                    'state': 'open',
                                    'protocol': 'tcp',
                                    'scan_status': 'completed',
                                    'scan_type': 'manual',
                                    'notes': 'Detected by manual scan'
                                }
                            )
                            saved_count += 1
                        except Exception as save_error:
                            self.logger.error(f"Error saving manual port result: {str(save_error)}")
                    
                    self.logger.info(f"Saved {saved_count} manual port results to database")
                    
                    return {
                        'status': 'success',
                        'scan_info': {
                            'scan_type': 'manual',
                            'command_line': 'Manual port check',
                        },
                        'results': [{
                            'host': target_url,
                            'state': 'up',
                            'ports': manual_ports
                        }],
                        'manual_only': True,
                        'database_saved': {
                            'saved_count': saved_count,
                            'target': target_url
                        }
                    }
                
                self.logger.error(f"Port scanning error: {error_msg}")
                return {
                    'status': 'error',
                    'error': error_msg
                }
        except Exception as e:
            self.logger.error(f"Port scanning failed: {str(e)}")
            return {
                'status': 'error',
                'error': f"Port scanning failed: {str(e)}"
            }
    
    def _run_service_identification(self, task: ScanTask) -> dict:
        """Run service identification task with improved URL handling and database storage"""
        # Get the original target from the task
        original_target = task.workflow.target
        scan_profile = task.workflow.scan_profile
        
        # Clean the target URL to get just the hostname
        target_url = self.parse_target_url(original_target)
        
        # Map scan profile to service ID scan type
        scan_type = {
            'quick': 'quick',
            'standard': 'standard',
            'full': 'standard'  # Use 'standard' for full profile for better reliability
        }.get(scan_profile, 'standard')
        
        # Set time limit based on profile
        time_limit = {
            'quick': 180,    # 3 minutes
            'standard': 300, # 5 minutes
            'full': 600      # 10 minutes 
        }.get(scan_profile, 300)
        
        self.logger.info(f"Starting service identification for {target_url} with type: {scan_type}, timeout: {time_limit}s")
        
        try:
            # First check if we have any port scan results to work with
            port_scan_task = ScanTask.objects.filter(
                workflow=task.workflow,
                task_type='port_scanning',
                status='completed'
            ).first()
            
            if not port_scan_task or not port_scan_task.result:
                self.logger.warning(f"No completed port scan found for service identification")
                
                # Do a quick manual check for common ports
                try:
                    common_ports = [80, 443, 8080, 8443, 22, 21]
                    found_ports = []
                    
                    for port in common_ports:
                        try:
                            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                                sock.settimeout(1)
                                result = sock.connect_ex((target_url, port))
                                if result == 0:
                                    found_ports.append(port)
                        except:
                            pass
                    
                    if found_ports:
                        self.logger.info(f"Manual check found {len(found_ports)} open ports for service identification")
                        # Create simple service details
                        services = []
                        for port in found_ports:
                            service_name = 'https' if port in [443, 8443] else 'http' if port in [80, 8080] else 'unknown'
                            category = 'web' if port in [80, 443, 8080, 8443] else 'other'
                            risk_level = 'LOW' if service_name == 'https' else 'MEDIUM'
                            
                            services.append({
                                'port': port,
                                'protocol': 'tcp',
                                'state': 'open',
                                'service': {
                                    'name': service_name,
                                    'product': '',
                                    'version': '',
                                    'extrainfo': 'Detected by manual scan',
                                },
                                'category': category,
                                'risk_level': risk_level
                            })
                        
                        # Save services to database
                        self._save_services_to_database(target_url, services)
                        
                        return {
                            'status': 'success',
                            'target': original_target,
                            'services': services,
                            'manual_detection': True,
                            'database_saved': True
                        }
                    else:
                        # Return empty results to continue workflow
                        return {
                            'status': 'success',
                            'target': original_target,
                            'services': [],
                            'warning': 'No port scan results available'
                        }
                except Exception as e:
                    self.logger.error(f"Manual port check failed: {str(e)}")
                    # Still return success with empty results to continue workflow
                    return {
                        'status': 'success',
                        'target': original_target,
                        'services': [],
                        'warning': 'No port scan results available'
                    }
            
            # Try to parse port scan results
            try:
                import json
                scan_results = json.loads(port_scan_task.result)
                
                # Check if manually detected ports exist
                if scan_results.get('manual_scan') or scan_results.get('manual_detected') or scan_results.get('manual_only') or scan_results.get('manual_added'):
                    self.logger.info("Using manually detected ports for service identification")
                    
                    services = []
                    for host in scan_results.get('results', []):
                        for port_data in host.get('ports', []):
                            port = port_data.get('port')
                            state = port_data.get('state')
                            service_name = port_data.get('service', 'unknown')
                            
                            if state == 'open':
                                category = 'web' if service_name in ['http', 'https'] else 'other'
                                risk_level = 'LOW' if service_name == 'https' else 'MEDIUM'
                                
                                services.append({
                                    'port': port,
                                    'protocol': 'tcp',
                                    'state': 'open',
                                    'service': {
                                        'name': service_name,
                                        'product': '',
                                        'version': '',
                                        'extrainfo': 'Detected by manual scan',
                                    },
                                    'category': category,
                                    'risk_level': risk_level
                                })
                    
                    # Save services to database
                    self._save_services_to_database(target_url, services)
                    
                    return {
                        'status': 'success',
                        'target': original_target,
                        'services': services,
                        'manual_identification': True,
                        'database_saved': True
                    }
                
                # Regular processing
                if not scan_results.get('results') or not any(host.get('ports') for host in scan_results.get('results', [])):
                    self.logger.warning(f"No open ports found in port scan results")
                    return {
                        'status': 'success',
                        'target': original_target,
                        'services': [],
                        'warning': 'No open ports found for service identification'
                    }
            except Exception as parse_error:
                self.logger.error(f"Error parsing port scan results: {str(parse_error)}")
            
            # Import needed for timeout handling
            import threading
            import time
            import queue
            
            # Create a queue to get results
            result_queue = queue.Queue()
            
            # Define a worker function
            def service_scan_worker():
                try:
                    scan_result = self.service_identifier.identify_services(target_url, scan_type)
                    result_queue.put(scan_result)
                except Exception as e:
                    self.logger.error(f"Service scan worker error: {str(e)}")
                    result_queue.put({
                        'status': 'error',
                        'error': str(e)
                    })
            
            # Start the worker thread
            worker_thread = threading.Thread(target=service_scan_worker)
            worker_thread.daemon = True
            worker_thread.start()
            
            # Wait for result with timeout
            start_time = time.time()
            try:
                result = result_queue.get(timeout=time_limit)
                self.logger.info(f"Service identification completed in {time.time() - start_time:.1f} seconds")
            except queue.Empty:
                self.logger.error(f"Service identification timed out after {time_limit} seconds")
                # Return success with limited info to continue workflow
                return {
                    'status': 'success',
                    'target': original_target,
                    'services': [],
                    'warning': f'Service identification timed out after {time_limit}s'
                }
            
            if result.get('status') == 'success':
                # If target in result doesn't match our original, update it
                if 'target' in result:
                    result['target'] = original_target
                
                # Save services to database
                services = result.get('services', [])
                if services:
                    self._save_services_to_database(target_url, services)
                    result['database_saved'] = True
                
                return result
            else:
                self.logger.error(f"Service identification failed: {result.get('error')}")
                # Even if the scan fails, return success with empty results to continue workflow
                return {
                    'status': 'success',
                    'target': original_target,
                    'services': [],
                    'warning': f"Service identification error: {result.get('error', 'Unknown error')}"
                }
        except Exception as e:
            self.logger.error(f"Service identification failed: {str(e)}")
            # Return success with empty results to continue workflow
            return {
                'status': 'success',
                'target': original_target,
                'services': [],
                'warning': f"Service identification error: {str(e)}"
            }

    def _save_services_to_database(self, target: str, services: List[Dict]) -> int:
        """Save service identification results to database
        
        Args:
            target: The target domain/IP
            services: List of service dictionaries
            
        Returns:
            int: Number of services saved
        """
        if not services:
            return 0
            
        from reconnaissance.models import Service
        from vulnerability.models import Vulnerability
        
        saved_count = 0
        
        # High risk services that should be flagged as vulnerabilities
        high_risk_services = {
            'ftp': 'File Transfer Protocol (FTP)',
            'telnet': 'Telnet Remote Access',
            'rsh': 'Remote Shell (RSH)',
            'rlogin': 'Remote Login (Rlogin)',
            'smb': 'Windows File Sharing (SMB)'
        }
        
        # Medium risk services
        medium_risk_services = {
            'smtp': 'Mail Server (SMTP)',
            'pop3': 'Mail Server (POP3)',
            'vnc': 'VNC Remote Desktop',
            'mysql': 'MySQL Database',
            'mssql': 'Microsoft SQL Server'
        }
        
        for service_data in services:
            try:
                port = service_data.get('port')
                if not port:
                    continue
                    
                # Extract service details
                protocol = service_data.get('protocol', 'tcp')
                state = service_data.get('state', 'open')
                service_info = service_data.get('service', {})
                
                if not service_info:
                    continue
                    
                service_name = service_info.get('name', 'unknown')
                product = service_info.get('product', '')
                version = service_info.get('version', '')
                extra_info = service_info.get('extrainfo', '')
                category = service_data.get('category', 'other')
                risk_level = service_data.get('risk_level', 'MEDIUM')
                
                # Create or update service record
                service_obj, created = Service.objects.update_or_create(
                    host=target,
                    port=port,
                    protocol=protocol,
                    defaults={
                        'name': service_name,
                        'product': product,
                        'version': version,
                        'extra_info': extra_info,
                        'category': category,
                        'risk_level': risk_level,
                        'is_active': True
                    }
                )
                
                saved_count += 1
                
                # Check if this service should be flagged as a vulnerability
                if state == 'open':
                    # For high risk services
                    if service_name.lower() in high_risk_services:
                        service_title = high_risk_services[service_name.lower()]
                        
                        # Create vulnerability entry
                        Vulnerability.objects.get_or_create(
                            target=target,
                            name=f"{service_title} on port {port}",
                            defaults={
                                'description': f"Port {port} is running {service_name}, which is potentially insecure. {product} {version}".strip(),
                                'severity': 'HIGH',
                                'vuln_type': 'insecure_service',
                                'evidence': f"Service detected on port {port}. {extra_info}".strip(),
                                'source': 'service_identification',
                                'confidence': 'high',
                                'cvss_score': 7.5,
                                'is_fixed': False
                            }
                        )
                    
                    # For medium risk services
                    elif service_name.lower() in medium_risk_services:
                        service_title = medium_risk_services[service_name.lower()]
                        
                        # Create vulnerability entry
                        Vulnerability.objects.get_or_create(
                            target=target,
                            name=f"{service_title} on port {port}",
                            defaults={
                                'description': f"Port {port} is running {service_name}, which might pose security risks if not properly configured. {product} {version}".strip(),
                                'severity': 'MEDIUM',
                                'vuln_type': 'potentially_risky_service',
                                'evidence': f"Service detected on port {port}. {extra_info}".strip(),
                                'source': 'service_identification',
                                'confidence': 'medium',
                                'cvss_score': 5.0,
                                'is_fixed': False
                            }
                        )
                        
                    # Flag uncommon open ports    
                    elif port not in [80, 443, 8080, 8443, 22] and port < 1024:
                        # Create vulnerability entry for uncommon open ports
                        Vulnerability.objects.get_or_create(
                            target=target,
                            name=f"Uncommon service on port {port} ({service_name})",
                            defaults={
                                'description': f"Port {port} is open and running {service_name}, which is uncommon and might indicate unnecessary services.",
                                'severity': 'LOW',
                                'vuln_type': 'uncommon_port',
                                'evidence': f"Service {service_name} detected on port {port}.",
                                'source': 'service_identification',
                                'confidence': 'medium',
                                'cvss_score': 3.0,
                                'is_fixed': False
                            }
                        )
                    
            except Exception as e:
                self.logger.error(f"Error saving service to database: {str(e)}")
        
        return saved_count
    
    def _run_vulnerability_scanning(self, task: ScanTask) -> dict:
        """Run vulnerability scanning task with improved URL handling"""
        # Get the original target from the task
        original_target = task.workflow.target
        scan_profile = task.workflow.scan_profile
        
        # Clean the target URL to get just the hostname
        target_url = self.parse_target_url(original_target)
        
        # Determine scanners to use based on scan profile
        include_zap = scan_profile in ['standard', 'full']
        include_nuclei = True  # Always use Nuclei
        nuclei_scan_type = 'advanced' if scan_profile == 'full' else 'basic'
        
        try:
            self.logger.info(f"Starting vulnerability scan for {target_url} (original: {original_target})")
            
            results = self.vulnerability_scanner.scan_target(
                target=target_url,
                scan_type=scan_profile,
                include_zap=include_zap,
                include_nuclei=include_nuclei,
                nuclei_scan_type=nuclei_scan_type,
                use_advanced_correlation=True
            )
            
            if results.get('status') == 'success':
                # Check for critical vulnerabilities
                high_vulns = 0
                critical_vulns = 0
                
                for vuln in results.get('vulnerabilities', []):
                    if vuln.get('severity') == 'CRITICAL':
                        critical_vulns += 1
                    elif vuln.get('severity') == 'HIGH':
                        high_vulns += 1
                
                # Add notification for critical vulnerabilities
                if (critical_vulns > 0 or high_vulns > 2) and task.workflow.notification_email:
                    self.notification_manager.send_critical_vulnerability_notification(
                        task.workflow, critical_vulns, high_vulns
                    )
                
                return results
            else:
                return {
                    'status': 'error',
                    'error': results.get('error', 'Vulnerability scanning failed without specific error')
                }
        except Exception as e:
            self.logger.error(f"Vulnerability scanning failed: {str(e)}")
            return {
                'status': 'error', 
                'error': f"Vulnerability scanning failed: {str(e)}"
            }
    
    def _run_network_mapping(self, task: ScanTask) -> dict:
        """Run network mapping task with improved visualization data"""
        # Get the original target from the task
        original_target = task.workflow.target
        
        # Clean the target URL to get just the hostname
        target_url = self.parse_target_url(original_target)
        
        try:
            self.logger.info(f"Starting network mapping for {target_url} (original: {original_target})")
            
            # Get services from previous tasks to include in the network map
            services_data = []
            try:
                service_task = ScanTask.objects.filter(
                    workflow=task.workflow,
                    task_type='service_identification',
                    status='completed'
                ).first()
                
                if service_task and service_task.result:
                    service_result = json.loads(service_task.result)
                    services_data = service_result.get('services', [])
            except Exception as e:
                self.logger.warning(f"Error fetching service data for network mapping: {str(e)}")
            
            # Get subdomains from previous tasks
            subdomains_data = []
            try:
                subdomain_task = ScanTask.objects.filter(
                    workflow=task.workflow,
                    task_type='subdomain_enumeration',
                    status='completed'
                ).first()
                
                if subdomain_task and subdomain_task.result:
                    subdomain_result = json.loads(subdomain_task.result)
                    subdomains_data = subdomain_result.get('subdomains', [])
            except Exception as e:
                self.logger.warning(f"Error fetching subdomain data for network mapping: {str(e)}")
            
            # Create network map
            results = self.topology_mapper.create_network_map(
                target_url, 
                services=services_data,
                subdomains=subdomains_data
            )
            
            # Get the number of nodes and connections for proper report display
            nodes_count = 0
            connections_count = 0
            
            if results.get('status') == 'success':
                # Try to get network node counts from the database
                from network_visualization.models import NetworkNode, NetworkConnection
                
                try:
                    nodes_count = NetworkNode.objects.filter(domain=target_url, is_active=True).count()
                    connections_count = NetworkConnection.objects.filter(
                        source__domain=target_url,
                        is_active=True
                    ).count()
                    
                    self.logger.info(f"Network map created with {nodes_count} nodes and {connections_count} connections")
                except Exception as db_error:
                    self.logger.error(f"Error counting network nodes from database: {str(db_error)}")
                
                # Add the node and connection counts to the result
                results['nodes'] = nodes_count
                results['connections'] = connections_count
                
                return results
            else:
                return {
                    'status': 'error',
                    'error': results.get('error', 'Network mapping failed without specific error'),
                    'nodes': 0,
                    'connections': 0
                }
        except Exception as e:
            self.logger.error(f"Network mapping failed: {str(e)}")
            return {
                'status': 'error',
                'error': f"Network mapping failed: {str(e)}",
                'nodes': 0,
                'connections': 0
            }
    
    def _run_report_generation(self, task: ScanTask) -> dict:
        """Run report generation task with improved URL handling"""
        # Get the original target from the task
        original_target = task.workflow.target
        
        # Clean the target URL for DB lookups if needed
        target_url = self.parse_target_url(original_target)
        
        scan_profile = task.workflow.scan_profile
        
        # Map scan profile to report type
        report_type = {
            'quick': 'basic',
            'standard': 'detailed',
            'full': 'executive'
        }.get(scan_profile, 'detailed')
        
        try:
            # Get vulnerability scanning task result
            vuln_scan_task = ScanTask.objects.filter(
                workflow=task.workflow,
                task_type='vulnerability_scanning',
                status='completed'
            ).first()
            
            # Parse scan results if available
            scan_results = None
            if vuln_scan_task and vuln_scan_task.result:
                try:
                    scan_results = json.loads(vuln_scan_task.result)
                except:
                    logger.error("Failed to parse vulnerability scan results")
            
            # Use original target for report generation
            logger.info(f"Generating {report_type} report for {original_target}")
            report_html = self.report_generator.generate_report(report_type, original_target, 'html', scan_results)
            
            # Only send a single notification email
            if task.workflow.notification_email:
                self.notification_manager.send_workflow_completion_notification(
                    task.workflow, 
                    report_id=report_html.id
                )
            
            return {
                'status': 'success',
                'target': original_target,
                'report_types': [report_type],
                'report_formats': ['html'],
                'report_ids': {
                    'html': report_html.id
                }
            }
        except Exception as e:
            logger.error(f"Report generation failed: {str(e)}")
            return {
                'status': 'error',
                'error': f"Report generation failed: {str(e)}"
            }
    
    def _fail_workflow(self, workflow: ScanWorkflow, reason: str) -> None:
        """Mark workflow as failed and send notifications"""
        workflow.status = 'failed'
        workflow.end_time = timezone.now()
        workflow.save()
        
        logger.error(f"Workflow {workflow.id} for {workflow.target} failed: {reason}")
        
        # Update pending tasks to skipped
        ScanTask.objects.filter(workflow=workflow, status='pending').update(
            status='skipped',
            result=json.dumps({'skipped_reason': reason})
        )
        
        # Send failure notification
        if workflow.notification_email:
            self.notification_manager.send_workflow_failure_notification(workflow, reason)
    
    def cancel_workflow(self, workflow_id: int) -> bool:
        """
        Cancel a running or scheduled workflow
        
        Args:
            workflow_id: ID of the workflow to cancel
            
        Returns:
            bool: True if successfully canceled
        """
        try:
            workflow = ScanWorkflow.objects.get(id=workflow_id)
            
            if workflow.status in ['completed', 'failed', 'canceled']:
                logger.warning(f"Workflow {workflow_id} already in terminal state: {workflow.status}")
                return False
            
            # Update workflow status
            original_status = workflow.status
            workflow.status = 'canceled'
            workflow.end_time = timezone.now()
            workflow.save()
            
            # Update in-progress tasks to canceled
            ScanTask.objects.filter(workflow=workflow, status='in_progress').update(
                status='canceled',
                end_time=timezone.now(),
                result=json.dumps({'canceled_reason': 'Workflow canceled by user'})
            )
            
            # Update pending tasks to skipped
            ScanTask.objects.filter(workflow=workflow, status='pending').update(
                status='skipped',
                result=json.dumps({'skipped_reason': 'Workflow canceled by user'})
            )
            
            logger.info(f"Workflow {workflow_id} canceled (was {original_status})")
            
            # Send cancellation notification
            if workflow.notification_email:
                self.notification_manager.send_workflow_cancellation_notification(workflow)
                
            return True
            
        except ScanWorkflow.DoesNotExist:
            logger.error(f"Workflow {workflow_id} not found")
            return False
        except Exception as e:
            logger.error(f"Error canceling workflow {workflow_id}: {str(e)}")
            return False
    
    def get_workflow_status(self, workflow_id: int) -> dict:
        """
        Get detailed status of a workflow
        
        Args:
            workflow_id: ID of the workflow
            
        Returns:
            dict: Workflow status details
        """
        try:
            workflow = ScanWorkflow.objects.get(id=workflow_id)
            tasks = ScanTask.objects.filter(workflow=workflow).order_by('order')
            
            # Calculate progress percentage
            total_tasks = tasks.count()
            completed_tasks = tasks.filter(status__in=['completed', 'skipped', 'canceled']).count()
            progress = int(completed_tasks / total_tasks * 100) if total_tasks > 0 else 0
            
            # Format task results
            task_results = []
            for task in tasks:
                result_data = {}
                if task.result:
                    try:
                        result_data = json.loads(task.result)
                    except:
                        result_data = {'error': 'Invalid JSON result'}
                
                task_results.append({
                    'id': task.id,
                    'name': task.name,
                    'type': task.task_type,
                    'status': task.status,
                    'start_time': task.start_time.isoformat() if task.start_time else None,
                    'end_time': task.end_time.isoformat() if task.end_time else None,
                    'duration': str(task.end_time - task.start_time) if task.start_time and task.end_time else None,
                    'result_summary': self._summarize_task_result(task.task_type, result_data)
                })
            
            return {
                'id': workflow.id,
                'name': workflow.name,
                'target': workflow.target,
                'status': workflow.status,
                'scan_profile': workflow.scan_profile,
                'scheduled_time': workflow.scheduled_time.isoformat() if workflow.scheduled_time else None,
                'start_time': workflow.start_time.isoformat() if workflow.start_time else None,
                'end_time': workflow.end_time.isoformat() if workflow.end_time else None,
                'duration': str(workflow.end_time - workflow.start_time) if workflow.start_time and workflow.end_time else None,
                'progress': progress,
                'tasks': task_results,
                'notification_email': workflow.notification_email
            }
            
        except ScanWorkflow.DoesNotExist:
            logger.error(f"Workflow {workflow_id} not found")
            return {'error': 'Workflow not found'}
        except Exception as e:
            logger.error(f"Error getting workflow status: {str(e)}")
            return {'error': str(e)}
    
    def _summarize_task_result(self, task_type: str, result: dict) -> dict:
        """Generate a summary of task results for display"""
        summary = {}
        
        if task_type == 'subdomain_enumeration':
            summary['subdomains_found'] = result.get('subdomains_found', 0)
        elif task_type == 'port_scanning':
            hosts = result.get('results', [])
            open_ports = 0
            for host in hosts:
                open_ports += len([p for p in host.get('ports', []) if p.get('state') == 'open'])
            summary['hosts_scanned'] = len(hosts)
            summary['open_ports'] = open_ports
        elif task_type == 'service_identification':
            summary['services_found'] = len(result.get('services', []))
        elif task_type == 'vulnerability_scanning':
            vulns = result.get('vulnerabilities', [])
            severity_counts = {
                'critical': len([v for v in vulns if v.get('severity') == 'CRITICAL']),
                'high': len([v for v in vulns if v.get('severity') == 'HIGH']),
                'medium': len([v for v in vulns if v.get('severity') == 'MEDIUM']),
                'low': len([v for v in vulns if v.get('severity') == 'LOW'])
            }
            summary['vulnerabilities_found'] = len(vulns)
            summary['severity_counts'] = severity_counts
        elif task_type == 'network_mapping':
            summary['nodes'] = result.get('nodes', 0)
            summary['connections'] = result.get('connections', 0)
        elif task_type == 'report_generation':
            summary['report_types'] = result.get('report_types', [])
            summary['report_formats'] = result.get('report_formats', [])
            summary['report_ids'] = result.get('report_ids', {})
            
        return summary# automation/scheduler.py

import logging
import json
from datetime import datetime, timedelta
from django.utils import timezone
from croniter import croniter

from .models import ScheduledTask, ScanWorkflow
from .workflow_orchestrator import WorkflowOrchestrator

logger = logging.getLogger(__name__)

class ScanScheduler:
    """
    Handles scheduling and execution of recurring security scans
    """
    
    def __init__(self):
        self.orchestrator = WorkflowOrchestrator()
    
    def process_scheduled_tasks(self) -> int:
        """
        Check for scheduled tasks that need to be triggered and create workflows
        
        Returns:
            int: Number of workflows created
        """
        now = timezone.now()
        active_schedules = ScheduledTask.objects.filter(is_active=True)
        
        workflows_created = 0
        
        for schedule in active_schedules:
            try:
                # Skip if end date is set and has passed
                if schedule.end_date and schedule.end_date < now.date():
                    continue
                
                # Determine next execution time
                next_run = self._calculate_next_run(schedule)
                
                # Check if it's time to run (or overdue)
                if next_run and next_run <= now:
                    # Create a new workflow
                    name = f"{schedule.name} - {now.strftime('%Y-%m-%d %H:%M')}"
                    
                    workflow = self.orchestrator.create_workflow(
                        target=schedule.target,
                        name=name,
                        scan_profile=schedule.scan_profile,
                        notify_email=schedule.notification_email
                    )
                    
                    # Start the workflow immediately
                    self.orchestrator.start_workflow(workflow.id)
                    
                    # Update the schedule's last execution
                    schedule.last_execution = now
                    schedule.last_status = 'started'
                    schedule.last_workflow = workflow
                    schedule.save()
                    
                    logger.info(f"Created scheduled workflow {workflow.id} for schedule {schedule.id}")
                    workflows_created += 1
            
            except Exception as e:
                logger.error(f"Error processing scheduled task {schedule.id}: {str(e)}")
        
        return workflows_created
    
    def _calculate_next_run(self, schedule: ScheduledTask) -> datetime:
        """
        Calculate the next run time for a scheduled task
        
        Args:
            schedule: The scheduled task
            
        Returns:
            datetime: Next execution time or None if cannot be determined
        """
        now = timezone.now()
        
        # If never run, use start_date as base
        if schedule.last_execution is None:
            base_time = datetime.combine(schedule.start_date, datetime.min.time())
            base_time = timezone.make_aware(base_time)
            
            # If start date is in future, return that
            if base_time > now:
                return base_time
        else:
            base_time = schedule.last_execution
        
        # Calculate next run based on frequency
        if schedule.frequency == 'daily':
            # Add 24 hours to last execution
            return base_time + timedelta(days=1)
            
        elif schedule.frequency == 'weekly':
            # Add 7 days to last execution
            return base_time + timedelta(days=7)
            
        elif schedule.frequency == 'monthly':
            # Add roughly a month (30 days) to last execution
            return base_time + timedelta(days=30)
            
        elif schedule.frequency == 'custom' and schedule.cron_expression:
            try:
                # Use croniter to calculate next run based on cron expression
                cron = croniter(schedule.cron_expression, base_time)
                return cron.get_next(datetime)
            except Exception as e:
                logger.error(f"Error parsing cron expression for schedule {schedule.id}: {str(e)}")
                return None
        
        return None
    
    def create_scheduled_task(self, name: str, target: str, frequency: str, 
                          start_date: datetime.date, end_date=None, 
                          scan_profile: str='standard', cron_expression=None, 
                          notification_email=None, created_by=None) -> ScheduledTask:
        """
        Create a new scheduled task
        
        Args:
            name: Name of the scheduled task
            target: Target domain/IP
            frequency: Frequency (daily, weekly, monthly, custom)
            start_date: Start date
            end_date: End date (optional)
            scan_profile: Scan profile (quick, standard, full)
            cron_expression: Cron expression for custom frequency
            notification_email: Email to notify
            created_by: User who created the schedule
            
        Returns:
            ScheduledTask: The created scheduled task
        """
        if frequency == 'custom' and not cron_expression:
            raise ValueError("Cron expression is required for custom frequency")
        
        scheduled_task = ScheduledTask.objects.create(
            name=name,
            target=target,
            frequency=frequency,
            start_date=start_date,
            end_date=end_date,
            scan_profile=scan_profile,
            cron_expression=cron_expression,
            notification_email=notification_email,
            created_by=created_by
        )
        
        logger.info(f"Created scheduled task {scheduled_task.id} for {target}")
        return scheduled_task
    
    def update_scheduled_task(self, task_id: int, **kwargs) -> ScheduledTask:
        """
        Update a scheduled task
        
        Args:
            task_id: ID of the task to update
            **kwargs: Fields to update
            
        Returns:
            ScheduledTask: The updated task
        """
        try:
            task = ScheduledTask.objects.get(id=task_id)
            
            # Update fields
            for field, value in kwargs.items():
                if hasattr(task, field):
                    setattr(task, field, value)
            
            task.save()
            logger.info(f"Updated scheduled task {task_id}")
            return task
            
        except ScheduledTask.DoesNotExist:
            logger.error(f"Scheduled task {task_id} not found")
            raise ValueError(f"Scheduled task {task_id} not found")
    
    def delete_scheduled_task(self, task_id: int) -> bool:
        """
        Delete a scheduled task
        
        Args:
            task_id: ID of the task to delete
            
        Returns:
            bool: True if successfully deleted
        """
        try:
            task = ScheduledTask.objects.get(id=task_id)
            task.delete()
            logger.info(f"Deleted scheduled task {task_id}")
            return True
            
        except ScheduledTask.DoesNotExist:
            logger.error(f"Scheduled task {task_id} not found")
            return False
    
    def disable_scheduled_task(self, task_id: int) -> bool:
        """
        Disable a scheduled task
        
        Args:
            task_id: ID of the task to disable
            
        Returns:
            bool: True if successfully disabled
        """
        try:
            task = ScheduledTask.objects.get(id=task_id)
            task.is_active = False
            task.save()
            logger.info(f"Disabled scheduled task {task_id}")
            return True
            
        except ScheduledTask.DoesNotExist:
            logger.error(f"Scheduled task {task_id} not found")
            return False
    
    def enable_scheduled_task(self, task_id: int) -> bool:
        """
        Enable a scheduled task
        
        Args:
            task_id: ID of the task to enable
            
        Returns:
            bool: True if successfully enabled
        """
        try:
            task = ScheduledTask.objects.get(id=task_id)
            task.is_active = True
            task.save()
            logger.info(f"Enabled scheduled task {task_id}")
            return True
            
        except ScheduledTask.DoesNotExist:
            logger.error(f"Scheduled task {task_id} not found")
            return Falsefrom django.db import models

class NetworkNode(models.Model):
    """Represents a node in the network topology"""
    
    NODE_TYPES = [
        ('host', 'Host'),
        ('subdomain', 'Subdomain'),
        ('service', 'Service'),
        ('gateway', 'Gateway')
    ]
    
    domain = models.CharField(max_length=255, db_index=True)
    name = models.CharField(max_length=255)
    node_type = models.CharField(max_length=50, choices=NODE_TYPES)
    ip_address = models.GenericIPAddressField(null=True, blank=True)
    metadata = models.JSONField(default=dict, blank=True)
    
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    last_seen = models.DateTimeField(auto_now=True)
    is_active = models.BooleanField(default=True)
    
    class Meta:
        unique_together = ['domain', 'name', 'node_type']
        indexes = [
            models.Index(fields=['domain', 'is_active']),
            models.Index(fields=['node_type']),
        ]
    
    def __str__(self):
        return f"{self.name} ({self.node_type})"

class NetworkConnection(models.Model):
    """Represents a connection between two nodes in the network topology"""
    
    CONNECTION_TYPES = [
        ('domain', 'Domain Link'),
        ('service', 'Service Connection'),
        ('subdomain', 'Subdomain Link'),
        ('external', 'External Connection')
    ]
    
    source = models.ForeignKey(NetworkNode, on_delete=models.CASCADE, related_name='outgoing_connections')
    target = models.ForeignKey(NetworkNode, on_delete=models.CASCADE, related_name='incoming_connections')
    connection_type = models.CharField(max_length=50, choices=CONNECTION_TYPES)
    metadata = models.JSONField(default=dict, blank=True)
    
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    is_active = models.BooleanField(default=True)
    
    class Meta:
        unique_together = ['source', 'target', 'connection_type']
        indexes = [
            models.Index(fields=['source', 'is_active']),
            models.Index(fields=['target', 'is_active']),
        ]
    
    def __str__(self):
        return f"{self.source.name}  {self.target.name} ({self.connection_type})"import math
from typing import Dict, List, Optional
import logging
from django.db import transaction
from reconnaissance.models import PortScan, Subdomain, Service
from .models import NetworkNode, NetworkConnection
import socket
from subprocess import Popen, PIPE
from datetime import datetime
import json
from django.core.serializers.json import DjangoJSONEncoder
from vulnerability.models import Vulnerability

class TopologyMapper:
    """
    Handles network topology mapping and visualization
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    @transaction.atomic
    def create_network_map(self, target_domain: str, services: list = None, subdomains: list = None) -> dict:
        """
        Create a network map for a target domain
        
        Args:
            target_domain: The domain to map
            services: Optional list of service data from service identification
            subdomains: Optional list of subdomain data from subdomain enumeration
            
        Returns:
            dict: Status and count information about created nodes/connections
        """
        try:
            self.logger.info(f"Creating network map for {target_domain}")
            
            # Create domain node if it doesn't exist
            domain_node, created = NetworkNode.objects.get_or_create(
                domain=target_domain,
                name=target_domain,
                node_type='host',
                defaults={
                    'ip_address': None,
                    'metadata': {'main_domain': True},
                    'is_active': True
                }
            )
            
            if created:
                self.logger.info(f"Created domain node for {target_domain}")
            
            # Process subdomain data if provided
            if subdomains:
                self.logger.info(f"Processing {len(subdomains)} subdomains for network mapping")
                self._process_subdomains(target_domain, domain_node, subdomains)
            else:
                # Otherwise use database subdomain records
                self._process_database_subdomains(target_domain, domain_node)
                
            # Process service data if provided
            if services:
                self.logger.info(f"Processing {len(services)} services for network mapping")
                self._process_services(target_domain, domain_node, services)
            else:
                # Otherwise use database service records
                self._process_database_services(target_domain, domain_node)
            
            # Add gateway nodes for key external connections
            self._add_gateway_nodes(target_domain, domain_node)
            
            # Add vulnerability nodes
            self._add_vulnerability_nodes(target_domain, domain_node)
            
            # Count nodes and connections for the target domain
            node_count = NetworkNode.objects.filter(
                domain=target_domain,
                is_active=True
            ).count()
            
            connection_count = NetworkConnection.objects.filter(
                source__domain=target_domain,
                is_active=True
            ).count()
            
            self.logger.info(f"Network map created with {node_count} nodes and {connection_count} connections")
            
            # Get the network data for visualization
            network_data = self.get_network_data(target_domain)
            
            return {
                'status': 'success',
                'target': target_domain,
                'nodes': node_count,
                'connections': connection_count,
                'network_data': network_data  # Include the network data in the result
            }
            
        except Exception as e:
            self.logger.error(f"Error creating network map: {str(e)}")
            return {
                'status': 'error',
                'error': str(e)
            }

    def _process_subdomains(self, target_domain, domain_node, subdomains):
        """Process subdomain data from scan results"""
        for subdomain_data in subdomains:
            subdomain = subdomain_data.get('subdomain')
            if not subdomain:
                continue
                
            try:
                # Create subdomain node
                subdomain_node, created = NetworkNode.objects.get_or_create(
                    domain=target_domain,
                    name=subdomain,
                    node_type='subdomain',
                    defaults={
                        'ip_address': subdomain_data.get('ip_address'),
                        'metadata': {
                            'is_http': subdomain_data.get('is_http'),
                            'http_status': subdomain_data.get('http_status'),
                            'status': subdomain_data.get('status')
                        },
                        'is_active': True
                    }
                )
                
                # Create connection to main domain
                NetworkConnection.objects.get_or_create(
                    source=domain_node,
                    target=subdomain_node,
                    connection_type='domain',
                    defaults={
                        'metadata': {},
                        'is_active': True
                    }
                )
            except Exception as e:
                self.logger.error(f"Error processing subdomain {subdomain}: {str(e)}")

    def _process_services(self, target_domain, domain_node, services):
        """Process service data from scan results"""
        for service_data in services:
            port = service_data.get('port')
            if not port:
                continue
                
            service_info = service_data.get('service', {})
            if not service_info:
                continue
                
            service_name = service_info.get('name', 'unknown')
            node_name = f"{service_name}:{port}"
            
            try:
                # Create service node
                service_node, created = NetworkNode.objects.get_or_create(
                    domain=target_domain,
                    name=node_name,
                    node_type='service',
                    defaults={
                        'ip_address': None,
                        'metadata': {
                            'port': port,
                            'protocol': service_data.get('protocol', 'tcp'),
                            'product': service_info.get('product', ''),
                            'version': service_info.get('version', ''),
                            'category': service_data.get('category', 'other'),
                            'risk_level': service_data.get('risk_level', 'MEDIUM')
                        },
                        'is_active': True
                    }
                )
                
                # Create connection to main domain
                NetworkConnection.objects.get_or_create(
                    source=domain_node,
                    target=service_node,
                    connection_type='service',
                    defaults={
                        'metadata': {'port': port},
                        'is_active': True
                    }
                )
            except Exception as e:
                self.logger.error(f"Error processing service {node_name}: {str(e)}")

    def _process_database_subdomains(self, target_domain, domain_node):
        """Use subdomain information from the database"""
        from reconnaissance.models import Subdomain
        
        subdomains = Subdomain.objects.filter(domain=target_domain, is_active=True)
        self.logger.info(f"Processing {subdomains.count()} subdomains from database")
        
        for subdomain in subdomains:
            try:
                # Create subdomain node
                subdomain_node, created = NetworkNode.objects.get_or_create(
                    domain=target_domain,
                    name=subdomain.subdomain,
                    node_type='subdomain',
                    defaults={
                        'ip_address': subdomain.ip_address,
                        'metadata': {},
                        'is_active': True
                    }
                )
                
                # Create connection to main domain
                NetworkConnection.objects.get_or_create(
                    source=domain_node,
                    target=subdomain_node,
                    connection_type='subdomain',
                    defaults={
                        'metadata': {},
                        'is_active': True
                    }
                )
            except Exception as e:
                self.logger.error(f"Error processing DB subdomain {subdomain.subdomain}: {str(e)}")

    def _process_database_services(self, target_domain, domain_node):
        """Use service information from the database"""
        from reconnaissance.models import Service
        
        services = Service.objects.filter(host=target_domain, is_active=True)
        self.logger.info(f"Processing {services.count()} services from database")
        
        for service in services:
            try:
                node_name = f"{service.name}:{service.port}"
                
                # Create service node
                service_node, created = NetworkNode.objects.get_or_create(
                    domain=target_domain,
                    name=node_name,
                    node_type='service',
                    defaults={
                        'ip_address': None,
                        'metadata': {
                            'port': service.port,
                            'protocol': service.protocol,
                            'product': service.product,
                            'version': service.version,
                            'category': service.category,
                            'risk_level': service.risk_level
                        },
                        'is_active': True
                    }
                )
                
                # Create connection to main domain
                NetworkConnection.objects.get_or_create(
                    source=domain_node,
                    target=service_node,
                    connection_type='service',
                    defaults={
                        'metadata': {'port': service.port},
                        'is_active': True
                    }
                )
            except Exception as e:
                self.logger.error(f"Error processing DB service {service.name}:{service.port}: {str(e)}")

    def _add_gateway_nodes(self, target_domain, domain_node):
        """Add gateway nodes for external connections"""
        from vulnerability.models import Vulnerability
        
        # Check if target has vulnerabilities related to external services
        external_vulns = Vulnerability.objects.filter(
            target=target_domain,
            vuln_type__in=['ssrf', 'open_redirect', 'external_service']
        )
        
        for vuln in external_vulns:
            try:
                # Create a gateway node for the vulnerability
                gateway_name = f"External: {vuln.name[:30]}"
                
                gateway_node, created = NetworkNode.objects.get_or_create(
                    domain=target_domain,
                    name=gateway_name,
                    node_type='gateway',
                    defaults={
                        'ip_address': None,
                        'metadata': {
                            'vulnerability_id': vuln.id,
                            'severity': vuln.severity
                        },
                        'is_active': True
                    }
                )
                
                # Create connection to main domain
                NetworkConnection.objects.get_or_create(
                    source=domain_node,
                    target=gateway_node,
                    connection_type='external',
                    defaults={
                        'metadata': {'severity': vuln.severity},
                        'is_active': True
                    }
                )
            except Exception as e:
                self.logger.error(f"Error adding gateway node for {vuln.name}: {str(e)}")

    def _add_vulnerability_nodes(self, target_domain, domain_node):
        """Add vulnerability nodes to network map"""
        from vulnerability.models import Vulnerability
        
        vulnerabilities = Vulnerability.objects.filter(
            target=target_domain,
            is_fixed=False
        )
        for vuln in vulnerabilities:
            try:
                # Skip some vulnerability types to avoid cluttering
                if vuln.vuln_type in ['info_disclosure', 'outdated_component']:
                    continue
                    
                # Create node name based on severity
                severity_prefix = {
                    'CRITICAL': 'Critical:',
                    'HIGH': 'High:',
                    'MEDIUM': 'Medium:',
                    'LOW': 'Low:'
                }.get(vuln.severity, '')
                
                node_name = f"{severity_prefix} {vuln.name[:40]}"
                
                # Check if vulnerability node type exists in model choices
                # If not, handle potential database integrity errors
                try:
                    vuln_node, created = NetworkNode.objects.get_or_create(
                        domain=target_domain,
                        name=node_name,
                        node_type='vulnerability',
                        defaults={
                            'ip_address': None,
                            'metadata': {
                                'vulnerability_id': vuln.id,
                                'severity': vuln.severity,
                                'type': vuln.vuln_type,
                                'cvss': vuln.cvss_score
                            },
                            'is_active': True
                        }
                    )
                except Exception as type_error:
                    # Fall back to a more generic node type if 'vulnerability' not in choices
                    self.logger.warning(f"Could not create vulnerability node with type 'vulnerability', using 'gateway' instead: {str(type_error)}")
                    vuln_node, created = NetworkNode.objects.get_or_create(
                        domain=target_domain,
                        name=node_name,
                        node_type='gateway',  # Fallback to gateway which should exist
                        defaults={
                            'ip_address': None,
                            'metadata': {
                                'vulnerability_id': vuln.id,
                                'severity': vuln.severity,
                                'type': vuln.vuln_type,
                                'cvss': vuln.cvss_score,
                                'is_vulnerability': True  # Mark it as actually a vulnerability
                            },
                            'is_active': True
                        }
                    )
                
                # Connect to domain node
                NetworkConnection.objects.get_or_create(
                    source=domain_node,
                    target=vuln_node,
                    connection_type='external',  # Using 'external' for vulnerabilities too
                    defaults={
                        'metadata': {'severity': vuln.severity},
                        'is_active': True
                    }
                )
            except Exception as e:
                self.logger.error(f"Error adding vulnerability node for {vuln.name}: {str(e)}")

    def _cleanup_old_data(self, target_domain: str):
        """Clean up old nodes and connections for the target domain"""
        # Deactivate old nodes
        NetworkNode.objects.filter(domain=target_domain).update(is_active=False)
        
        # Deactivate old connections
        NetworkConnection.objects.filter(
            source__domain=target_domain
        ).update(is_active=False)

    def _get_or_create_node(self, name: str, domain: str, node_type: str, 
                           ip_address: Optional[str] = None, 
                           metadata: Optional[Dict] = None) -> NetworkNode:
        """Gets existing node or creates a new one"""
        node, created = NetworkNode.objects.get_or_create(
            name=name,
            domain=domain,
            node_type=node_type,
            defaults={
                'ip_address': ip_address,
                'metadata': metadata or {},
                'is_active': True
            }
        )
        if not created:
            node.ip_address = ip_address
            node.is_active = True
            if metadata:
                node.metadata.update(metadata)
            node.save()
        return node

    def _create_connection(self, source: NetworkNode, target: NetworkNode, 
                          connection_type: str, metadata: Optional[Dict] = None) -> NetworkConnection:
        """Creates or updates a connection between nodes"""
        connection, created = NetworkConnection.objects.get_or_create(
            source=source,
            target=target,
            connection_type=connection_type,
            defaults={
                'metadata': metadata or {},
                'is_active': True
            }
        )
        if not created:
            connection.is_active = True
            if metadata:
                connection.metadata.update(metadata)
            connection.save()
        return connection
            
    def get_network_data(self, target_domain: str) -> Dict:
        """
        Get formatted network data suitable for D3.js visualization
        
        Args:
            target_domain: The domain to get network data for
            
        Returns:
            dict: Network data with nodes and links arrays
        """
        try:
            nodes = []
            links = []
            
            # Get all active nodes for the domain
            domain_nodes = NetworkNode.objects.filter(
                domain=target_domain,
                is_active=True
            )
            
            if not domain_nodes.exists():
                self.logger.warning(f"No active nodes found for domain {target_domain}")
                return {
                    'nodes': [],
                    'links': []
                }
            
            # Create node data for visualization
            for node in domain_nodes:
                node_data = {
                    'id': str(node.id),  # Convert to string to ensure compatibility with D3
                    'name': node.name,
                    'type': node.node_type,
                    'info': str(node.ip_address) if node.ip_address else ''
                }
                
                # Add some metadata if available
                if isinstance(node.metadata, dict):
                    for key in ['port', 'product', 'version', 'category', 'risk_level', 'severity']:
                        if key in node.metadata:
                            node_data[key] = node.metadata[key]
                
                nodes.append(node_data)
            
            # Get all active connections for the domain
            node_ids = domain_nodes.values_list('id', flat=True)
            connections = NetworkConnection.objects.filter(
                source_id__in=node_ids,
                is_active=True
            )
            
            # Create link data for visualization, ensuring source/target are strings
            for conn in connections:
                links.append({
                    'source': str(conn.source_id),  # Must match the node id format (string)
                    'target': str(conn.target_id),  # Must match the node id format (string)
                    'type': conn.connection_type
                })
            
            # Log the data being returned
            self.logger.info(f"Returning network data with {len(nodes)} nodes and {len(links)} links for {target_domain}")
            
            if nodes:
                # Position main domain node at center
                center_node = next((node for node in nodes if node.get('type') == 'host'), nodes[0])
                center_node['x'] = 0
                center_node['y'] = 0
                
                # Position other nodes in concentric circles
                types_order = ['subdomain', 'service', 'gateway', 'vulnerability']
                
                for i, node_type in enumerate(types_order):
                    type_nodes = [node for node in nodes if node.get('type') == node_type]
                    radius = 150 * (i + 1)  # Increasing radius for each type
                    angle_step = (2 * math.pi) / max(len(type_nodes), 1)
                    
                    for j, node in enumerate(type_nodes):
                        angle = angle_step * j
                        node['x'] = radius * math.cos(angle)
                        node['y'] = radius * math.sin(angle)
            
            return {
                'nodes': nodes,
                'links': links
            }


            
        except Exception as e:
            self.logger.error(f"Error getting network data: {str(e)}")
            # Return minimal valid data structure instead of raising error
            return {
                'nodes': [],
                'links': [],
                'error': str(e)
            }

# Example dummy data for testing
def generate_test_network_data(target: str = "example.com") -> Dict:
    """Generate test network data for visualization testing"""
    return {
        "nodes": [
            {"id": "host_1", "name": target, "type": "host", "info": "Main target"},
            {"id": "subdomain_1", "name": f"www.{target}", "type": "subdomain", "info": "Web server"},
            {"id": "subdomain_2", "name": f"api.{target}", "type": "subdomain", "info": "API server"},
            {"id": "port_1", "name": "Port 80", "type": "port", "info": "HTTP port"},
            {"id": "port_2", "name": "Port 443", "type": "port", "info": "HTTPS port"},
            {"id": "port_3", "name": "Port 22", "type": "port", "info": "SSH port"},
            {"id": "service_1", "name": "HTTP", "type": "service", "info": "Web service"},
            {"id": "service_2", "name": "HTTPS", "type": "service", "info": "Secure web service"},
            {"id": "service_3", "name": "SSH", "type": "service", "info": "Secure shell"},
            {"id": "vuln_1", "name": "SQL Injection", "type": "vulnerability", "info": "Severity: HIGH"}
        ],
        "links": [
            {"source": "host_1", "target": "subdomain_1", "type": "contains"},
            {"source": "host_1", "target": "subdomain_2", "type": "contains"},
            {"source": "subdomain_1", "target": "port_1", "type": "contains"},
            {"source": "subdomain_1", "target": "port_2", "type": "contains"},
            {"source": "subdomain_2", "target": "port_3", "type": "contains"},
            {"source": "port_1", "target": "service_1", "type": "communicates"},
            {"source": "port_2", "target": "service_2", "type": "communicates"},
            {"source": "port_3", "target": "service_3", "type": "communicates"},
            {"source": "service_1", "target": "vuln_1", "type": "has_vulnerability"}
        ]
    }