# automation/models.py

from django.db import models
from django.utils import timezone

class ScanWorkflow(models.Model):
    """
    Represents a complete security scanning workflow
    """
    STATUS_CHOICES = [
        ('pending', 'Pending'),
        ('scheduled', 'Scheduled'),
        ('in_progress', 'In Progress'),
        ('completed', 'Completed'),
        ('failed', 'Failed'),
        ('canceled', 'Canceled')
    ]
    
    PROFILE_CHOICES = [
        ('quick', 'Quick Scan'),
        ('standard', 'Standard Scan'),
        ('full', 'Full Scan')
    ]
    
    name = models.CharField(max_length=255)
    target = models.CharField(max_length=255)
    scan_profile = models.CharField(max_length=20, choices=PROFILE_CHOICES, default='standard')
    status = models.CharField(max_length=20, choices=STATUS_CHOICES, default='pending')
    
    # Timing information
    created_at = models.DateTimeField(auto_now_add=True)
    scheduled_time = models.DateTimeField(null=True, blank=True)
    start_time = models.DateTimeField(null=True, blank=True)
    end_time = models.DateTimeField(null=True, blank=True)
    
    # Notification settings
    notification_email = models.EmailField(null=True, blank=True)
    
    # Additional metadata
    metadata = models.JSONField(default=dict, blank=True)
    
    class Meta:
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['status']),
            models.Index(fields=['target']),
            models.Index(fields=['created_at']),
        ]
    
    def __str__(self):
        return f"{self.name} - {self.target} ({self.status})"
    
    @property
    def duration(self):
        """Calculate workflow duration in seconds"""
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return None
    
    @property
    def is_active(self):
        """Check if workflow is active (not in terminal state)"""
        return self.status in ['pending', 'scheduled', 'in_progress']
    
    @property
    def is_scheduled(self):
        """Check if workflow is scheduled for future execution"""
        return self.status == 'scheduled' and self.scheduled_time and self.scheduled_time > timezone.now()


class ScanTask(models.Model):
    """
    Represents an individual task within a scanning workflow
    """
    STATUS_CHOICES = [
        ('pending', 'Pending'),
        ('in_progress', 'In Progress'),
        ('completed', 'Completed'),
        ('failed', 'Failed'),
        ('skipped', 'Skipped'),
        ('canceled', 'Canceled')
    ]
    
    TASK_TYPE_CHOICES = [
        ('subdomain_enumeration', 'Subdomain Enumeration'),
        ('port_scanning', 'Port Scanning'),
        ('service_identification', 'Service Identification'),
        ('vulnerability_scanning', 'Vulnerability Scanning'),
        ('network_mapping', 'Network Mapping'),
        ('report_generation', 'Report Generation')
    ]
    
    workflow = models.ForeignKey(ScanWorkflow, on_delete=models.CASCADE, related_name='tasks')
    task_type = models.CharField(max_length=50, choices=TASK_TYPE_CHOICES)
    name = models.CharField(max_length=255)
    status = models.CharField(max_length=20, choices=STATUS_CHOICES, default='pending')
    
    # Task dependencies
    dependencies = models.ManyToManyField('self', symmetrical=False, related_name='dependents', blank=True)
    order = models.IntegerField(default=0)
    
    # Timing information
    created_at = models.DateTimeField(auto_now_add=True)
    start_time = models.DateTimeField(null=True, blank=True)
    end_time = models.DateTimeField(null=True, blank=True)
    
    # Results
    result = models.TextField(null=True, blank=True)
    
    class Meta:
        ordering = ['workflow', 'order']
        indexes = [
            models.Index(fields=['workflow', 'status']),
            models.Index(fields=['task_type']),
        ]
    
    def __str__(self):
        return f"{self.name} ({self.status})"
    
    @property
    def duration(self):
        """Calculate task duration in seconds"""
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return None
    
    @property
    def has_dependencies(self):
        """Check if task has dependencies"""
        return self.dependencies.exists()
    
    @property
    def is_blocked(self):
        """Check if any dependencies are not completed"""
        return self.dependencies.exclude(status='completed').exists()


class Notification(models.Model):
    """
    Represents a notification sent to a user
    """
    NOTIFICATION_TYPE_CHOICES = [
        ('workflow_scheduled', 'Workflow Scheduled'),
        ('workflow_started', 'Workflow Started'),
        ('workflow_completed', 'Workflow Completed'),
        ('workflow_failed', 'Workflow Failed'),
        ('workflow_canceled', 'Workflow Canceled'),
        ('task_failed', 'Task Failed'),
        ('critical_vulnerabilities', 'Critical Vulnerabilities'),
        ('report_ready', 'Report Ready')
    ]
    
    workflow = models.ForeignKey(ScanWorkflow, on_delete=models.CASCADE, related_name='notifications')
    notification_type = models.CharField(max_length=50, choices=NOTIFICATION_TYPE_CHOICES)
    recipient = models.EmailField()
    subject = models.CharField(max_length=255)
    message = models.TextField()
    
    created_at = models.DateTimeField(auto_now_add=True)
    sent = models.BooleanField(default=False)
    sent_time = models.DateTimeField(null=True, blank=True)
    
    # For tracking email opens, clicks, etc.
    metadata = models.JSONField(default=dict, blank=True)
    
    class Meta:
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['notification_type']),
            models.Index(fields=['created_at']),
            models.Index(fields=['sent']),
        ]
    
    def __str__(self):
        return f"{self.notification_type} for {self.workflow.name}"


class ScheduledTask(models.Model):
    """
    Represents a recurring scheduled task/scan
    """
    FREQUENCY_CHOICES = [
        ('daily', 'Daily'),
        ('weekly', 'Weekly'),
        ('monthly', 'Monthly'),
        ('custom', 'Custom')
    ]
    
    name = models.CharField(max_length=255)
    target = models.CharField(max_length=255)
    scan_profile = models.CharField(max_length=20, choices=ScanWorkflow.PROFILE_CHOICES, default='standard')
    
    # Schedule
    frequency = models.CharField(max_length=20, choices=FREQUENCY_CHOICES)
    cron_expression = models.CharField(max_length=100, null=True, blank=True)
    start_date = models.DateField()
    end_date = models.DateField(null=True, blank=True)
    
    # Active flag
    is_active = models.BooleanField(default=True)
    
    # Notification settings
    notification_email = models.EmailField(null=True, blank=True)
    
    # Created by
    created_at = models.DateTimeField(auto_now_add=True)
    created_by = models.CharField(max_length=100, null=True, blank=True)
    
    # Last execution
    last_execution = models.DateTimeField(null=True, blank=True)
    last_status = models.CharField(max_length=20, null=True, blank=True)
    last_workflow = models.ForeignKey(ScanWorkflow, on_delete=models.SET_NULL, null=True, blank=True, related_name='schedule')
    
    class Meta:
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['is_active']),
            models.Index(fields=['frequency']),
            models.Index(fields=['target']),
        ]
    
    def __str__(self):
        return f"{self.name} - {self.target} ({self.frequency})"# automation/processor.py

import threading
import time
import logging
from django.conf import settings
from django.db import connection

from .workflow_orchestrator import WorkflowOrchestrator
from .scheduler import ScanScheduler

logger = logging.getLogger(__name__)

class AutomationProcessor:
    """
    Process automation workflows and scheduled tasks in the background.
    Implemented as a singleton to ensure only one instance is running.
    """
    _instance = None
    _lock = threading.Lock()
    
    @classmethod
    def get_instance(cls):
        """Get or create the singleton instance"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = cls()
        return cls._instance
    
    def __init__(self):
        self.orchestrator = WorkflowOrchestrator()
        self.scheduler = ScanScheduler()
        self.stop_flag = threading.Event()
        self.processing_thread = None
        self.interval = getattr(settings, 'AUTOMATION_PROCESSING_INTERVAL', 60)  # seconds
    
    def start(self):
        """Start the background processing thread if not already running"""
        if self.processing_thread and self.processing_thread.is_alive():
            logger.warning("Automation processor already running")
            return False
            
        self.stop_flag.clear()
        self.processing_thread = threading.Thread(
            target=self._processing_loop,
            daemon=True
        )
        self.processing_thread.start()
        logger.info("Automation processor started")
        return True
    
    def stop(self):
        """Stop the background processing thread"""
        if not self.processing_thread or not self.processing_thread.is_alive():
            logger.warning("Automation processor not running")
            return False
            
        self.stop_flag.set()
        self.processing_thread.join(timeout=10)
        logger.info("Automation processor stopped")
        return True
    
    def is_running(self):
        """Check if the processor is running"""
        return self.processing_thread is not None and self.processing_thread.is_alive()
    
    def _processing_loop(self):
        """Main processing loop for automation tasks"""
        logger.info("Automation processing loop started")
        
        while not self.stop_flag.is_set():
            try:
                # Process scheduled tasks
                scheduled_count = self.scheduler.process_scheduled_tasks()
                if scheduled_count > 0:
                    logger.info(f"Processed {scheduled_count} scheduled tasks")
                
                # Start pending workflows
                started_count = self.orchestrator.check_pending_workflows()
                if started_count > 0:
                    logger.info(f"Started {started_count} pending workflows")
                
                # Process workflow queue
                processed_count = self.orchestrator.process_workflow_queue()
                if processed_count > 0:
                    logger.info(f"Processed {processed_count} workflow tasks")
                
            except Exception as e:
                logger.error(f"Error in automation processing: {str(e)}")
            finally:
                # Close database connections to prevent connection leaks
                connection.close()
            
            # Sleep until next interval
            self.stop_flag.wait(self.interval)
        
        logger.info("Automation processing loop stopped")
    
    @classmethod
    def run_once(cls):
        """
        Run a single cycle of the processor, useful for cron jobs or manual triggers
        """
        processor = cls()
        
        try:
            # Process scheduled tasks
            scheduled_count = processor.scheduler.process_scheduled_tasks()
            
            # Start pending workflows
            started_count = processor.orchestrator.check_pending_workflows()
            
            # Process workflow queue
            processed_count = processor.orchestrator.process_workflow_queue()
            
            return {
                'scheduled_tasks_processed': scheduled_count,
                'workflows_started': started_count,
                'tasks_processed': processed_count
            }
            
        except Exception as e:
            logger.error(f"Error in one-time automation processing: {str(e)}")
            return {
                'error': str(e)
            }
        finally:
            # Close database connections
            connection.close()# automation/workflow_orchestrator.py

import logging
import json
import time
from datetime import datetime, timedelta
from django.utils import timezone
from django.db import transaction
from django.conf import settings

from reconnaissance.subdomain_enumerator import SubdomainEnumerator
from reconnaissance.scanner import PortScanner
from reconnaissance.service_identifier import ServiceIdentifier
from vulnerability.unified_scanner import UnifiedVulnerabilityScanner
from network_visualization.topology_mapper import TopologyMapper
from reporting.report_generator import ReportGenerator
from .models import ScanWorkflow, ScanTask, Notification
from .notification_manager import NotificationManager

logger = logging.getLogger(__name__)

class WorkflowOrchestrator:
    """
    Orchestrates the complete scanning workflow including reconnaissance,
    vulnerability scanning, network visualization, and report generation.
    
    Supports automatic scheduling, task dependencies, and failure handling.
    """
    
    # Workflow task types
    TASK_TYPES = {
        'subdomain_enumeration': 'Subdomain Enumeration',
        'port_scanning': 'Port Scanning',
        'service_identification': 'Service Identification',
        'vulnerability_scanning': 'Vulnerability Scanning',
        'network_mapping': 'Network Mapping',
        'report_generation': 'Report Generation'
    }
    
    # Task dependencies - keys depend on values
    TASK_DEPENDENCIES = {
        'port_scanning': ['subdomain_enumeration'],
        'service_identification': ['port_scanning'],
        'vulnerability_scanning': ['service_identification'],
        'network_mapping': ['service_identification'],
        'report_generation': ['vulnerability_scanning', 'network_mapping']
    }
    
    def __init__(self):
        self.subdomain_enumerator = SubdomainEnumerator()
        self.port_scanner = PortScanner()
        self.service_identifier = ServiceIdentifier()
        self.vulnerability_scanner = UnifiedVulnerabilityScanner()
        self.topology_mapper = TopologyMapper()
        self.report_generator = ReportGenerator()
        self.notification_manager = NotificationManager()
    
    def setup_workflow(self, workflow, target: str, scan_profile: str = 'standard'):
        """
        Set up tasks for an existing workflow
        
        Args:
            workflow: The existing ScanWorkflow object
            target: The domain or IP to scan
            scan_profile: Scan intensity level (quick, standard, full)
            
        Returns:
            Updated workflow
        """
        with transaction.atomic():
            # Create tasks with proper dependencies
            task_ids = {}
            
            # Create all tasks first
            for task_type, task_name in self.TASK_TYPES.items():
                task = ScanTask.objects.create(
                    workflow=workflow,
                    task_type=task_type,
                    name=f"{task_name} - {target}",
                    status='pending',
                    order=list(self.TASK_TYPES.keys()).index(task_type)
                )
                task_ids[task_type] = task.id
            
            # Set dependencies
            for task_type, dependencies in self.TASK_DEPENDENCIES.items():
                task = ScanTask.objects.get(id=task_ids[task_type])
                for dependency in dependencies:
                    dependency_task = ScanTask.objects.get(id=task_ids[dependency])
                    task.dependencies.add(dependency_task)
                task.save()
                
            # If scheduled for the future, create a notification 
            if workflow.scheduled_time and workflow.notification_email:
                Notification.objects.create(
                    workflow=workflow,
                    notification_type='workflow_scheduled',
                    recipient=workflow.notification_email,
                    subject=f"Scan scheduled: {workflow.name}",
                    message=f"A security scan for {target} has been scheduled to start at {workflow.scheduled_time}."
                )
                
            return workflow
        
    def _complete_workflow(self, workflow: ScanWorkflow) -> None:
        """Mark workflow as completed and send notifications"""
        workflow.status = 'completed'
        workflow.end_time = timezone.now()
        workflow.save()
        
        logger.info(f"Workflow {workflow.id} for {workflow.target} completed successfully")
        
        # Send completion notification
        if workflow.notification_email:
            self.notification_manager.send_workflow_completion_notification(workflow)
                
    def create_workflow(self, target: str, name: str = None, scan_profile: str = 'standard',
                      scheduled_time: datetime = None, notify_email: str = None) -> ScanWorkflow:
        """
        Create a new scanning workflow for a target
        
        Args:
            target: The domain or IP to scan
            name: Optional name for this workflow
            scan_profile: Scan intensity level (quick, standard, full)
            scheduled_time: When to start the scan (None = immediate)
            notify_email: Email to notify when scan completes
            
        Returns:
            ScanWorkflow object
        """
        if not name:
            name = f"Scan {target} - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
            
        with transaction.atomic():
            # Create the workflow
            workflow = ScanWorkflow.objects.create(
                name=name,
                target=target,
                scan_profile=scan_profile,
                scheduled_time=scheduled_time,
                status='scheduled' if scheduled_time else 'pending',
                notification_email=notify_email
            )
            
            # Create tasks with proper dependencies
            task_ids = {}
            
            # Create all tasks first
            for task_type, task_name in self.TASK_TYPES.items():
                task = ScanTask.objects.create(
                    workflow=workflow,
                    task_type=task_type,
                    name=f"{task_name} - {target}",
                    status='pending',
                    order=list(self.TASK_TYPES.keys()).index(task_type)
                )
                task_ids[task_type] = task.id
            
            # Set dependencies
            for task_type, dependencies in self.TASK_DEPENDENCIES.items():
                task = ScanTask.objects.get(id=task_ids[task_type])
                for dependency in dependencies:
                    dependency_task = ScanTask.objects.get(id=task_ids[dependency])
                    task.dependencies.add(dependency_task)
                task.save()
                
            # If scheduled for the future, create a notification 
            if scheduled_time and notify_email:
                Notification.objects.create(
                    workflow=workflow,
                    notification_type='workflow_scheduled',
                    recipient=notify_email,
                    subject=f"Scan scheduled: {name}",
                    message=f"A security scan for {target} has been scheduled to start at {scheduled_time}."
                )
                
            return workflow
    
    def start_workflow(self, workflow_id: int) -> bool:
        """
        Start a workflow by ID
        
        Args:
            workflow_id: ID of the workflow to start
            
        Returns:
            bool: True if successfully started
        """
        try:
            workflow = ScanWorkflow.objects.get(id=workflow_id)
            
            # Check if it's time to start a scheduled workflow
            if workflow.status == 'scheduled' and workflow.scheduled_time:
                now = timezone.now()
                if now < workflow.scheduled_time:
                    logger.info(f"Workflow {workflow_id} is scheduled for {workflow.scheduled_time}, not starting yet")
                    return False
            
            # Update workflow status
            workflow.status = 'in_progress'
            workflow.start_time = timezone.now()
            workflow.save()
            
            logger.info(f"Starting workflow {workflow_id} for target {workflow.target}")
            
            # Get tasks with no dependencies (entry points)
            entry_tasks = ScanTask.objects.filter(
                workflow=workflow, 
                dependencies__isnull=True
            ).order_by('order')
            
            # Start entry tasks
            for task in entry_tasks:
                self._execute_task(task)
                
            return True
            
        except ScanWorkflow.DoesNotExist:
            logger.error(f"Workflow {workflow_id} not found")
            return False
        except Exception as e:
            logger.error(f"Error starting workflow {workflow_id}: {str(e)}")
            return False
    
    def check_pending_workflows(self) -> int:
        """
        Check for scheduled workflows that should be started
        
        Returns:
            int: Number of workflows started
        """
        now = timezone.now()
        
        # Find scheduled workflows that should start now
        scheduled_workflows = ScanWorkflow.objects.filter(
            status='scheduled',
            scheduled_time__lte=now
        )
        
        count = 0
        for workflow in scheduled_workflows:
            if self.start_workflow(workflow.id):
                count += 1
                
        return count
    
    def process_workflow_queue(self) -> int:
        """
        Process the workflow queue - check for tasks that can be started
        
        Returns:
            int: Number of tasks started
        """
        # Find in-progress workflows
        active_workflows = ScanWorkflow.objects.filter(
            status='in_progress'
        )
        
        tasks_started = 0
        
        for workflow in active_workflows:
            # Get pending tasks for this workflow
            pending_tasks = ScanTask.objects.filter(
                workflow=workflow,
                status='pending'
            )
            
            for task in pending_tasks:
                # Check if all dependencies are completed
                dependencies = task.dependencies.all()
                all_completed = all(dep.status == 'completed' for dep in dependencies)
                
                if all_completed:
                    self._execute_task(task)
                    tasks_started += 1
            
            # Check if workflow is complete
            if not ScanTask.objects.filter(workflow=workflow).exclude(status='completed').exists():
                self._complete_workflow(workflow)
                
        return tasks_started
    
    def _execute_task(self, task: ScanTask) -> None:
        """
        Execute a specific workflow task
        
        Args:
            task: The task to execute
        """
        try:
            # Update task status
            task.status = 'in_progress'
            task.start_time = timezone.now()
            task.save()
            
            logger.info(f"Executing task {task.id} ({task.task_type}) for workflow {task.workflow.id}")
            
            # Execute appropriate task type
            if task.task_type == 'subdomain_enumeration':
                result = self._run_subdomain_enumeration(task)
            elif task.task_type == 'port_scanning':
                result = self._run_port_scanning(task)
            elif task.task_type == 'service_identification':
                result = self._run_service_identification(task)
            elif task.task_type == 'vulnerability_scanning':
                result = self._run_vulnerability_scanning(task)
            elif task.task_type == 'network_mapping':
                result = self._run_network_mapping(task)
            elif task.task_type == 'report_generation':
                result = self._run_report_generation(task)
            else:
                raise ValueError(f"Unknown task type: {task.task_type}")
            
            # Update task status based on result
            if result.get('status') == 'success':
                task.status = 'completed'
                task.result = json.dumps(result)
            else:
                task.status = 'failed'
                task.result = json.dumps({'error': result.get('error', 'Unknown error')})
                
                # Create error notification
                if task.workflow.notification_email:
                    self.notification_manager.send_task_failure_notification(
                        task, result.get('error', 'Unknown error')
                    )
            
            task.end_time = timezone.now()
            task.save()
            
            # Check for critical failures that should stop the workflow
            if task.status == 'failed' and task.task_type in ['subdomain_enumeration', 'port_scanning']:
                self._fail_workflow(task.workflow, f"Critical task {task.task_type} failed")
                
        except Exception as e:
            logger.error(f"Error executing task {task.id}: {str(e)}")
            task.status = 'failed'
            task.result = json.dumps({'error': str(e)})
            task.end_time = timezone.now()
            task.save()
            
            # Create error notification
            if task.workflow.notification_email:
                self.notification_manager.send_task_failure_notification(task, str(e))
    
    def _run_subdomain_enumeration(self, task: ScanTask) -> dict:
        """Run subdomain enumeration task"""
        target = task.workflow.target
        try:
            results = self.subdomain_enumerator.enumerate_subdomains(target)
            return {
                'status': 'success',
                'target': target,
                'subdomains_found': len(results),
                'subdomains': results
            }
        except Exception as e:
            logger.error(f"Subdomain enumeration failed: {str(e)}")
            return {
                'status': 'error',
                'error': f"Subdomain enumeration failed: {str(e)}"
            }
    
# In automation/workflow_orchestrator.py
    def _run_port_scanning(self, task: ScanTask) -> dict:
        """Run port scanning task"""
        target = task.workflow.target
        scan_profile = task.workflow.scan_profile
        
        # Map scan profile to scan type
        scan_type = {
            'quick': 'quick',
            'standard': 'partial',
            'full': 'complete'
        }.get(scan_profile, 'partial')
        
        try:
            # Validate target before scanning
            import socket
            try:
                # Try to resolve hostname to ensure it's valid
                socket.gethostbyname(target)
            except socket.gaierror:
                logger.error(f"Unable to resolve target: {target}")
                return {
                    'status': 'error',
                    'error': f"Unable to resolve target: {target}. Please check the domain name."
                }
            
            # Run the scan
            results = self.port_scanner.scan(target, scan_type)
            
            # Check for success
            if results.get('status') == 'success':
                # Ensure we have some results
                if not results.get('results'):
                    logger.warning(f"Port scan completed but no results were found for {target}")
                    # Return a partial success even with no results to avoid workflow failure
                    return {
                        'status': 'success',
                        'target': target,
                        'scan_info': results.get('scan_info', {}),
                        'results': [],
                        'message': 'No open ports found or target is not responding'
                    }
                return results
            else:
                error_msg = results.get('error', 'Port scanning failed without specific error')
                
                # Check for root privileges error specifically
                if "root privileges" in error_msg:
                    logger.warning(f"Port scan attempted OS detection without root privileges. Using simplified scan.")
                    # Try again with a simpler scan that doesn't require root
                    modified_results = self.port_scanner.scan(target, 'quick')
                    if modified_results.get('status') == 'success':
                        modified_results['warning'] = "Limited port scan performed (OS detection disabled)"
                        return modified_results
                
                logger.error(f"Port scanning error: {error_msg}")
                return {
                    'status': 'error',
                    'error': error_msg
                }
        except Exception as e:
            logger.error(f"Port scanning failed: {str(e)}")
            return {
                'status': 'error',
                'error': f"Port scanning failed: {str(e)}"
            }
    
# Update in automation/workflow_orchestrator.py
    def _run_service_identification(self, task: ScanTask) -> dict:
        """Run service identification task"""
        target = task.workflow.target
        scan_profile = task.workflow.scan_profile
        
        # Map scan profile to service ID scan type
        scan_type = {
            'quick': 'quick',
            'standard': 'standard',
            'full': 'full'
        }.get(scan_profile, 'standard')
        
        try:
            results = self.service_identifier.identify_services(target, scan_type)
            if results.get('status') == 'success':
                return results
            else:
                return {
                    'status': 'error',
                    'error': results.get('error', 'Service identification failed without specific error')
                }
        except Exception as e:
            logger.error(f"Service identification failed: {str(e)}")
            return {
                'status': 'error',
                'error': f"Service identification failed: {str(e)}"
            }
    
    def _run_vulnerability_scanning(self, task: ScanTask) -> dict:
        """Run vulnerability scanning task"""
        target = task.workflow.target
        scan_profile = task.workflow.scan_profile
        
        # Determine scanners to use based on scan profile
        include_zap = scan_profile in ['standard', 'full']
        include_nuclei = True  # Always use Nuclei
        nuclei_scan_type = 'advanced' if scan_profile == 'full' else 'basic'
        
        try:
            results = self.vulnerability_scanner.scan_target(
                target=target,
                scan_type=scan_profile,
                include_zap=include_zap,
                include_nuclei=include_nuclei,
                nuclei_scan_type=nuclei_scan_type,
                use_advanced_correlation=True
            )
            
            if results.get('status') == 'success':
                # Check for critical vulnerabilities
                high_vulns = 0
                critical_vulns = 0
                
                for vuln in results.get('vulnerabilities', []):
                    if vuln.get('severity') == 'CRITICAL':
                        critical_vulns += 1
                    elif vuln.get('severity') == 'HIGH':
                        high_vulns += 1
                
                # Add notification for critical vulnerabilities
                if (critical_vulns > 0 or high_vulns > 2) and task.workflow.notification_email:
                    self.notification_manager.send_critical_vulnerability_notification(
                        task.workflow, critical_vulns, high_vulns
                    )
                
                return results
            else:
                return {
                    'status': 'error',
                    'error': results.get('error', 'Vulnerability scanning failed without specific error')
                }
        except Exception as e:
            logger.error(f"Vulnerability scanning failed: {str(e)}")
            return {
                'status': 'error', 
                'error': f"Vulnerability scanning failed: {str(e)}"
            }
    
    def _run_network_mapping(self, task: ScanTask) -> dict:
        """Run network mapping task"""
        target = task.workflow.target
        
        try:
            results = self.topology_mapper.create_network_map(target)
            if results.get('status') == 'success':
                return results
            else:
                return {
                    'status': 'error',
                    'error': results.get('error', 'Network mapping failed without specific error')
                }
        except Exception as e:
            logger.error(f"Network mapping failed: {str(e)}")
            return {
                'status': 'error',
                'error': f"Network mapping failed: {str(e)}"
            }
    
    def _run_report_generation(self, task: ScanTask) -> dict:
        """Run report generation task"""
        target = task.workflow.target
        scan_profile = task.workflow.scan_profile
        
        # Map scan profile to report type
        report_type = {
            'quick': 'basic',
            'standard': 'detailed',
            'full': 'executive'
        }.get(scan_profile, 'detailed')
        
        try:
            # Get vulnerability scanning task result
            vuln_scan_task = ScanTask.objects.filter(
                workflow=task.workflow,
                task_type='vulnerability_scanning',
                status='completed'
            ).first()
            
            # Parse scan results if available
            scan_results = None
            if vuln_scan_task and vuln_scan_task.result:
                try:
                    scan_results = json.loads(vuln_scan_task.result)
                except:
                    logger.error("Failed to parse vulnerability scan results")
            
            # Generate HTML report
            report_html = self.report_generator.generate_report(report_type, target, 'html', scan_results)
            
            # Only send a single notification email
            if task.workflow.notification_email:
                self.notification_manager.send_workflow_completion_notification(
                    task.workflow, 
                    report_id=report_html.id
                )
            
            return {
                'status': 'success',
                'target': target,
                'report_types': [report_type],
                'report_formats': ['html'],
                'report_ids': {
                    'html': report_html.id
                }
            }
        except Exception as e:
            logger.error(f"Report generation failed: {str(e)}")
            return {
                'status': 'error',
                'error': f"Report generation failed: {str(e)}"
            }
    
    def _fail_workflow(self, workflow: ScanWorkflow, reason: str) -> None:
        """Mark workflow as failed and send notifications"""
        workflow.status = 'failed'
        workflow.end_time = timezone.now()
        workflow.save()
        
        logger.error(f"Workflow {workflow.id} for {workflow.target} failed: {reason}")
        
        # Update pending tasks to skipped
        ScanTask.objects.filter(workflow=workflow, status='pending').update(
            status='skipped',
            result=json.dumps({'skipped_reason': reason})
        )
        
        # Send failure notification
        if workflow.notification_email:
            self.notification_manager.send_workflow_failure_notification(workflow, reason)
    
    def cancel_workflow(self, workflow_id: int) -> bool:
        """
        Cancel a running or scheduled workflow
        
        Args:
            workflow_id: ID of the workflow to cancel
            
        Returns:
            bool: True if successfully canceled
        """
        try:
            workflow = ScanWorkflow.objects.get(id=workflow_id)
            
            if workflow.status in ['completed', 'failed', 'canceled']:
                logger.warning(f"Workflow {workflow_id} already in terminal state: {workflow.status}")
                return False
            
            # Update workflow status
            original_status = workflow.status
            workflow.status = 'canceled'
            workflow.end_time = timezone.now()
            workflow.save()
            
            # Update in-progress tasks to canceled
            ScanTask.objects.filter(workflow=workflow, status='in_progress').update(
                status='canceled',
                end_time=timezone.now(),
                result=json.dumps({'canceled_reason': 'Workflow canceled by user'})
            )
            
            # Update pending tasks to skipped
            ScanTask.objects.filter(workflow=workflow, status='pending').update(
                status='skipped',
                result=json.dumps({'skipped_reason': 'Workflow canceled by user'})
            )
            
            logger.info(f"Workflow {workflow_id} canceled (was {original_status})")
            
            # Send cancellation notification
            if workflow.notification_email:
                self.notification_manager.send_workflow_cancellation_notification(workflow)
                
            return True
            
        except ScanWorkflow.DoesNotExist:
            logger.error(f"Workflow {workflow_id} not found")
            return False
        except Exception as e:
            logger.error(f"Error canceling workflow {workflow_id}: {str(e)}")
            return False
    
    def get_workflow_status(self, workflow_id: int) -> dict:
        """
        Get detailed status of a workflow
        
        Args:
            workflow_id: ID of the workflow
            
        Returns:
            dict: Workflow status details
        """
        try:
            workflow = ScanWorkflow.objects.get(id=workflow_id)
            tasks = ScanTask.objects.filter(workflow=workflow).order_by('order')
            
            # Calculate progress percentage
            total_tasks = tasks.count()
            completed_tasks = tasks.filter(status__in=['completed', 'skipped', 'canceled']).count()
            progress = int(completed_tasks / total_tasks * 100) if total_tasks > 0 else 0
            
            # Format task results
            task_results = []
            for task in tasks:
                result_data = {}
                if task.result:
                    try:
                        result_data = json.loads(task.result)
                    except:
                        result_data = {'error': 'Invalid JSON result'}
                
                task_results.append({
                    'id': task.id,
                    'name': task.name,
                    'type': task.task_type,
                    'status': task.status,
                    'start_time': task.start_time.isoformat() if task.start_time else None,
                    'end_time': task.end_time.isoformat() if task.end_time else None,
                    'duration': str(task.end_time - task.start_time) if task.start_time and task.end_time else None,
                    'result_summary': self._summarize_task_result(task.task_type, result_data)
                })
            
            return {
                'id': workflow.id,
                'name': workflow.name,
                'target': workflow.target,
                'status': workflow.status,
                'scan_profile': workflow.scan_profile,
                'scheduled_time': workflow.scheduled_time.isoformat() if workflow.scheduled_time else None,
                'start_time': workflow.start_time.isoformat() if workflow.start_time else None,
                'end_time': workflow.end_time.isoformat() if workflow.end_time else None,
                'duration': str(workflow.end_time - workflow.start_time) if workflow.start_time and workflow.end_time else None,
                'progress': progress,
                'tasks': task_results,
                'notification_email': workflow.notification_email
            }
            
        except ScanWorkflow.DoesNotExist:
            logger.error(f"Workflow {workflow_id} not found")
            return {'error': 'Workflow not found'}
        except Exception as e:
            logger.error(f"Error getting workflow status: {str(e)}")
            return {'error': str(e)}
    
    def _summarize_task_result(self, task_type: str, result: dict) -> dict:
        """Generate a summary of task results for display"""
        summary = {}
        
        if task_type == 'subdomain_enumeration':
            summary['subdomains_found'] = result.get('subdomains_found', 0)
        elif task_type == 'port_scanning':
            hosts = result.get('results', [])
            open_ports = 0
            for host in hosts:
                open_ports += len([p for p in host.get('ports', []) if p.get('state') == 'open'])
            summary['hosts_scanned'] = len(hosts)
            summary['open_ports'] = open_ports
        elif task_type == 'service_identification':
            summary['services_found'] = len(result.get('services', []))
        elif task_type == 'vulnerability_scanning':
            vulns = result.get('vulnerabilities', [])
            severity_counts = {
                'critical': len([v for v in vulns if v.get('severity') == 'CRITICAL']),
                'high': len([v for v in vulns if v.get('severity') == 'HIGH']),
                'medium': len([v for v in vulns if v.get('severity') == 'MEDIUM']),
                'low': len([v for v in vulns if v.get('severity') == 'LOW'])
            }
            summary['vulnerabilities_found'] = len(vulns)
            summary['severity_counts'] = severity_counts
        elif task_type == 'network_mapping':
            summary['nodes'] = result.get('nodes', 0)
            summary['connections'] = result.get('connections', 0)
        elif task_type == 'report_generation':
            summary['report_types'] = result.get('report_types', [])
            summary['report_formats'] = result.get('report_formats', [])
            summary['report_ids'] = result.get('report_ids', {})
            
        return summary
    
    # automation/scheduler.py

import logging
import json
from datetime import datetime, timedelta
from django.utils import timezone
from croniter import croniter

from .models import ScheduledTask, ScanWorkflow
from .workflow_orchestrator import WorkflowOrchestrator

logger = logging.getLogger(__name__)

class ScanScheduler:
    """
    Handles scheduling and execution of recurring security scans
    """
    
    def __init__(self):
        self.orchestrator = WorkflowOrchestrator()
    
    def process_scheduled_tasks(self) -> int:
        """
        Check for scheduled tasks that need to be triggered and create workflows
        
        Returns:
            int: Number of workflows created
        """
        now = timezone.now()
        active_schedules = ScheduledTask.objects.filter(is_active=True)
        
        workflows_created = 0
        
        for schedule in active_schedules:
            try:
                # Skip if end date is set and has passed
                if schedule.end_date and schedule.end_date < now.date():
                    continue
                
                # Determine next execution time
                next_run = self._calculate_next_run(schedule)
                
                # Check if it's time to run (or overdue)
                if next_run and next_run <= now:
                    # Create a new workflow
                    name = f"{schedule.name} - {now.strftime('%Y-%m-%d %H:%M')}"
                    
                    workflow = self.orchestrator.create_workflow(
                        target=schedule.target,
                        name=name,
                        scan_profile=schedule.scan_profile,
                        notify_email=schedule.notification_email
                    )
                    
                    # Start the workflow immediately
                    self.orchestrator.start_workflow(workflow.id)
                    
                    # Update the schedule's last execution
                    schedule.last_execution = now
                    schedule.last_status = 'started'
                    schedule.last_workflow = workflow
                    schedule.save()
                    
                    logger.info(f"Created scheduled workflow {workflow.id} for schedule {schedule.id}")
                    workflows_created += 1
            
            except Exception as e:
                logger.error(f"Error processing scheduled task {schedule.id}: {str(e)}")
        
        return workflows_created
    
    def _calculate_next_run(self, schedule: ScheduledTask) -> datetime:
        """
        Calculate the next run time for a scheduled task
        
        Args:
            schedule: The scheduled task
            
        Returns:
            datetime: Next execution time or None if cannot be determined
        """
        now = timezone.now()
        
        # If never run, use start_date as base
        if schedule.last_execution is None:
            base_time = datetime.combine(schedule.start_date, datetime.min.time())
            base_time = timezone.make_aware(base_time)
            
            # If start date is in future, return that
            if base_time > now:
                return base_time
        else:
            base_time = schedule.last_execution
        
        # Calculate next run based on frequency
        if schedule.frequency == 'daily':
            # Add 24 hours to last execution
            return base_time + timedelta(days=1)
            
        elif schedule.frequency == 'weekly':
            # Add 7 days to last execution
            return base_time + timedelta(days=7)
            
        elif schedule.frequency == 'monthly':
            # Add roughly a month (30 days) to last execution
            return base_time + timedelta(days=30)
            
        elif schedule.frequency == 'custom' and schedule.cron_expression:
            try:
                # Use croniter to calculate next run based on cron expression
                cron = croniter(schedule.cron_expression, base_time)
                return cron.get_next(datetime)
            except Exception as e:
                logger.error(f"Error parsing cron expression for schedule {schedule.id}: {str(e)}")
                return None
        
        return None
    
    def create_scheduled_task(self, name: str, target: str, frequency: str, 
                          start_date: datetime.date, end_date=None, 
                          scan_profile: str='standard', cron_expression=None, 
                          notification_email=None, created_by=None) -> ScheduledTask:
        """
        Create a new scheduled task
        
        Args:
            name: Name of the scheduled task
            target: Target domain/IP
            frequency: Frequency (daily, weekly, monthly, custom)
            start_date: Start date
            end_date: End date (optional)
            scan_profile: Scan profile (quick, standard, full)
            cron_expression: Cron expression for custom frequency
            notification_email: Email to notify
            created_by: User who created the schedule
            
        Returns:
            ScheduledTask: The created scheduled task
        """
        if frequency == 'custom' and not cron_expression:
            raise ValueError("Cron expression is required for custom frequency")
        
        scheduled_task = ScheduledTask.objects.create(
            name=name,
            target=target,
            frequency=frequency,
            start_date=start_date,
            end_date=end_date,
            scan_profile=scan_profile,
            cron_expression=cron_expression,
            notification_email=notification_email,
            created_by=created_by
        )
        
        logger.info(f"Created scheduled task {scheduled_task.id} for {target}")
        return scheduled_task
    
    def update_scheduled_task(self, task_id: int, **kwargs) -> ScheduledTask:
        """
        Update a scheduled task
        
        Args:
            task_id: ID of the task to update
            **kwargs: Fields to update
            
        Returns:
            ScheduledTask: The updated task
        """
        try:
            task = ScheduledTask.objects.get(id=task_id)
            
            # Update fields
            for field, value in kwargs.items():
                if hasattr(task, field):
                    setattr(task, field, value)
            
            task.save()
            logger.info(f"Updated scheduled task {task_id}")
            return task
            
        except ScheduledTask.DoesNotExist:
            logger.error(f"Scheduled task {task_id} not found")
            raise ValueError(f"Scheduled task {task_id} not found")
    
    def delete_scheduled_task(self, task_id: int) -> bool:
        """
        Delete a scheduled task
        
        Args:
            task_id: ID of the task to delete
            
        Returns:
            bool: True if successfully deleted
        """
        try:
            task = ScheduledTask.objects.get(id=task_id)
            task.delete()
            logger.info(f"Deleted scheduled task {task_id}")
            return True
            
        except ScheduledTask.DoesNotExist:
            logger.error(f"Scheduled task {task_id} not found")
            return False
    
    def disable_scheduled_task(self, task_id: int) -> bool:
        """
        Disable a scheduled task
        
        Args:
            task_id: ID of the task to disable
            
        Returns:
            bool: True if successfully disabled
        """
        try:
            task = ScheduledTask.objects.get(id=task_id)
            task.is_active = False
            task.save()
            logger.info(f"Disabled scheduled task {task_id}")
            return True
            
        except ScheduledTask.DoesNotExist:
            logger.error(f"Scheduled task {task_id} not found")
            return False
    
    def enable_scheduled_task(self, task_id: int) -> bool:
        """
        Enable a scheduled task
        
        Args:
            task_id: ID of the task to enable
            
        Returns:
            bool: True if successfully enabled
        """
        try:
            task = ScheduledTask.objects.get(id=task_id)
            task.is_active = True
            task.save()
            logger.info(f"Enabled scheduled task {task_id}")
            return True
            
        except ScheduledTask.DoesNotExist:
            logger.error(f"Scheduled task {task_id} not found")
            return Falsefrom django.db import models

# Create your models here.
class Report(models.Model):
    title = models.CharField(max_length=255)
    creation_date = models.DateTimeField(auto_now_add=True)
    content = models.TextField()
    report_type = models.CharField(max_length=50)from datetime import datetime
import json
from django.core.serializers.json import DjangoJSONEncoder
from django.forms.models import model_to_dict
from .models import Report
from reconnaissance.models import Subdomain, PortScan
from vulnerability.models import Vulnerability

class ReportGenerator:
    def generate_report(self, report_type: str, target: str, output_format: str = 'json', scan_results: dict = None) -> Report:
        """
        Generate a security report
        
        Args:
            report_type: Type of report ('basic', 'detailed', 'executive')
            target: Target hostname/domain
            output_format: Format to generate (currently only 'json' is supported)
            scan_results: Optional dictionary containing scan results to include
            
        Returns:
            Report: The generated report object
        """
        # Clean target string
        target = target.strip()
        
        if report_type not in ['basic', 'detailed', 'executive']:
            report_type = 'basic'
        
        # Generate report content based on type
        if report_type == 'detailed':
            content = self.generate_detailed_report(target, scan_results)
        elif report_type == 'executive':
            content = self.generate_executive_report(target, scan_results)
        else:
            content = self.generate_basic_report(target, scan_results)
        
        # Properly serialize content to JSON string (all formats are currently JSON)
        json_content = json.dumps(content, cls=DjangoJSONEncoder)
        
        # Create and save the report
        report = Report.objects.create(
            title=f"{report_type.capitalize()} Security Report - {target}",
            content=json_content,
            report_type=f"{report_type}_{output_format}"
        )
        
        return report

    def _serialize_port_scan(self, port_scan):
        """Serialize port scan data"""
        return {
            'port': port_scan.port,
            'service': port_scan.service,
            'state': port_scan.state,
            'protocol': port_scan.protocol,
            'banner': port_scan.banner if port_scan.banner else '',
            'scan_date': port_scan.scan_date.isoformat()
        }

    def _get_open_ports(self, target: str) -> list:
        """Get all open ports with details"""
        open_ports = PortScan.objects.filter(
            host=target,
            state='open'
        ).order_by('port')
        return [self._serialize_port_scan(port) for port in open_ports]

    def _serialize_vulnerability(self, vuln):
        """Properly serialize a vulnerability instance"""
        return {
            'id': vuln.id,
            'target': vuln.target,
            'name': vuln.name,
            'description': vuln.description,
            'severity': vuln.severity,
            'vuln_type': vuln.vuln_type,
            'evidence': vuln.evidence,
            'discovery_date': vuln.discovery_date.isoformat(),
            'is_fixed': vuln.is_fixed,
            'fix_date': vuln.fix_date.isoformat() if vuln.fix_date else None,
            'source': vuln.source,
            'confidence': vuln.confidence,
            'cvss_score': vuln.cvss_score,
            'solution': vuln.solution if vuln.solution else '',
            'references': list(vuln.references) if vuln.references else [],
            'cwe': vuln.cwe if vuln.cwe else '',
            'metadata': dict(vuln.metadata) if vuln.metadata else {}
        }
    
    def generate_basic_report(self, target: str, scan_results: dict = None) -> dict:
        """Generate a basic security report"""
        subdomains = Subdomain.objects.filter(domain=target)
        port_scans = PortScan.objects.filter(host=target)
        vulnerabilities = Vulnerability.objects.filter(target=target, is_fixed=False)
        open_ports = self._get_open_ports(target)

        # Use provided scan_results if available
        if scan_results:
            # You can integrate the scan_results into your report here
            # Example: Extract vulnerabilities from the scan_results
            if 'vulnerabilities' in scan_results:
                # Process the vulnerabilities from scan_results
                pass

        return {
            'target': target,
            'scan_date': datetime.now().isoformat(),
            'summary': {
                'total_subdomains': subdomains.count(),
                'total_ports_scanned': port_scans.count(),
                'total_vulnerabilities': vulnerabilities.count(),
                'open_ports_count': len(open_ports)
            },
            'subdomains': [model_to_dict(sub, exclude=['id']) for sub in subdomains],
            'open_ports': open_ports,
            'port_scan_summary': {
                'total_scanned': port_scans.count(),
                'open': port_scans.filter(state='open').count(),
                'closed': port_scans.filter(state='closed').count(),
                'filtered': port_scans.filter(state='filtered').count()
            },
            'vulnerabilities': [self._serialize_vulnerability(vuln) for vuln in vulnerabilities]
        }

    def generate_detailed_report(self, target: str, scan_results: dict = None) -> dict:
        """Generate a detailed security report"""
        basic_report = self.generate_basic_report(target, scan_results)
        vulnerabilities = Vulnerability.objects.filter(target=target, is_fixed=False)
        
        # Enhanced port analysis
        open_ports = self._get_open_ports(target)
        port_risks = self._analyze_port_risks(open_ports)
        
        detailed_info = {
            'port_analysis': {
                'high_risk_ports': port_risks['high_risk'],
                'medium_risk_ports': port_risks['medium_risk'],
                'low_risk_ports': port_risks['low_risk']
            },
            'vulnerability_severity': {
                'critical': vulnerabilities.filter(severity='CRITICAL').count(),
                'high': vulnerabilities.filter(severity='HIGH').count(),
                'medium': vulnerabilities.filter(severity='MEDIUM').count(),
                'low': vulnerabilities.filter(severity='LOW').count()
            },
            'vulnerability_sources': {
                'internal': vulnerabilities.filter(source='internal').count(),
                'zap': vulnerabilities.filter(source='zap').count(),
                'nuclei': vulnerabilities.filter(source='nuclei').count(),
                'multiple': vulnerabilities.exclude(source__in=['internal', 'zap', 'nuclei']).count()
            }
        }
        
        return {**basic_report, 'detailed_info': detailed_info}

    def _analyze_port_risks(self, open_ports: list) -> dict:
        """Analyze risks associated with open ports"""
        high_risk_ports = [21, 23, 445, 3389]  # FTP, Telnet, SMB, RDP
        medium_risk_ports = [22, 25, 110, 143]  # SSH, SMTP, POP3, IMAP
        
        port_risks = {
            'high_risk': [],
            'medium_risk': [],
            'low_risk': []
        }
        
        for port_data in open_ports:
            port = port_data['port']
            if port in high_risk_ports:
                port_risks['high_risk'].append(port_data)
            elif port in medium_risk_ports:
                port_risks['medium_risk'].append(port_data)
            else:
                port_risks['low_risk'].append(port_data)
                
        return port_risks

    def generate_executive_report(self, target: str, scan_results: dict = None) -> dict:
        """Generate an executive summary report"""
        basic_report = self.generate_basic_report(target, scan_results)
        vulnerabilities = Vulnerability.objects.filter(target=target, is_fixed=False)
        high_vulns = vulnerabilities.filter(severity='HIGH')
        critical_vulns = vulnerabilities.filter(severity='CRITICAL')
        
        # Enhanced metrics
        risk_metrics = {
            'critical_severity_vulns': critical_vulns.count(),
            'high_severity_vulns': high_vulns.count(),
            'open_ports': len(basic_report['open_ports']),
            'total_vulnerabilities': vulnerabilities.count(),
            'high_risk_ports': len(self._analyze_port_risks(basic_report['open_ports'])['high_risk'])
        }
        
        # Combine critical and high vulnerabilities in findings
        top_findings = []
        for vuln in critical_vulns:
            top_findings.append(self._serialize_vulnerability(vuln))
        if len(top_findings) < 5:  # Limit to 5 findings total
            for vuln in high_vulns[:5-len(top_findings)]:
                top_findings.append(self._serialize_vulnerability(vuln))
        
        executive_summary = {
            'risk_level': self._calculate_risk_level(risk_metrics),
            'critical_findings': top_findings,
            'risk_metrics': risk_metrics,
            'recommendations': self._generate_recommendations(risk_metrics, basic_report)
        }
        
        return {**basic_report, 'executive_summary': executive_summary}
    
    def _calculate_risk_level(self, metrics):
        """Calculate overall risk level based on metrics"""
        if metrics['critical_severity_vulns'] > 0:
            return 'CRITICAL'
        elif metrics['high_severity_vulns'] > 2:
            return 'HIGH'
        elif metrics['high_severity_vulns'] > 0 or metrics['high_risk_ports'] > 0:
            return 'MEDIUM'
        else:
            return 'LOW'
    
    def _generate_recommendations(self, metrics, basic_report):
        """Generate security recommendations based on findings"""
        recommendations = []
        
        # Add recommendations based on findings
        if metrics['critical_severity_vulns'] > 0 or metrics['high_severity_vulns'] > 0:
            recommendations.append({
                'title': 'Address High and Critical Vulnerabilities',
                'description': 'Fix identified critical and high vulnerabilities as a priority.'
            })
            
        if metrics['high_risk_ports'] > 0:
            recommendations.append({
                'title': 'Secure High Risk Ports',
                'description': 'Restrict access to high risk services or replace with more secure alternatives.'
            })
            
        # Always add general recommendations
        recommendations.append({
            'title': 'Regular Security Testing',
            'description': 'Perform regular security assessments to identify new vulnerabilities.'
        })
        
        return recommendations
    
    from django.shortcuts import render, get_object_or_404
from django.http import JsonResponse, HttpResponse, FileResponse
from django.views import View
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from django.template.loader import render_to_string
from django.conf import settings
from .models import Report
from .report_generator import ReportGenerator
import json
import os
import logging
import tempfile
from datetime import datetime
from django.core.serializers.json import DjangoJSONEncoder
from weasyprint import HTML, CSS

logger = logging.getLogger(__name__)

@method_decorator(csrf_exempt, name='dispatch')
class GenerateReportView(View):
    def __init__(self):
        super().__init__()
        self.generator = ReportGenerator()

    def post(self, request):
        try:
            data = json.loads(request.body)
            report_type = data.get('report_type', 'basic')
            target = data.get('target')

            if not target:
                return JsonResponse({
                    'error': 'Target is required'
                }, status=400)

            # Generate and save the report
            report = self.generator.generate_report(report_type, target)

            return JsonResponse({
                'status': 'success',
                'message': 'Report generated successfully',
                'report_id': report.id,
                'report_type': report_type,
                'report_title': report.title
            })

        except Exception as e:
            return JsonResponse({
                'error': 'Report generation failed',
                'details': str(e)
            }, status=500)

class ReportListView(View):
    def get(self, request):
        reports = Report.objects.all().values(
            'id', 'title', 'creation_date', 'report_type'
        ).order_by('-creation_date')
        return JsonResponse(list(reports), safe=False)

@method_decorator(csrf_exempt, name='dispatch')
class DownloadReportView(View):
    def get(self, request, report_id):
        try:
            report = Report.objects.get(id=report_id)
            
            try:
                # Parse the stored JSON content
                report_content = json.loads(report.content)
            except json.JSONDecodeError as e:
                return JsonResponse({
                    'error': 'Invalid report format',
                    'details': str(e)
                }, status=500)
            
            # Format the complete report
            formatted_report = {
                'id': report.id,
                'title': report.title,
                'content': report_content,
                'creation_date': report.creation_date.isoformat(),
                'report_type': report.report_type
            }
            
            # Handle download request
            if request.GET.get('download') == 'true':
                try:
                    # Create a temporary file
                    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as tmp_file:
                        # Write formatted JSON to temp file
                        json.dump(formatted_report, tmp_file, indent=2, cls=DjangoJSONEncoder)
                    
                    # Prepare file response
                    filename = f"security_report_{report.id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                    response = FileResponse(
                        open(tmp_file.name, 'rb'),
                        content_type='application/json',
                        as_attachment=True,
                        filename=filename
                    )
                    
                    # Clean up temp file after response is sent
                    os.unlink(tmp_file.name)
                    
                    return response
                    
                except Exception as e:
                    return JsonResponse({
                        'error': 'Error creating download file',
                        'details': str(e)
                    }, status=500)
            
            # Return regular JSON response
            return JsonResponse(formatted_report)
            
        except Report.DoesNotExist:
            return JsonResponse({
                'error': 'Report not found'
            }, status=404)
        except Exception as e:
            return JsonResponse({
                'error': 'Error retrieving report',
                'details': str(e)
            }, status=500)

def download_pdf_view(request, report_id):
    """Generate and download a PDF version of the report"""
    report = get_object_or_404(Report, id=report_id)
    
    try:
        # Parse the JSON content
        import json
        report_data = json.loads(report.content)
        
        # Get workflow info if available
        workflow_id = request.GET.get('workflow_id')
        workflow = None
        tasks = []
        task_results = []
        
        if workflow_id:
            from automation.models import ScanWorkflow, ScanTask
            try:
                workflow = ScanWorkflow.objects.get(id=workflow_id)
                tasks = ScanTask.objects.filter(workflow_id=workflow_id).order_by('order')
                
                # Collect task results
                for task in tasks:
                    result_data = {}
                    if task.result:
                        try:
                            result_data = json.loads(task.result)
                        except:
                            result_data = {'error': 'Invalid JSON result'}
                    
                    task_results.append({
                        'id': task.id,
                        'name': task.name,
                        'type': task.task_type,
                        'status': task.status,
                        'start_time': task.start_time,
                        'end_time': task.end_time,
                        'duration': task.duration,
                        'result_data': result_data
                    })
            except ScanWorkflow.DoesNotExist:
                pass
        
        # Render HTML content for PDF
        html_string = render_to_string('reporting/pdf_report.html', {
            'report': report,
            'report_data': report_data,
            'workflow': workflow,
            'tasks': tasks,
            'task_results': task_results
        })
        
        # Create PDF from HTML
        html = HTML(string=html_string, base_url=request.build_absolute_uri('/'))
        result = html.write_pdf()
        
        # Create response with PDF content
        response = HttpResponse(result, content_type='application/pdf')
        response['Content-Disposition'] = f'attachment; filename="report_{report_id}.pdf"'
        return response
        
    except Exception as e:
        logger.error(f"Error generating PDF for report {report_id}: {str(e)}")
        return HttpResponse(f"Error generating PDF: {str(e)}", status=500)    

def view_html_report(request, report_id):
    """View an HTML report"""
    report = get_object_or_404(Report, id=report_id)
    
    try:
        # Parse the JSON content
        import json
        report_data = json.loads(report.content)
        
        # Render the report using a template
        return render(request, 'reporting/html_report.html', {
            'report': report,
            'report_data': report_data
        })
    except:
        # If JSON parsing fails, just display the raw content
        return HttpResponse(f"<pre>{report.content}</pre>")
    
def comprehensive_html_report(request, report_id):
    """View a comprehensive HTML report that includes all phase results"""
    report = get_object_or_404(Report, id=report_id)
    
    try:
        # Parse the JSON content
        import json
        report_data = json.loads(report.content)
        
        # Get the workflow id from the URL query parameter
        workflow_id = request.GET.get('workflow_id')
        
        # If workflow_id provided, fetch all task results
        if workflow_id:
            from automation.models import ScanWorkflow, ScanTask
            
            try:
                workflow = ScanWorkflow.objects.get(id=workflow_id)
                tasks = ScanTask.objects.filter(workflow_id=workflow_id).order_by('order')
                
                # Collect all task results
                task_results = []
                for task in tasks:
                    result_data = {}
                    if task.result:
                        try:
                            result_data = json.loads(task.result)
                        except:
                            result_data = {'error': 'Invalid JSON result'}
                    
                    task_results.append({
                        'id': task.id,
                        'name': task.name,
                        'type': task.task_type,
                        'status': task.status,
                        'start_time': task.start_time,
                        'end_time': task.end_time,
                        'duration': task.duration,
                        'result_data': result_data
                    })
                
                # Render the comprehensive report
                return render(request, 'reporting/comprehensive_report.html', {
                    'report': report,
                    'report_data': report_data,
                    'workflow': workflow,
                    'tasks': tasks,
                    'task_results': task_results
                })
                
            except ScanWorkflow.DoesNotExist:
                # Fall back to regular report if workflow not found
                pass
        
        # Default to standard report if no workflow_id or workflow not found
        return render(request, 'reporting/html_report.html', {
            'report': report,
            'report_data': report_data
        })
    except Exception as e:
        logger.error(f"Error rendering comprehensive report: {str(e)}")
        # If JSON parsing fails, just display the raw content
        return HttpResponse(f"<pre>{report.content}</pre>")from django.db import models
from reconnaissance.models import Subdomain, Service

class NetworkNode(models.Model):
    """Represents a node in the network topology"""
    NODE_TYPES = [
        ('host', 'Host'),
        ('subdomain', 'Subdomain'),
        ('service', 'Service'),
        ('gateway', 'Gateway'),
    ]

    name = models.CharField(max_length=255)
    domain = models.CharField(max_length=255)  # Added this field
    node_type = models.CharField(max_length=20, choices=NODE_TYPES)
    ip_address = models.GenericIPAddressField(null=True, blank=True)
    metadata = models.JSONField(default=dict)
    last_seen = models.DateTimeField(auto_now=True)
    is_active = models.BooleanField(default=True)

    class Meta:
        indexes = [
            models.Index(fields=['domain']),
            models.Index(fields=['name']),
            models.Index(fields=['node_type']),
        ]

    def __str__(self):
        return f"{self.name} ({self.node_type})"

class NetworkConnection(models.Model):
    """Represents a connection between network nodes"""
    CONNECTION_TYPES = [
        ('direct', 'Direct Connection'),
        ('gateway', 'Gateway Connection'),
        ('service', 'Service Connection'),
    ]

    source = models.ForeignKey(NetworkNode, on_delete=models.CASCADE, related_name='outgoing_connections')
    target = models.ForeignKey(NetworkNode, on_delete=models.CASCADE, related_name='incoming_connections')
    connection_type = models.CharField(max_length=20, choices=CONNECTION_TYPES)
    metadata = models.JSONField(default=dict)
    last_seen = models.DateTimeField(auto_now=True)
    is_active = models.BooleanField(default=True)

    class Meta:
        unique_together = ('source', 'target', 'connection_type')
        indexes = [
            models.Index(fields=['connection_type']),
            models.Index(fields=['is_active']),
        ]

    def __str__(self):
        return f"{self.source.name} -> {self.target.name} ({self.connection_type})"from typing import Dict, List, Optional
import logging
from django.db import transaction
from reconnaissance.models import Subdomain, Service
from .models import NetworkNode, NetworkConnection
import socket
from subprocess import Popen, PIPE
from datetime import datetime

class TopologyMapper:
    def __init__(self):
        self.logger = logging.getLogger(__name__)

    @transaction.atomic
    def create_network_map(self, target_domain: str) -> Dict:
        """Creates or updates the network topology for a target domain"""
        try:
            # Clean up old nodes and connections for this domain
            self._cleanup_old_data(target_domain)
            
            # Get all subdomains and services
            subdomains = Subdomain.objects.filter(domain=target_domain)
            
            # Create or update the main domain node
            try:
                domain_ip = socket.gethostbyname(target_domain)
            except:
                domain_ip = None

            domain_node = self._get_or_create_node(
                name=target_domain,
                domain=target_domain,
                node_type='host',
                ip_address=domain_ip
            )

            # Process subdomains
            subdomain_nodes = []
            for subdomain in subdomains:
                subdomain_node = self._get_or_create_node(
                    name=subdomain.subdomain,
                    domain=target_domain,
                    node_type='subdomain',
                    ip_address=subdomain.ip_address
                )
                subdomain_nodes.append(subdomain_node)
                
                # Create connection to domain
                self._create_connection(domain_node, subdomain_node, 'direct')

                # Process services for this subdomain
                services = Service.objects.filter(host=subdomain.subdomain)
                for service in services:
                    service_node = self._get_or_create_node(
                        name=f"{service.name}:{service.port}",
                        domain=target_domain,
                        node_type='service',
                        metadata={
                            'port': service.port,
                            'protocol': service.protocol,
                            'version': service.version
                        }
                    )
                    self._create_connection(subdomain_node, service_node, 'service')

            # Map network routes
            self._map_network_routes(domain_node, subdomain_nodes)

            # Get current node and connection counts for this domain
            current_nodes = NetworkNode.objects.filter(domain=target_domain, is_active=True).count()
            current_connections = NetworkConnection.objects.filter(
                source__domain=target_domain,
                is_active=True
            ).count()

            return {
                'status': 'success',
                'nodes': current_nodes,
                'connections': current_connections,
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            self.logger.error(f"Error creating network map: {str(e)}")
            return {
                'status': 'error',
                'error': str(e)
            }

    def _cleanup_old_data(self, target_domain: str):
        """Clean up old nodes and connections for the target domain"""
        # Deactivate old nodes
        NetworkNode.objects.filter(domain=target_domain).update(is_active=False)
        
        # Deactivate old connections
        NetworkConnection.objects.filter(
            source__domain=target_domain
        ).update(is_active=False)

    def _get_or_create_node(self, name: str, domain: str, node_type: str, 
                           ip_address: Optional[str] = None, 
                           metadata: Optional[Dict] = None) -> NetworkNode:
        """Gets existing node or creates a new one"""
        node, created = NetworkNode.objects.get_or_create(
            name=name,
            domain=domain,
            node_type=node_type,
            defaults={
                'ip_address': ip_address,
                'metadata': metadata or {},
                'is_active': True
            }
        )
        if not created:
            node.ip_address = ip_address
            node.is_active = True
            if metadata:
                node.metadata.update(metadata)
            node.save()
        return node

    def _create_connection(self, source: NetworkNode, target: NetworkNode, 
                          connection_type: str, metadata: Optional[Dict] = None) -> NetworkConnection:
        """Creates or updates a connection between nodes"""
        connection, created = NetworkConnection.objects.get_or_create(
            source=source,
            target=target,
            connection_type=connection_type,
            defaults={
                'metadata': metadata or {},
                'is_active': True
            }
        )
        if not created:
            connection.is_active = True
            if metadata:
                connection.metadata.update(metadata)
            connection.save()
        return connection

    def _map_network_routes(self, domain_node: NetworkNode, subdomain_nodes: List[NetworkNode]):
        """Maps network routes between nodes using traceroute"""
        for subdomain_node in subdomain_nodes:
            if not subdomain_node.ip_address:
                continue

            try:
                # Create direct connection for local targets
                if any(local in subdomain_node.name for local in ['localhost', '127.0.0.1']):
                    self._create_connection(domain_node, subdomain_node, 'direct')
                    continue

                # For remote targets, try traceroute
                hops = self._run_traceroute(subdomain_node.ip_address)
                previous_node = domain_node

                for i, hop_ip in enumerate(hops):
                    gateway_node = self._get_or_create_node(
                        name=f"gateway_{hop_ip}",
                        domain=domain_node.domain,
                        node_type='gateway',
                        ip_address=hop_ip,
                        metadata={'hop_number': i + 1}
                    )
                    
                    self._create_connection(previous_node, gateway_node, 'gateway')
                    previous_node = gateway_node

                # Connect last hop to subdomain
                if previous_node != domain_node:
                    self._create_connection(previous_node, subdomain_node, 'gateway')
                else:
                    self._create_connection(domain_node, subdomain_node, 'direct')

            except Exception as e:
                self.logger.error(f"Error mapping route to {subdomain_node.name}: {str(e)}")
                self._create_connection(domain_node, subdomain_node, 'direct')

    def _run_traceroute(self, ip_address: str) -> List[str]:
        """Run traceroute command and return list of hops"""
        try:
            process = Popen(['traceroute', '-n', ip_address], stdout=PIPE, stderr=PIPE)
            output, error = process.communicate()
            
            if process.returncode != 0:
                self.logger.error(f"Traceroute failed: {error.decode()}")
                return []
                
            hops = []
            for line in output.decode().split('\n')[1:]:
                if line.strip():
                    parts = line.split()
                    if len(parts) >= 2:
                        ip = parts[1]
                        if ip != '*':
                            hops.append(ip)
            return hops
        except Exception as e:
            self.logger.error(f"Error running traceroute: {str(e)}")
            return []from django.http import JsonResponse
from django.views.decorators.http import require_http_methods
from .models import NetworkNode, NetworkConnection
from .topology_mapper import TopologyMapper
import logging

logger = logging.getLogger(__name__)

@require_http_methods(["GET"])
def get_network_topology(request, target_domain):
    """Get network topology for visualization"""
    try:
        # Create/update network map
        mapper = TopologyMapper()
        result = mapper.create_network_map(target_domain)
        
        if result['status'] == 'error':
            return JsonResponse(result, status=500)
            
        # Prepare nodes and links for visualization
        nodes = []
        links = []
        
        # Get domain-specific nodes
        domain_nodes = NetworkNode.objects.filter(
            domain=target_domain,
            is_active=True
        )
        
        # Get all nodes
        for node in domain_nodes:
            nodes.append({
                'id': node.id,
                'name': node.name,
                'type': node.node_type,
                'ip': node.ip_address,
                'metadata': node.metadata
            })
            
        # Get all connections for domain nodes
        node_ids = domain_nodes.values_list('id', flat=True)
        connections = NetworkConnection.objects.filter(
            source_id__in=node_ids,
            is_active=True
        )
        
        for conn in connections:
            links.append({
                'source': conn.source_id,
                'target': conn.target_id,
                'type': conn.connection_type,
                'metadata': conn.metadata
            })
            
        return JsonResponse({
            'status': 'success',
            'nodes': nodes,
            'links': links,
            'stats': {
                'total_nodes': len(nodes),
                'total_links': len(links)
            }
        })
        
    except Exception as e:
        logger.error(f"Error getting network topology: {str(e)}")
        return JsonResponse({
            'status': 'error',
            'error': str(e)
        }, status=500)

@require_http_methods(["GET"])
def get_node_details(request, node_id):
    """Get detailed information about a specific node"""
    try:
        node = NetworkNode.objects.get(id=node_id, is_active=True)
        
        # Get connected nodes
        connected_nodes = NetworkNode.objects.filter(
            outgoing_connections__target=node,
            is_active=True
        ) | NetworkNode.objects.filter(
            incoming_connections__source=node,
            is_active=True
        )
        
        return JsonResponse({
            'status': 'success',
            'node': {
                'id': node.id,
                'name': node.name,
                'domain': node.domain,
                'type': node.node_type,
                'ip': node.ip_address,
                'metadata': node.metadata,
                'last_seen': node.last_seen.isoformat(),
                'connected_nodes': [
                    {
                        'id': n.id,
                        'name': n.name,
                        'type': n.node_type,
                        'connection_type': n.outgoing_connections.filter(target=node).first().connection_type
                        if n.outgoing_connections.filter(target=node).exists()
                        else n.incoming_connections.filter(source=node).first().connection_type
                    }
                    for n in connected_nodes
                ]
            }
        })
        
    except NetworkNode.DoesNotExist:
        return JsonResponse({
            'status': 'error',
            'error': 'Node not found'
        }, status=404)
    except Exception as e:
        logger.error(f"Error getting node details: {str(e)}")
        return JsonResponse({
            'status': 'error',
            'error': str(e)
        }, status=500)

@require_http_methods(["GET"])
def get_network_stats(request, target_domain):
    """Get statistics about the network topology"""
    try:
        nodes = NetworkNode.objects.filter(
            domain=target_domain,
            is_active=True
        )
        node_ids = nodes.values_list('id', flat=True)
        connections = NetworkConnection.objects.filter(
            source_id__in=node_ids,
            is_active=True
        )
        
        stats = {
            'total_nodes': nodes.count(),
            'nodes_by_type': {},
            'total_connections': connections.count(),
            'connections_by_type': {},
            'domain_info': {
                'host_nodes': nodes.filter(node_type='host').count(),
                'subdomain_nodes': nodes.filter(node_type='subdomain').count(),
                'service_nodes': nodes.filter(node_type='service').count(),
                'gateway_nodes': nodes.filter(node_type='gateway').count(),
            }
        }
        
        # Count nodes by type
        for node_type, _ in NetworkNode.NODE_TYPES:
            stats['nodes_by_type'][node_type] = nodes.filter(node_type=node_type).count()
            
        # Count connections by type
        for conn_type, _ in NetworkConnection.CONNECTION_TYPES:
            stats['connections_by_type'][conn_type] = connections.filter(connection_type=conn_type).count()
            
        return JsonResponse({
            'status': 'success',
            'stats': stats
        })
        
    except Exception as e:
        logger.error(f"Error getting network stats: {str(e)}")
        return JsonResponse({
            'status': 'error',
            'error': str(e)
        }, status=500)from django.db import models
from django.core.validators import MinValueValidator, MaxValueValidator
import logging

logger = logging.getLogger(__name__)

class Vulnerability(models.Model):
    SEVERITY_CHOICES = [
        ('LOW', 'Low'),
        ('MEDIUM', 'Medium'),
        ('HIGH', 'High'),
        ('CRITICAL', 'Critical'),
    ]

    # Updated source choices to include more options
    SOURCE_CHOICES = [
        ('internal', 'Internal Scanner'),
        ('zap', 'OWASP ZAP'),
        ('nuclei', 'Nuclei Scanner'),
        ('openvas', 'OpenVAS Scanner'),
        ('manual', 'Manual Entry'),
        ('multiple', 'Multiple Sources')  # For correlated findings
    ]

    # Basic Information
    target = models.CharField(max_length=255, db_index=True)
    name = models.CharField(max_length=255)
    description = models.TextField()
    severity = models.CharField(max_length=10, choices=SEVERITY_CHOICES, db_index=True)
    vuln_type = models.CharField(max_length=50, db_index=True)
    
    # Evidence and Details
    evidence = models.TextField()
    solution = models.TextField(blank=True)
    references = models.JSONField(default=list)
    
    # Source and Confidence - increased max_length to accommodate combined sources
    source = models.CharField(max_length=100)  # Removed choices constraint and increased length
    confidence = models.CharField(max_length=50, default='medium')
    
    # Status and Tracking
    discovery_date = models.DateTimeField(auto_now_add=True)
    is_fixed = models.BooleanField(default=False, db_index=True)
    fix_date = models.DateTimeField(null=True, blank=True)
    notes = models.TextField(blank=True)
    
    # Additional Metadata
    cwe = models.CharField(max_length=50, blank=True)
    cvss_score = models.FloatField(
        null=True, 
        blank=True,
        validators=[MinValueValidator(0.0), MaxValueValidator(10.0)]
    )
    metadata = models.JSONField(default=dict)  # Added metadata field
    
    def __str__(self):
        return f"{self.target} - {self.name} ({self.severity})"

    def save(self, *args, **kwargs):
        # Ensure severity is uppercase
        if self.severity:
            self.severity = self.severity.upper()
        
        # Set fix_date when vulnerability is marked as fixed
        if self.is_fixed and not self.fix_date:
            from django.utils import timezone
            self.fix_date = timezone.now()
        
        # Handle source field for multiple sources
        if ',' in self.source:
            # Store original sources in metadata for reference
            if not self.metadata:
                self.metadata = {}
            self.metadata['original_sources'] = self.source.split(',')
            
        super().save(*args, **kwargs)

    @classmethod
    def deduplicate_vulnerabilities(cls, target):
        """
        Deduplicate vulnerabilities for a target by merging duplicates
        
        Args:
            target: The target hostname/domain
            
        Returns:
            dict: Statistics about deduplication
        """
        from django.db.models import Count
        from django.db import transaction
        
        # Keep track of statistics
        stats = {
            'original_count': cls.objects.filter(target=target).count(),
            'merged_count': 0,
            'final_count': 0
        }
        
        try:
            with transaction.atomic():
                # Find groups of vulnerabilities with the same name, type, and severity
                # These are candidates for deduplication
                duplicate_groups = cls.objects.filter(target=target).values(
                    'name', 'vuln_type', 'severity'
                ).annotate(
                    count=Count('id')
                ).filter(count__gt=1)
                
                # Process each group
                for group in duplicate_groups:
                    duplicates = cls.objects.filter(
                        target=target,
                        name=group['name'],
                        vuln_type=group['vuln_type'],
                        severity=group['severity']
                    ).order_by('discovery_date')
                    
                    # Keep the first (oldest) vulnerability as the canonical one
                    if duplicates.count() > 1:
                        primary_vuln = duplicates.first()
                        
                        # Process other duplicates
                        for dup in duplicates[1:]:
                            # Combine sources if different
                            sources = set(primary_vuln.source.split(','))
                            for source in dup.source.split(','):
                                sources.add(source)
                            primary_vuln.source = ','.join(sorted(sources))
                            
                            # Use the higher CVSS score if available
                            if dup.cvss_score and (not primary_vuln.cvss_score or dup.cvss_score > primary_vuln.cvss_score):
                                primary_vuln.cvss_score = dup.cvss_score
                            
                            # Take the most recent evidence if available
                            if dup.evidence and len(dup.evidence) > len(primary_vuln.evidence):
                                primary_vuln.evidence = dup.evidence
                                
                            # Merge references
                            primary_refs = set(primary_vuln.references)
                            for ref in dup.references:
                                primary_refs.add(ref)
                            primary_vuln.references = list(primary_refs)
                            
                            # Track the merged duplicate in metadata
                            if not primary_vuln.metadata:
                                primary_vuln.metadata = {}
                            if 'merged_duplicates' not in primary_vuln.metadata:
                                primary_vuln.metadata['merged_duplicates'] = []
                            primary_vuln.metadata['merged_duplicates'].append({
                                'id': dup.id,
                                'discovery_date': dup.discovery_date.isoformat(),
                                'source': dup.source
                            })
                            
                            # Delete the duplicate
                            dup.delete()
                            stats['merged_count'] += 1
                        
                        # Save the updated primary vulnerability
                        primary_vuln.save()
            
            # Calculate final counts
            stats['final_count'] = cls.objects.filter(target=target).count()
            return stats
            
        except Exception as e:
            logger.error(f"Error deduplicating vulnerabilities: {str(e)}")
            return {'error': str(e)}
    
    @property
    def age_in_days(self):
        from django.utils import timezone
        return (timezone.now() - self.discovery_date).days

    @property
    def risk_score(self):
        """Calculate risk score based on CVSS and age"""
        base_score = self.cvss_score if self.cvss_score else {
            'CRITICAL': 9.0,
            'HIGH': 7.0,
            'MEDIUM': 5.0,
            'LOW': 3.0
        }.get(self.severity, 1.0)
        
        # Age factor: 1.0 - 2.0 based on age (caps at 90 days)
        age_factor = min(1 + (self.age_in_days / 90), 2.0)
        
        return base_score * age_factor
    
    @property
    def source_list(self):
        """Return the source as a list for easier filtering"""
        return self.source.split(',')

class NucleiFinding(models.Model):
    SEVERITY_CHOICES = [
        ('CRITICAL', 'Critical'),
        ('HIGH', 'High'),
        ('MEDIUM', 'Medium'),
        ('LOW', 'Low'),
        ('INFO', 'Info'),
    ]

    template_id = models.CharField(max_length=255)
    name = models.CharField(max_length=255)
    severity = models.CharField(max_length=10, choices=SEVERITY_CHOICES)
    finding_type = models.CharField(max_length=50)
    host = models.CharField(max_length=255)
    matched_at = models.URLField(max_length=500)
    description = models.TextField()
    tags = models.JSONField(default=list)
    references = models.JSONField(default=list)
    cwe = models.CharField(max_length=50, blank=True)
    cvss_score = models.FloatField(null=True, blank=True)
    discovery_date = models.DateTimeField(auto_now_add=True)
    scan_id = models.CharField(max_length=100)
    target = models.CharField(max_length=255)
    
    class Meta:
        indexes = [
            models.Index(fields=['template_id']),
            models.Index(fields=['severity']),
            models.Index(fields=['discovery_date']),
            models.Index(fields=['target']),
        ]from typing import Dict, List
import logging
from datetime import datetime
from .scanner import VulnerabilityScanner
from .zap_manager import ZAPManager
from .models import Vulnerability, NucleiFinding
from .nuclei_scanner import NucleiScanner
from .correlation import VulnerabilityCorrelator  # Import the enhanced correlator

class UnifiedVulnerabilityScanner:
    # Define valid scan types
    VALID_SCAN_TYPES = {
        'quick': 'Fast scan with basic checks',
        'standard': 'Standard comprehensive scan',
        'full': 'Full detailed scan with all checks'
    }

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.internal_scanner = VulnerabilityScanner()
        self.zap_manager = ZAPManager()
        self.correlator = VulnerabilityCorrelator()  # Initialize the correlator
        try:
            self.nuclei_scanner = NucleiScanner()
        except Exception as e:
            self.logger.error(f"Failed to initialize Nuclei scanner: {str(e)}")
            self.nuclei_scanner = None

    def validate_scan_type(self, scan_type: str) -> str:
        """Validate scan type and return normalized version"""
        if not scan_type:
            return 'standard'  # Default scan type
            
        normalized_type = scan_type.lower().strip()
        if normalized_type not in self.VALID_SCAN_TYPES:
            raise ValueError(
                f"Invalid scan type: '{scan_type}'. Valid types are: {list(self.VALID_SCAN_TYPES.keys())}"
            )
        return normalized_type

    def get_scan_types(self) -> Dict[str, str]:
        """Return available scan types and their descriptions"""
        return self.VALID_SCAN_TYPES

    def scan_target(self, target: str, scan_type: str = 'standard', 
                   include_zap: bool = True, include_nuclei: bool = True,
                   nuclei_scan_type: str = 'basic',
                   use_advanced_correlation: bool = True) -> Dict:
        """
        Perform a comprehensive scan using multiple scanners based on parameters
        """
        try:
            # Validate scan type first
            validated_scan_type = self.validate_scan_type(scan_type)
            
            results = {
                'target': target,
                'scan_start': datetime.now().isoformat(),
                'vulnerabilities': [],
                'scanners_used': ['internal'],
                'summary': {
                    'high': 0,
                    'medium': 0,
                    'low': 0,
                    'total': 0
                },
                'correlation': {}
            }

            # Track findings from each scanner for correlation
            internal_results = []
            zap_results = []
            nuclei_results = []

            # Run internal scanner with validated scan type
            self.logger.info(f"Starting internal scanner with scan type: {validated_scan_type}")
            internal_scan = self.internal_scanner.scan_target(target, validated_scan_type)
            
            if internal_scan.get('vulnerabilities'):
                internal_results = internal_scan['vulnerabilities']
                results['scanners_used'].append('internal')
                self.logger.info(f"Internal scanner found {len(internal_results)} vulnerabilities")

            # Run ZAP scan if requested
            if include_zap:
                self.logger.info("Starting ZAP scanner")
                if self.zap_manager.ensure_zap_running():
                    results['scanners_used'].append('zap')
                    zap_scan = self.zap_manager.run_scan(target)
                    if zap_scan.get('status') == 'success' and zap_scan.get('alerts'):
                        zap_results = zap_scan['alerts']
                        self.logger.info(f"ZAP scanner found {len(zap_results)} vulnerabilities")
                else:
                    self.logger.warning("ZAP scanner not available")

            # Run Nuclei scan if requested
            if include_nuclei and self.nuclei_scanner:
                try:
                    self.logger.info(f"Starting Nuclei scan with type: {nuclei_scan_type}")
                    if nuclei_scan_type.lower() == 'advanced':
                        nuclei_scan = self.nuclei_scanner.run_advanced_scan(target)
                    else:
                        nuclei_scan = self.nuclei_scanner.run_basic_scan(target)
                    
                    if nuclei_scan.get('status') == 'success' and nuclei_scan.get('findings'):
                        results['scanners_used'].append('nuclei')
                        nuclei_results = nuclei_scan['findings']
                        self.logger.info(f"Nuclei scanner found {len(nuclei_results)} vulnerabilities")
                except Exception as e:
                    self.logger.error(f"Nuclei scan failed: {str(e)}")

            # Use enhanced correlation if enabled
            if use_advanced_correlation:
                self.logger.info("Using advanced correlation for findings")
                correlation_result = self.correlator.correlate_findings(
                    internal_results=internal_results,
                    zap_results=zap_results,
                    nuclei_results=nuclei_results,
                    target=target
                )
                
                if correlation_result.get('status') == 'success':
                    results['vulnerabilities'] = correlation_result.get('findings', [])
                    results['correlation'] = {
                        'original_count': correlation_result.get('original_count', 0),
                        'correlated_count': correlation_result.get('correlated_count', 0),
                        'reduction_percentage': correlation_result.get('statistics', {}).get('reduction_percentage', 0),
                        'stats': correlation_result.get('statistics', {})
                    }
                    self.logger.info(f"Correlation reduced findings from {correlation_result.get('original_count', 0)} to {correlation_result.get('correlated_count', 0)}")
                else:
                    # Fallback to basic processing if correlation failed
                    self.logger.warning("Advanced correlation failed, falling back to basic processing")
                    self._process_scanner_results(results, internal_results, zap_results, nuclei_results)
            else:
                # Use basic processing
                self.logger.info("Using basic processing for findings")
                self._process_scanner_results(results, internal_results, zap_results, nuclei_results)
            
            # Update summary
            self._update_summary(results)
            
            # Add scan configuration to results
            results['scan_config'] = {
                'scan_type': validated_scan_type,
                'scanners': {
                    'internal': True,
                    'zap': include_zap,
                    'nuclei': {
                        'enabled': include_nuclei,
                        'type': nuclei_scan_type if include_nuclei else 'disabled'
                    }
                },
                'correlation': {
                    'advanced': use_advanced_correlation
                }
            }
            
            results['scan_end'] = datetime.now().isoformat()
            results['status'] = 'success'
            
            # Deduplicate vulnerabilities
            deduplication_stats = Vulnerability.deduplicate_vulnerabilities(target)
            
            # Add deduplication stats to results
            results['deduplication_stats'] = deduplication_stats
            
            return results

        except ValueError as e:
            # Handle validation errors
            error_msg = str(e)
            self.logger.error(f"Validation error: {error_msg}")
            return {
                'status': 'error',
                'error': error_msg,
                'valid_scan_types': list(self.VALID_SCAN_TYPES.keys())
            }
        except Exception as e:
            self.logger.error(f"Unified scan failed: {str(e)}")
            return {
                'status': 'error',
                'error': str(e)
            }
            
    def _process_scanner_results(self, results: Dict, internal_results: List, 
                                zap_results: List, nuclei_results: List) -> None:
        """Process results from different scanners using basic approach"""
        # Process internal scanner results
        for vuln in internal_results:
            results['vulnerabilities'].append({
                'source': 'internal',
                'name': vuln.get('name', ''),
                'description': vuln.get('description', ''),
                'severity': vuln.get('severity', 'LOW'),
                'type': vuln.get('type', ''),
                'evidence': vuln.get('evidence', ''),
                'confidence': vuln.get('confidence', 'medium'),
                'cvss_score': vuln.get('cvss', 0.0)
            })

        # Process ZAP scanner results
        for alert in zap_results:
            results['vulnerabilities'].append({
                'source': 'zap',
                'name': alert.get('name', ''),
                'description': alert.get('description', ''),
                'severity': self._normalize_severity(alert.get('risk')),
                'type': 'web',
                'evidence': alert.get('evidence', ''),
                'confidence': alert.get('confidence', 'medium'),
                'solution': alert.get('solution', ''),
                'cwe': alert.get('cweid', ''),
                'metadata': {
                    'url': alert.get('url', ''),
                    'parameter': alert.get('parameter', '')
                }
            })

        # Process Nuclei scanner results
        for finding in nuclei_results:
            results['vulnerabilities'].append({
                'source': 'nuclei',
                'name': finding.get('name', ''),
                'description': finding.get('description', ''),
                'severity': finding.get('severity', 'LOW'),
                'type': finding.get('type', 'nuclei'),
                'evidence': finding.get('evidence', ''),
                'confidence': 'high',
                'cvss_score': finding.get('cvss_score', 0.0),
                'cwe': finding.get('cwe', ''),
                'references': finding.get('references', []),
                'metadata': {
                    'template_id': finding.get('template_id', ''),
                    'tags': finding.get('tags', []),
                    'matched_at': finding.get('matched', ''),
                    'host': finding.get('host', '')
                }
            })
        
        # Basic deduplication by name and severity
        results['vulnerabilities'] = self._basic_deduplicate(results['vulnerabilities'])
        
    def _basic_deduplicate(self, vulnerabilities: List[Dict]) -> List[Dict]:
        """Simple deduplication of vulnerabilities by name and severity"""
        unique_vulns = {}
        
        for vuln in vulnerabilities:
            # Create a key for deduplication
            key = f"{vuln['name']}_{vuln['severity']}"
            
            if key in unique_vulns:
                existing = unique_vulns[key]
                # Merge sources
                sources = set([existing['source']])
                sources.add(vuln['source'])
                existing['source'] = ','.join(sources)
                # Take highest confidence
                if vuln.get('confidence') == 'high':
                    existing['confidence'] = 'high'
            else:
                unique_vulns[key] = vuln
                
        return list(unique_vulns.values())

    def _normalize_severity(self, severity: str) -> str:
        """Normalize severity ratings across different scanners"""
        severity = str(severity).lower()
        
        if severity in ['critical', 'high', '3', '4']:
            return 'HIGH'
        elif severity in ['medium', 'warning', '2']:
            return 'MEDIUM'
        elif severity in ['low', 'info', '1']:
            return 'LOW'
        return 'INFO'

    def _update_summary(self, results: Dict) -> None:
        """Update the summary counts"""
        summary = {'high': 0, 'medium': 0, 'low': 0, 'total': 0}
        
        for vuln in results['vulnerabilities']:
            severity = vuln['severity'].lower()
            if severity in summary:
                summary[severity] += 1
            summary['total'] += 1

        results['summary'] = summary

    def get_scanner_status(self) -> Dict:
        """Get status of all scanners"""
        status = {
            'internal': {
                'status': 'available',
                'checks': list(self.internal_scanner.checks.keys())
            },
            'zap': self.zap_manager.get_status()
        }

        # Add Nuclei status if initialized
        if self.nuclei_scanner:
            try:
                nuclei_info = self.nuclei_scanner.get_template_info()
                status['nuclei'] = {
                    'status': 'available',
                    'templates': nuclei_info.get('total_templates', 0),
                    'template_types': nuclei_info.get('template_types', {})
                }
            except Exception as e:
                status['nuclei'] = {
                    'status': 'error',
                    'error': str(e)
                }

        return status

    def _save_findings(self, vulnerabilities: List[Dict], target: str) -> None:
        """Save findings to database"""
        for vuln in vulnerabilities:
            # Extract metadata
            metadata = {
                'url': vuln.get('url'),
                'parameter': vuln.get('parameter'),
                'extra_info': vuln.get('metadata', {})
            }
            
            try:
                Vulnerability.objects.create(
                    target=target,
                    name=vuln['name'],
                    description=vuln.get('description', ''),
                    severity=vuln['severity'],
                    vuln_type=vuln.get('type', 'unknown'),
                    evidence=vuln.get('evidence', ''),
                    source=vuln['source'],
                    confidence=vuln['confidence'],
                    solution=vuln.get('solution', ''),
                    cwe=vuln.get('cwe', ''),
                    cvss_score=vuln.get('cvss_score'),
                    references=vuln.get('references', []),
                    metadata=metadata
                )
            except Exception as e:
                self.logger.error(f"Error saving vulnerability: {str(e)}")
                self.logger.error(f"Vulnerability data: {vuln}")import subprocess
import json
import logging
import os
import re
from typing import Dict, List, Optional, Tuple
from pathlib import Path
from datetime import datetime
from django.conf import settings

class NucleiScanner:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # Define Nuclei paths
        self.go_path = str(Path.home() / "go")  # More reliable GOPATH
        self.nuclei_path = os.path.join(self.go_path, "bin", "nuclei")
        self.logger.info(f"Using Nuclei at: {self.nuclei_path}")
        
        # Set up directories
        self.base_dir = Path(settings.BASE_DIR)
        self.results_dir = self.base_dir / "vulnerability" / "nuclei-results"
        self.debug_dir = self.base_dir / "vulnerability" / "debug_logs"
        
        # Create directories if they don't exist
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.debug_dir.mkdir(parents=True, exist_ok=True)
        
        # Set up environment with Go paths
        self.env = os.environ.copy()
        self.env["PATH"] = f"{self.go_path}/bin:{self.env.get('PATH', '')}"
        self.env["GOPATH"] = self.go_path
        
        # Verify installation
        if not self._verify_nuclei_installation():
            error_msg = (
                "Nuclei not found. Please install using:\n"
                "1. go install -v github.com/projectdiscovery/nuclei/v2/cmd/nuclei@latest\n"
                f"2. Verify installation at: {self.nuclei_path}"
            )
            raise Exception(error_msg)

    def _verify_nuclei_installation(self) -> bool:
        """Verify Nuclei is installed and working"""
        try:
            result = subprocess.run(
                [self.nuclei_path, '-version'],
                capture_output=True,
                text=True,
                env=self.env,
                timeout=10
            )
            
            if result.stderr and 'INF' in result.stderr:
                version_line = result.stderr.splitlines()[0] if result.stderr.splitlines() else "Unknown version"
                self.logger.info(f"Nuclei version verified: {version_line}")
                if "outdated" in result.stderr:
                    self.logger.warning("Nuclei is outdated. Consider updating for better results.")
            
            # Update templates if needed
            self.update_templates()
            
            return result.returncode == 0
        except Exception as e:
            self.logger.error(f"Error verifying nuclei: {str(e)}")
            return False

    def scan_target(self, target: str, scan_options: Optional[Dict] = None, 
                additional_flags: Optional[List[str]] = None, timeout: int = 300) -> Dict:
        """Core scanning method used by both basic and advanced scans"""
        try:
            # Clean target URL
            if not target.startswith(('http://', 'https://')):
                target = f"http://{target}"
                
            # Generate scan ID and output file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_target = re.sub(r'[^\w\-_]', '_', target)
            scan_id = f"scan_{timestamp}_{safe_target}"
            output_file = self.results_dir / f"{scan_id}.json"
            
            self.logger.info(f"Starting Nuclei scan {scan_id} for target: {target}")
            
            # Build base command
            cmd = [
                self.nuclei_path,
                "-target", target,
                "-j",               # JSON output
                "-o", str(output_file),
                "-stats",           # Show statistics
                "-duc",            # Disable update check
                "-no-color",        # No color output
                "-silent"           # Reduce output verbosity
            ]

            # Add templates path
            templates_path = str(Path.home() / "nuclei-templates" / "http" / "vulnerabilities")
            cmd.extend(["-t", templates_path])

            # Add severity levels
            cmd.extend(["-severity", "critical,high,medium,low"])

            # Add any additional flags
            if additional_flags:
                cmd.extend(additional_flags)

            # Log command to debug file instead of console
            debug_log_file = self.debug_dir / f"nuclei_cmd_{scan_id}.log"
            with open(debug_log_file, 'w') as f:
                f.write(f"Command: {' '.join(cmd)}\n")
            
            # Run the scan
            process = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                env=self.env,
                timeout=timeout
            )
            
            # Log stderr and stdout to debug file instead of console
            if process.stderr or process.stdout:
                with open(debug_log_file, 'a') as f:
                    if process.stderr:
                        f.write(f"\nSTDERR:\n{process.stderr}\n")
                    if process.stdout:
                        f.write(f"\nSTDOUT:\n{process.stdout}\n")
            
            # Process results
            findings = []
            if output_file.exists():
                try:
                    with open(output_file) as f:
                        content = f.read().strip()
                        if content:
                            for line in content.split('\n'):
                                try:
                                    if line.strip():
                                        finding = json.loads(line)
                                        findings.append(finding)
                                except json.JSONDecodeError:
                                    continue
                except Exception as e:
                    self.logger.error(f"Error reading output file: {str(e)}")

            processed_findings = self._process_findings(findings)
            
            # Log summary of findings instead of each individual finding
            summary = self._generate_summary(processed_findings)
            self.logger.info(f"Nuclei scan complete: {summary['total_findings']} findings ({', '.join([f'{sev}: {count}' for sev, count in summary['severity_distribution'].items() if count > 0])})")
            
            return {
                "status": "success",
                "target": target,
                "scan_id": scan_id,
                "timestamp": datetime.now().isoformat(),
                "findings": processed_findings,
                "summary": summary
            }

        except Exception as e:
            self.logger.error(f"Error during Nuclei scan: {str(e)}")
            return {
                "status": "error",
                "error": str(e)
            }

    def run_basic_scan(self, target: str) -> Dict:
        """Run a basic vulnerability scan using auto-scan mode"""
        try:
            # Clean target URL
            if not target.startswith(('http://', 'https://')):
                target = f"http://{target}"
                
            # Generate scan ID and output file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_target = re.sub(r'[^\w\-_]', '_', target)
            scan_id = f"scan_{timestamp}_{safe_target}"
            output_file = self.results_dir / f"{scan_id}.json"
            
            self.logger.info(f"Starting basic Nuclei scan for {target}")
            
            # Simple, effective command based on working example
            cmd = [
                self.nuclei_path,
                "-target", target,
                "-j",               # JSON output
                "-o", str(output_file),
                "-as",              # Automatic scan mode
                "-stats",           # Show statistics
                "-silent"           # Reduce output verbosity
            ]

            # Log command to debug file instead of console
            debug_log_file = self.debug_dir / f"nuclei_basic_{scan_id}.log"
            with open(debug_log_file, 'w') as f:
                f.write(f"Command: {' '.join(cmd)}\n")
            
            # Run the scan
            process = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                env=self.env,
                timeout=300
            )
            
            # Log stderr and stdout to debug file instead of console
            if process.stderr or process.stdout:
                with open(debug_log_file, 'a') as f:
                    if process.stderr:
                        f.write(f"\nSTDERR:\n{process.stderr}\n")
                    if process.stdout:
                        f.write(f"\nSTDOUT:\n{process.stdout}\n")
            
            # Process results
            findings = []
            if output_file.exists():
                try:
                    with open(output_file) as f:
                        content = f.read().strip()
                        if content:
                            for line in content.split('\n'):
                                try:
                                    if line.strip():
                                        finding = json.loads(line)
                                        findings.append(finding)
                                except json.JSONDecodeError:
                                    continue
                except Exception as e:
                    self.logger.error(f"Error reading output file: {str(e)}")

            processed_findings = self._process_findings(findings)
            
            # Log summary of findings instead of each individual finding
            summary = self._generate_summary(processed_findings)
            self.logger.info(f"Basic Nuclei scan complete: {summary['total_findings']} findings ({', '.join([f'{sev}: {count}' for sev, count in summary['severity_distribution'].items() if count > 0])})")
            
            return {
                "status": "success",
                "target": target,
                "scan_id": scan_id,
                "timestamp": datetime.now().isoformat(),
                "findings": processed_findings,
                "raw_output_file": str(debug_log_file),  # Reference to raw output file instead of including in response
                "summary": summary
            }

        except Exception as e:
            self.logger.error(f"Error during basic Nuclei scan: {str(e)}")
            return {
                "status": "error",
                "error": str(e)
            }

    def run_advanced_scan(self, target: str, options: Optional[Dict] = None) -> Dict:
        """Run a more comprehensive scan with additional options"""
        try:
            # Clean target URL
            if not target.startswith(('http://', 'https://')):
                target = f"http://{target}"
                
            # Generate scan ID and output file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_target = re.sub(r'[^\w\-_]', '_', target)
            scan_id = f"scan_{timestamp}_{safe_target}"
            output_file = self.results_dir / f"{scan_id}.json"
            
            self.logger.info(f"Starting advanced Nuclei scan for {target}")
            
            # Extended command with additional options
            cmd = [
                self.nuclei_path,
                "-target", target,
                "-j",               # JSON output
                "-o", str(output_file),
                "-as",              # Automatic scan mode
                "-stats",           # Show statistics
                "-c", "50",         # Increased concurrency
                "-timeout", "15",   # Extended timeout
                "-retries", "2",    # Retry failed requests
                "-silent"           # Reduce output verbosity
            ]

            # Log command to debug file instead of console
            debug_log_file = self.debug_dir / f"nuclei_advanced_{scan_id}.log"
            with open(debug_log_file, 'w') as f:
                f.write(f"Command: {' '.join(cmd)}\n")
            
            # Run the scan with longer timeout
            process = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                env=self.env,
                timeout=600  # 10 minutes
            )
            
            # Log stderr and stdout to debug file instead of console
            if process.stderr or process.stdout:
                with open(debug_log_file, 'a') as f:
                    if process.stderr:
                        f.write(f"\nSTDERR:\n{process.stderr}\n")
                    if process.stdout:
                        f.write(f"\nSTDOUT:\n{process.stdout}\n")
            
            # Process results
            findings = []
            if output_file.exists():
                try:
                    with open(output_file) as f:
                        content = f.read().strip()
                        if content:
                            for line in content.split('\n'):
                                try:
                                    if line.strip():
                                        finding = json.loads(line)
                                        findings.append(finding)
                                except json.JSONDecodeError:
                                    continue
                except Exception as e:
                    self.logger.error(f"Error reading output file: {str(e)}")

            processed_findings = self._process_findings(findings)
            
            # Log summary of findings instead of each individual finding
            summary = self._generate_summary(processed_findings)
            self.logger.info(f"Advanced Nuclei scan complete: {summary['total_findings']} findings ({', '.join([f'{sev}: {count}' for sev, count in summary['severity_distribution'].items() if count > 0])})")
            
            return {
                "status": "success",
                "target": target,
                "scan_id": scan_id,
                "timestamp": datetime.now().isoformat(),
                "findings": processed_findings,
                "raw_output_file": str(debug_log_file),  # Reference to raw output file instead of including in response
                "summary": summary
            }

        except Exception as e:
            self.logger.error(f"Error during advanced Nuclei scan: {str(e)}")
            return {
                "status": "error",
                "error": str(e)
            }

    def _process_findings(self, findings: List[Dict]) -> List[Dict]:
        """Process and normalize Nuclei findings with CVSS handling"""
        processed = []
        
        # Severity to CVSS base mapping
        severity_cvss = {
            'CRITICAL': 9.5,
            'HIGH': 7.5,
            'MEDIUM': 5.0,
            'LOW': 2.5,
            'INFO': 0.0,
            'unknown': 0.0
        }

        for finding in findings:
            try:
                # Get info block
                info = finding.get("info", {})
                if not info and "name" in finding:
                    info = finding

                # Extract matched data
                matched_data = ""
                if "matcher-name" in finding:
                    matched_data = f"Matcher: {finding['matcher-name']}"
                if "matched-at" in finding:
                    matched_data += f"\nMatched at: {finding['matched-at']}"
                if "extracted-results" in finding:
                    matched_data += f"\nExtracted: {finding['extracted-results']}"

                # Determine CVSS score
                cvss_score = None
                if 'classification' in info and 'cvss-score' in info['classification']:
                    try:
                        cvss_score = float(info['classification']['cvss-score'])
                    except (ValueError, TypeError):
                        # Use severity-based score if CVSS parse fails
                        severity = info.get('severity', 'unknown').upper()
                        cvss_score = severity_cvss.get(severity, 0.0)
                else:
                    # Use severity-based score if no CVSS provided
                    severity = info.get('severity', 'unknown').upper()
                    cvss_score = severity_cvss.get(severity, 0.0)

                processed.append({
                    "template_id": finding.get("template-id", ""),
                    "name": info.get("name", "Unknown Finding"),
                    "severity": info.get("severity", "unknown").upper(),
                    "type": finding.get("type", "unknown"),
                    "host": finding.get("host", ""),
                    "matched": finding.get("matched-at", ""),
                    "evidence": matched_data,
                    "description": info.get("description", ""),
                    "tags": info.get("tags", []),
                    "references": info.get("reference", []),
                    "cwe": info.get("classification", {}).get("cwe-id", ""),
                    "cvss_score": cvss_score,
                    "timestamp": finding.get("timestamp", datetime.now().isoformat())
                })
            except Exception as e:
                self.logger.error(f"Error processing finding: {str(e)}")

        return processed

    def _generate_summary(self, findings: List[Dict]) -> Dict:
        """Generate summary of findings"""
        severity_counts = {
            "critical": 0,
            "high": 0,
            "medium": 0,
            "low": 0,
            "info": 0
        }
        
        for finding in findings:
            severity = finding.get("severity", "").lower()
            if severity in severity_counts:
                severity_counts[severity] += 1
            else:
                severity_counts["info"] += 1

        return {
            "total_findings": len(findings),
            "severity_distribution": severity_counts,
            "unique_templates": len(set(f.get("template_id", "") for f in findings))
        }

    def update_templates(self) -> Dict:
        """Update Nuclei templates"""
        try:
            cmd = [self.nuclei_path, "-update-templates", "-silent"]
            process = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                env=self.env,
                timeout=180
            )
            
            if process.returncode != 0:
                return {
                    "status": "error",
                    "error": process.stderr or "Template update failed"
                }

            return {
                "status": "success",
                "message": "Templates updated successfully"
            }

        except Exception as e:
            self.logger.error(f"Error updating templates: {str(e)}")
            return {
                "status": "error",
                "error": str(e)
            }

    def get_template_info(self) -> Dict:
        """Get information about available templates"""
        try:
            cmd = [self.nuclei_path, "-tl", "-silent"]
            process = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                env=self.env,
                timeout=60
            )
            
            if process.returncode != 0:
                return {
                    "status": "error",
                    "error": process.stderr or "Failed to get template list"
                }

            templates = process.stdout.strip().split("\n")
            template_count = len(templates)
            
            template_types = {
                "cve": 0,
                "vulnerability": 0,
                "exposure": 0,
                "technology": 0,
                "misconfiguration": 0,
                "default-login": 0,
                "other": 0
            }
            
            for template in templates:
                template_lower = template.lower()
                if "cve" in template_lower:
                    template_types["cve"] += 1
                elif "vuln" in template_lower:
                    template_types["vulnerability"] += 1
                elif "exposure" in template_lower:
                    template_types["exposure"] += 1
                elif "tech" in template_lower:
                    template_types["technology"] += 1
                elif "misconfig" in template_lower:
                    template_types["misconfiguration"] += 1
                elif "default" in template_lower and "login" in template_lower:
                    template_types["default-login"] += 1
                else:
                    template_types["other"] += 1
            
            return {
                "status": "success",
                "total_templates": template_count,
                "template_types": template_types,
                "templates": templates[:50],
                "note": "Response limited to 50 templates" if template_count > 50 else ""
            }

        except Exception as e:
            self.logger.error(f"Error getting template info: {str(e)}")
            return {
                "status": "error",
                "error": str(e)
            }# vulnerability/correlation.py

from typing import List, Dict, Tuple, Set, Optional
import logging
from .models import Vulnerability
from datetime import datetime
import hashlib
import re
from django.db import transaction

class VulnerabilityCorrelator:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # Correlation thresholds
        self.title_similarity_threshold = 0.8  # Title similarity threshold
        self.description_similarity_threshold = 0.6  # Description similarity threshold
        
        # Define correlation rules
        self.correlation_rules = [
            self._correlation_by_name_and_type,  # Exact name + type match
            self._correlation_by_fingerprint,    # Using vulnerability fingerprint
            self._correlation_by_cve,            # By CVE/reference match
            self._correlation_by_similarity      # By content similarity
        ]

    def correlate_findings(self, 
                           internal_results: List[Dict] = None, 
                           zap_results: List[Dict] = None, 
                           nuclei_results: List[Dict] = None,
                           openvas_results: List[Dict] = None, 
                           manual_results: List[Dict] = None,
                           target: str = None) -> Dict:
        """
        Correlate findings from multiple scanners
        Returns a structured result with correlated findings and statistics
        """
        try:
            # Initialize empty lists if not provided
            internal_results = internal_results or []
            zap_results = zap_results or []
            nuclei_results = nuclei_results or []
            openvas_results = openvas_results or []
            manual_results = manual_results or []
            
            # Normalize findings from each scanner
            normalized_findings = []
            
            # Process results from each scanner
            for finding in internal_results:
                normalized = self._normalize_internal_finding(finding)
                normalized_findings.append(normalized)

            for finding in zap_results:
                normalized = self._normalize_zap_finding(finding)
                normalized_findings.append(normalized)
                
            for finding in nuclei_results:
                normalized = self._normalize_nuclei_finding(finding)
                normalized_findings.append(normalized)

            for finding in openvas_results:
                normalized = self._normalize_openvas_finding(finding)
                normalized_findings.append(normalized)
                
            for finding in manual_results:
                normalized = self._normalize_manual_finding(finding)
                normalized_findings.append(normalized)

            # Apply correlation to identify related findings
            self.logger.info(f"Correlating {len(normalized_findings)} findings from multiple scanners")
            correlated_findings = self._correlate_findings(normalized_findings)
            
            # Save correlated findings if target is provided
            saved_findings = []
            if target:
                saved_findings = self._save_findings(correlated_findings, target)
            
            # Generate correlation statistics
            stats = self._generate_correlation_stats(normalized_findings, correlated_findings)
            
            return {
                'status': 'success',
                'original_count': len(normalized_findings),
                'correlated_count': len(correlated_findings),
                'findings': saved_findings if target else correlated_findings,
                'statistics': stats,
                'correlation_timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            self.logger.error(f"Error correlating findings: {str(e)}")
            return {
                'status': 'error',
                'error': str(e)
            }

    #
    # Normalization methods for different scanner outputs
    #
    
    def _normalize_internal_finding(self, finding: Dict) -> Dict:
        """Normalize internal scanner findings"""
        return {
            'name': finding.get('name', 'Unknown Finding'),
            'description': finding.get('description', ''),
            'severity': finding.get('severity', 'LOW'),
            'evidence': finding.get('evidence', ''),
            'source': 'internal',
            'confidence': finding.get('confidence', 'medium'),
            'type': finding.get('type', 'unknown'),
            'references': finding.get('references', []),
            'cve': self._extract_cve_references(finding.get('references', [])),
            'cvss_score': finding.get('cvss', 0.0),
            'cwe': finding.get('cwe', ''),
            'fingerprint': self._generate_finding_fingerprint(finding),
            'metadata': {
                'original_source': 'internal_scanner',
                'original_id': finding.get('id'),
                'scan_date': datetime.now().isoformat()
            }
        }

    def _normalize_zap_finding(self, finding: Dict) -> Dict:
        """Normalize ZAP findings"""
        name = finding.get('name', 'Unknown ZAP Finding')
        description = finding.get('description', '')
        
        # Extract any CVEs mentioned in the description or alerts
        cves = self._extract_cve_from_text(description)
        if 'alertItems' in finding:
            for item in finding['alertItems']:
                item_desc = item.get('description', '')
                cves.update(self._extract_cve_from_text(item_desc))
        
        refs = finding.get('references', [])
        if isinstance(refs, str):
            refs = [refs]
        
        return {
            'name': name,
            'description': description,
            'severity': self._map_zap_severity(finding.get('risk')),
            'evidence': finding.get('evidence', ''),
            'source': 'zap',
            'confidence': self._map_zap_confidence(finding.get('confidence', '')),
            'type': 'web',
            'references': refs,
            'cve': list(cves),
            'cwe': str(finding.get('cweid', '')),
            'cvss_score': self._calculate_cvss_from_severity(self._map_zap_severity(finding.get('risk'))),
            'fingerprint': self._generate_finding_fingerprint(finding),
            'metadata': {
                'original_source': 'zap',
                'original_id': finding.get('id'),
                'url': finding.get('url', ''),
                'parameter': finding.get('parameter', ''),
                'attack': finding.get('attack', ''),
                'wascid': finding.get('wascid', ''),
                'solution': finding.get('solution', ''),
                'scan_date': datetime.now().isoformat()
            }
        }

    def _normalize_nuclei_finding(self, finding: Dict) -> Dict:
        """Normalize Nuclei findings"""
        # Extract CVEs from template ID or name
        cves = set()
        template_id = finding.get('template_id', '')
        name = finding.get('name', '')
        
        # Check if template_id or name contains CVE reference
        cves.update(self._extract_cve_from_text(template_id))
        cves.update(self._extract_cve_from_text(name))
        
        # Add any CVEs from references
        if finding.get('references'):
            for ref in finding['references']:
                cves.update(self._extract_cve_from_text(ref))
        
        return {
            'name': finding.get('name', 'Unknown Nuclei Finding'),
            'description': finding.get('description', ''),
            'severity': finding.get('severity', 'LOW'),
            'evidence': finding.get('evidence', finding.get('matched', '')),
            'source': 'nuclei',
            'confidence': 'high',  # Nuclei typically has high confidence due to specific template matching
            'type': finding.get('type', 'nuclei'),
            'references': finding.get('references', []),
            'cve': list(cves),
            'cwe': finding.get('cwe', ''),
            'cvss_score': finding.get('cvss_score', 0.0),
            'fingerprint': self._generate_finding_fingerprint(finding),
            'metadata': {
                'original_source': 'nuclei',
                'template_id': finding.get('template_id', ''),
                'matched_at': finding.get('matched', ''),
                'host': finding.get('host', ''),
                'tags': finding.get('tags', []),
                'scan_date': datetime.now().isoformat()
            }
        }

    def _normalize_openvas_finding(self, finding: Dict) -> Dict:
        """Normalize OpenVAS findings"""
        cves = set()
        
        # Extract CVEs from direct field or references
        if 'cve' in finding:
            if isinstance(finding['cve'], list):
                for cve in finding['cve']:
                    cves.update(self._extract_cve_from_text(cve))
            else:
                cves.update(self._extract_cve_from_text(finding['cve']))
                
        return {
            'name': finding.get('name', 'Unknown OpenVAS Finding'),
            'description': finding.get('description', ''),
            'severity': self._map_openvas_severity(finding.get('severity')),
            'evidence': f"Port: {finding.get('port')} - Host: {finding.get('host')}",
            'source': 'openvas',
            'confidence': 'high',
            'type': 'network',
            'references': finding.get('references', []),
            'cve': list(cves),
            'cwe': finding.get('cwe', ''),
            'cvss_score': float(finding.get('cvss', 0.0)) if finding.get('cvss') else 0.0,
            'fingerprint': self._generate_finding_fingerprint(finding),
            'metadata': {
                'original_source': 'openvas',
                'solution': finding.get('solution'),
                'port': finding.get('port'),
                'scan_date': datetime.now().isoformat()
            }
        }
        
    def _normalize_manual_finding(self, finding: Dict) -> Dict:
        """Normalize manually entered findings"""
        return {
            'name': finding.get('name', 'Manual Finding'),
            'description': finding.get('description', ''),
            'severity': finding.get('severity', 'LOW'),
            'evidence': finding.get('evidence', ''),
            'source': 'manual',
            'confidence': finding.get('confidence', 'high'),
            'type': finding.get('type', 'manual'),
            'references': finding.get('references', []),
            'cve': self._extract_cve_from_text(finding.get('description', '')),
            'cwe': finding.get('cwe', ''),
            'cvss_score': finding.get('cvss_score', 0.0),
            'fingerprint': self._generate_finding_fingerprint(finding),
            'metadata': {
                'original_source': 'manual',
                'author': finding.get('author', 'Unknown'),
                'notes': finding.get('notes', ''),
                'scan_date': datetime.now().isoformat()
            }
        }

    #
    # Correlation methods
    #
    
    def _correlate_findings(self, findings: List[Dict]) -> List[Dict]:
        """
        Apply correlation rules to findings to identify related issues
        """
        if not findings:
            return []
            
        self.logger.info(f"Starting correlation of {len(findings)} findings")
        
        # Create groups of correlated findings
        groups = []
        processed = set()
        
        for i, finding in enumerate(findings):
            if i in processed:
                continue
                
            # Create a new group with this finding
            group = [finding]
            processed.add(i)
            
            # Check all other findings for correlation
            for j, other in enumerate(findings):
                if j in processed or i == j:
                    continue
                    
                # Apply correlation rules
                if self._are_findings_related(finding, other):
                    group.append(other)
                    processed.add(j)
            
            groups.append(group)
        
        # Merge findings in each group
        correlated_findings = []
        for group in groups:
            if len(group) == 1:
                correlated_findings.append(group[0])
            else:
                merged = self._merge_findings(group)
                correlated_findings.append(merged)
                
        self.logger.info(f"Correlation complete. Reduced {len(findings)} findings to {len(correlated_findings)}")
        return correlated_findings

    def _are_findings_related(self, finding1: Dict, finding2: Dict) -> bool:
        """
        Check if two findings are related using multiple correlation rules
        Returns True if any correlation rule matches
        """
        for rule in self.correlation_rules:
            if rule(finding1, finding2):
                return True
        return False

    def _correlation_by_name_and_type(self, finding1: Dict, finding2: Dict) -> bool:
        """Correlation rule: same name and vulnerability type"""
        if finding1.get('name') and finding2.get('name') and finding1.get('type') and finding2.get('type'):
            # Normalize names for comparison
            name1 = self._normalize_text(finding1['name'])
            name2 = self._normalize_text(finding2['name'])
            return name1 == name2 and finding1['type'] == finding2['type']
        return False

    def _correlation_by_fingerprint(self, finding1: Dict, finding2: Dict) -> bool:
        """Correlation rule: match by vulnerability fingerprint"""
        if finding1.get('fingerprint') and finding2.get('fingerprint'):
            return finding1['fingerprint'] == finding2['fingerprint']
        return False

    def _correlation_by_cve(self, finding1: Dict, finding2: Dict) -> bool:
        """Correlation rule: match by CVE references"""
        cves1 = set(finding1.get('cve', []))
        cves2 = set(finding2.get('cve', []))
        
        # If both have CVEs and there's an overlap, they're related
        return bool(cves1 and cves2 and cves1.intersection(cves2))

    def _correlation_by_similarity(self, finding1: Dict, finding2: Dict) -> bool:
        """Correlation rule: match by content similarity"""
        # Compare titles for similarity
        name1 = finding1.get('name', '')
        name2 = finding2.get('name', '')
        
        if name1 and name2:
            name_similarity = self._calculate_similarity(name1, name2)
            if name_similarity >= self.title_similarity_threshold:
                return True
                
        # Compare descriptions for similarity
        desc1 = finding1.get('description', '')
        desc2 = finding2.get('description', '')
        
        if desc1 and desc2:
            desc_similarity = self._calculate_similarity(desc1, desc2)
            if desc_similarity >= self.description_similarity_threshold:
                return True
                
        return False

    def _merge_findings(self, findings: List[Dict]) -> Dict:
        """
        Merge a group of related findings into a single comprehensive finding
        """
        if not findings:
            return {}
            
        if len(findings) == 1:
            return findings[0]
            
        # Start with the highest severity finding as the base
        findings.sort(key=lambda x: self._severity_to_score(x.get('severity', 'LOW')), reverse=True)
        base = findings[0].copy()
        
        # Track merged sources
        sources = set([base['source']])
        references = set(base.get('references', []))
        cves = set(base.get('cve', []))
        
        # Create a comprehensive description
        descriptions = [f"{base['source']}: {base['description']}"]
        evidences = [f"{base['source']}: {base['evidence']}"]
        
        # Merge additional findings
        for finding in findings[1:]:
            sources.add(finding['source'])
            
            # Add references
            for ref in finding.get('references', []):
                references.add(ref)
                
            # Add CVEs
            for cve in finding.get('cve', []):
                cves.add(cve)
                
            # Add description and evidence
            if finding.get('description'):
                descriptions.append(f"{finding['source']}: {finding['description']}")
            
            if finding.get('evidence'):
                evidences.append(f"{finding['source']}: {finding['evidence']}")
                
            # Merge metadata
            for key, value in finding.get('metadata', {}).items():
                if key not in base['metadata']:
                    base['metadata'][key] = value
                    
        # Update merged finding
        base['source'] = ','.join(sources)
        base['references'] = list(references)
        base['cve'] = list(cves)
        base['description'] = '\n\n'.join(descriptions)
        base['evidence'] = '\n\n'.join(evidences)
        
        # Set confidence based on number of sources
        base['confidence'] = 'high' if len(sources) > 1 else base['confidence']
        
        # Calculate highest CVSS score
        base['cvss_score'] = max([f.get('cvss_score', 0.0) for f in findings])
        
        # Add correlation metadata
        base['metadata']['correlated'] = True
        base['metadata']['correlated_count'] = len(findings)
        base['metadata']['correlated_sources'] = list(sources)
        
        return base

    #
    # Helper methods
    #
    
    def _generate_finding_fingerprint(self, finding: Dict) -> str:
        """
        Generate a unique fingerprint for a finding to aid in correlation
        """
        # Extract key attributes for fingerprinting
        name = finding.get('name', '')
        desc = finding.get('description', '')
        
        # Create a string representation
        fingerprint_str = f"{name}:{desc}"
        
        # For specific scanner types, add additional identifiers
        source = finding.get('source', '')
        if source == 'zap' and 'alertItems' in finding:
            fingerprint_str += f":{finding.get('cweid', '')}"
        elif source == 'nuclei':
            fingerprint_str += f":{finding.get('template_id', '')}"
        
        # Generate SHA256 hash
        return hashlib.sha256(fingerprint_str.encode()).hexdigest()

    def _extract_cve_from_text(self, text: str) -> Set[str]:
        """
        Extract CVE IDs from text using regex
        """
        if not text:
            return set()
            
        # CVE pattern: CVE-YYYY-NNNNN (YYYY is year, NNNNN is ID number)
        cve_pattern = r'CVE-\d{4}-\d{4,7}'
        
        # Find all matches
        cves = re.findall(cve_pattern, text, re.IGNORECASE)
        return set(cve.upper() for cve in cves)

    def _extract_cve_references(self, references: List[str]) -> List[str]:
        """
        Extract CVE IDs from reference URLs
        """
        cves = set()
        for ref in references:
            cves.update(self._extract_cve_from_text(ref))
        return list(cves)

    def _normalize_text(self, text: str) -> str:
        """
        Normalize text for more accurate comparison:
        - Convert to lowercase
        - Remove punctuation
        - Remove common words
        """
        if not text:
            return ""
            
        # Convert to lowercase
        text = text.lower()
        
        # Remove punctuation
        text = re.sub(r'[^\w\s]', '', text)
        
        # Remove common words
        stop_words = {'a', 'an', 'the', 'in', 'on', 'at', 'of', 'for', 'with', 'by'}
        words = text.split()
        filtered_words = [word for word in words if word not in stop_words]
        
        return ' '.join(filtered_words)

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """
        Calculate similarity between two strings using Jaccard similarity
        Returns value between 0.0 and 1.0
        """
        if not text1 or not text2:
            return 0.0
            
        # Normalize texts
        text1 = self._normalize_text(text1)
        text2 = self._normalize_text(text2)
        
        # Split into words
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        # Calculate Jaccard similarity: intersection / union
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        if union == 0:
            return 0.0
            
        return intersection / union

    def _severity_to_score(self, severity: str) -> int:
        """
        Convert severity string to numeric score for comparison
        """
        severity = severity.upper()
        scores = {
            'CRITICAL': 4,
            'HIGH': 3,
            'MEDIUM': 2,
            'LOW': 1,
            'INFO': 0
        }
        return scores.get(severity, 0)

    def _map_zap_severity(self, severity: str) -> str:
        """Map ZAP severity to standard severity"""
        if not severity:
            return 'LOW'
            
        mapping = {
            'Informational': 'INFO',
            'Low': 'LOW',
            'Medium': 'MEDIUM',
            'High': 'HIGH',
            'Critical': 'CRITICAL',
            '0': 'INFO',
            '1': 'LOW',
            '2': 'MEDIUM',
            '3': 'HIGH',
            '4': 'CRITICAL'
        }
        return mapping.get(severity, 'LOW')

    def _map_zap_confidence(self, confidence: str) -> str:
        """Map ZAP confidence to standard confidence"""
        if not confidence:
            return 'medium'
            
        mapping = {
            'False Positive': 'low',
            'Low': 'low',
            'Medium': 'medium',
            'High': 'high',
            'Confirmed': 'high',
            '0': 'low',
            '1': 'low',
            '2': 'medium',
            '3': 'high',
            '4': 'high'
        }
        return mapping.get(confidence, 'medium')

    def _map_openvas_severity(self, severity: str) -> str:
        """Map OpenVAS severity to standard severity"""
        try:
            severity_float = float(severity)
            if severity_float >= 9.0:
                return 'CRITICAL'
            elif severity_float >= 7.0:
                return 'HIGH'
            elif severity_float >= 4.0:
                return 'MEDIUM'
            elif severity_float > 0:
                return 'LOW'
            return 'INFO'
        except (ValueError, TypeError):
            return 'LOW'

    def _calculate_cvss_from_severity(self, severity: str) -> float:
        """Calculate estimated CVSS score from severity string"""
        severity = severity.upper()
        cvss_mapping = {
            'CRITICAL': 9.5,
            'HIGH': 7.5,
            'MEDIUM': 5.0,
            'LOW': 2.5,
            'INFO': 0.0
        }
        return cvss_mapping.get(severity, 0.0)

    def _generate_correlation_stats(self, original_findings: List[Dict], 
                                   correlated_findings: List[Dict]) -> Dict:
        """
        Generate statistics about the correlation process
        """
        # Count by source
        original_by_source = {}
        for finding in original_findings:
            source = finding.get('source', 'unknown')
            original_by_source[source] = original_by_source.get(source, 0) + 1
            
        # Count by severity
        severity_distribution = {
            'CRITICAL': 0,
            'HIGH': 0,
            'MEDIUM': 0,
            'LOW': 0,
            'INFO': 0
        }
        
        correlation_groups = []
        for finding in correlated_findings:
            # Update severity count
            severity = finding.get('severity', 'LOW').upper()
            if severity in severity_distribution:
                severity_distribution[severity] += 1
                
            # Track correlation groups
            if ',' in finding.get('source', ''):
                sources = finding.get('source', '').split(',')
                correlation_groups.append({
                    'name': finding.get('name', 'Unknown'),
                    'sources': sources,
                    'severity': severity
                })
        
        return {
            'original_count': len(original_findings),
            'correlated_count': len(correlated_findings),
            'reduction_percentage': round((1 - len(correlated_findings) / len(original_findings)) * 100, 2) if original_findings else 0,
            'original_by_source': original_by_source,
            'severity_distribution': severity_distribution,
            'correlation_groups': correlation_groups[:10]  # Limit to top 10 groups
        }

    @transaction.atomic
    def _save_findings(self, findings: List[Dict], target: str) -> List[Dict]:
        """
        Save correlated findings to database with transaction support
        """
        saved_findings = []
        
        for finding in findings:
            try:
                vulnerability = Vulnerability.objects.create(
                    target=target,
                    name=finding['name'],
                    description=finding['description'],
                    severity=finding['severity'],
                    vuln_type=finding.get('type', 'unknown'),
                    evidence=finding['evidence'],
                    source=finding['source'],
                    confidence=finding['confidence'],
                    references=finding.get('references', []),
                    solution=finding.get('metadata', {}).get('solution', ''),
                    cwe=finding.get('cwe', ''),
                    cvss_score=finding.get('cvss_score', 0.0),
                    metadata={
                        **finding.get('metadata', {}),
                        'cve': finding.get('cve', [])
                    }
                )
                saved_findings.append(self._serialize_vulnerability(vulnerability))
            except Exception as e:
                self.logger.error(f"Error saving vulnerability: {str(e)}")
        
        return saved_findings

    def _serialize_vulnerability(self, vuln: Vulnerability) -> Dict:
        """Serialize vulnerability for response"""
        return {
            'id': vuln.id,
            'name': vuln.name,
            'description': vuln.description,
            'severity': vuln.severity,
            'type': vuln.vuln_type,
            'evidence': vuln.evidence,
            'sources': vuln.source.split(','),
            'confidence': vuln.confidence,
            'discovery_date': vuln.discovery_date.isoformat(),
            'references': vuln.references,
            'cvss_score': vuln.cvss_score,
            'cwe': vuln.cwe,
            'metadata': vuln.metadata
        }# vulnerability/risk_scoring.py

from typing import Dict, List, Union
import math
from datetime import datetime, timezone
from .models import Vulnerability, NucleiFinding
from reconnaissance.models import Service

class RiskScorer:
    def __init__(self):
        # Base weights for different components
        self.weights = {
            'cvss': 0.4,
            'exposure': 0.2,
            'business_impact': 0.2,
            'exploitability': 0.2
        }
        
        # Severity base scores
        self.severity_scores = {
            'CRITICAL': 10.0,
            'HIGH': 8.0,
            'MEDIUM': 5.0,
            'LOW': 2.0,
            'INFO': 0.5
        }
        
        # Exploitability factors
        self.exploitability_factors = {
            'public_exploit': 2.0,
            'authentication_required': 0.8,
            'complex_exploit': 0.6
        }

    def calculate_risk_score(self, vulnerability: Union[Vulnerability, NucleiFinding]) -> float:
        """Calculate comprehensive risk score"""
        cvss_component = self._calculate_cvss_component(vulnerability)
        exposure_component = self._calculate_exposure_component(vulnerability)
        business_impact = self._calculate_business_impact(vulnerability)
        exploitability = self._calculate_exploitability(vulnerability)
        
        # Weight and combine components
        final_score = (
            cvss_component * self.weights['cvss'] +
            exposure_component * self.weights['exposure'] +
            business_impact * self.weights['business_impact'] +
            exploitability * self.weights['exploitability']
        )
        
        # Normalize to 0-10 scale
        return round(min(max(final_score, 0), 10), 2)

    def _calculate_cvss_component(self, vulnerability: Union[Vulnerability, NucleiFinding]) -> float:
        """Calculate CVSS-based component"""
        if hasattr(vulnerability, 'cvss_score') and vulnerability.cvss_score:
            return float(vulnerability.cvss_score)
        
        # Use severity as fallback
        return self.severity_scores.get(vulnerability.severity, 0.0)

    def _calculate_exposure_component(self, vulnerability: Union[Vulnerability, NucleiFinding]) -> float:
        """Calculate exposure based on various factors"""
        exposure_score = 5.0  # Base score
        
        # Check if the vulnerability is on a public-facing service
        if hasattr(vulnerability, 'target'):
            services = Service.objects.filter(host=vulnerability.target)
            for service in services:
                if service.port in [80, 443, 8080, 8443]:  # Web services
                    exposure_score += 2.0
                elif service.port < 1024:  # Well-known ports
                    exposure_score += 1.0
                    
        # Age factor (older vulnerabilities are higher risk)
        age_in_days = (datetime.now(timezone.utc) - vulnerability.discovery_date).days
        age_factor = min(age_in_days / 30, 3.0)  # Cap at 3x after 90 days
        exposure_score *= (1 + (age_factor * 0.1))
        
        return min(exposure_score, 10.0)

    def _calculate_business_impact(self, vulnerability: Union[Vulnerability, NucleiFinding]) -> float:
        """Calculate business impact score"""
        impact_score = 5.0  # Base score
        
        # Check for critical keywords in vulnerability name/description
        critical_terms = ['credentials', 'password', 'admin', 'authentication', 'sensitive', 'data breach']
        description = getattr(vulnerability, 'description', '').lower()
        name = getattr(vulnerability, 'name', '').lower()
        
        for term in critical_terms:
            if term in description or term in name:
                impact_score += 1.0
                
        # Adjust based on vulnerability type
        if hasattr(vulnerability, 'vuln_type'):
            high_impact_types = ['rce', 'sqli', 'cmdi', 'ssrf']
            if vulnerability.vuln_type.lower() in high_impact_types:
                impact_score += 2.0
                
        return min(impact_score, 10.0)

    def _calculate_exploitability(self, vulnerability: Union[Vulnerability, NucleiFinding]) -> float:
        """Calculate exploitability score"""
        exploitability_score = 5.0  # Base score
        
        # Check metadata for exploit information
        if hasattr(vulnerability, 'metadata'):
            metadata = vulnerability.metadata
            if metadata.get('has_exploit', False):
                exploitability_score *= self.exploitability_factors['public_exploit']
            if metadata.get('authentication_required', False):
                exploitability_score *= self.exploitability_factors['authentication_required']
            if metadata.get('complex_exploit', False):
                exploitability_score *= self.exploitability_factors['complex_exploit']
        
        # Check references for exploit-db or similar
        if hasattr(vulnerability, 'references'):
            references = vulnerability.references
            if any('exploit-db' in ref.lower() for ref in references):
                exploitability_score *= self.exploitability_factors['public_exploit']
                
        return min(exploitability_score, 10.0)

    def calculate_target_risk_score(self, target: str) -> Dict:
        """Calculate overall risk score for a target"""
        vulnerabilities = list(Vulnerability.objects.filter(target=target))
        nuclei_findings = list(NucleiFinding.objects.filter(target=target))
        
        all_findings = vulnerabilities + nuclei_findings
        if not all_findings:
            return {
                'target': target,
                'overall_risk_score': 0.0,
                'total_findings': 0,
                'risk_breakdown': {
                    'critical': 0,
                    'high': 0,
                    'medium': 0,
                    'low': 0,
                    'info': 0
                },
                'top_risks': []
            }
            
        # Calculate individual risk scores
        risk_scores = []
        severity_counts = {'CRITICAL': 0, 'HIGH': 0, 'MEDIUM': 0, 'LOW': 0, 'INFO': 0}
        
        for finding in all_findings:
            risk_score = self.calculate_risk_score(finding)
            severity_counts[finding.severity] += 1
            risk_scores.append({
                'finding': finding,
                'score': risk_score
            })
            
        # Sort by risk score
        risk_scores.sort(key=lambda x: x['score'], reverse=True)
        
        # Calculate overall risk score (weighted average)
        total_weight = sum(score['score'] for score in risk_scores)
        if total_weight > 0:
            overall_score = sum(score['score'] ** 2 for score in risk_scores) / total_weight
        else:
            overall_score = 0
            
        # Get top 5 risks
        top_risks = []
        for risk in risk_scores[:5]:
            finding = risk['finding']
            top_risks.append({
                'name': finding.name,
                'severity': finding.severity,
                'risk_score': risk['score'],
                'type': finding.__class__.__name__
            })
            
        return {
            'target': target,
            'overall_risk_score': round(overall_score, 2),
            'total_findings': len(all_findings),
            'risk_breakdown': severity_counts,
            'top_risks': top_risks
        }

    def calculate_asset_criticality(self, target: str) -> float:
        """Calculate asset criticality score"""
        criticality_score = 5.0  # Base score
        
        # Check for critical services
        services = Service.objects.filter(host=target)
        for service in services:
            if service.name.lower() in ['http', 'https', 'ssh', 'ftp', 'database']:
                criticality_score += 1.0
            if service.category == 'database':
                criticality_score += 2.0
                
        # Check for sensitive information exposure
        high_risk_vulns = Vulnerability.objects.filter(
            target=target,
            severity__in=['CRITICAL', 'HIGH']
        ).count()
        
        criticality_score += min(high_risk_vulns * 0.5, 3.0)
        
        return min(criticality_score, 10.0)

    def get_risk_trends(self, target: str, days: int = 30) -> Dict:
        """Get risk score trends over time"""
        from django.utils import timezone
        from datetime import timedelta
        
        start_date = timezone.now() - timedelta(days=days)
        
        # Get vulnerabilities and findings within the time range
        vulnerabilities = Vulnerability.objects.filter(
            target=target,
            discovery_date__gte=start_date
        ).order_by('discovery_date')
        
        nuclei_findings = NucleiFinding.objects.filter(
            target=target,
            discovery_date__gte=start_date
        ).order_by('discovery_date')
        
        # Calculate daily risk scores
        daily_scores = {}
        for vuln in vulnerabilities:
            date = vuln.discovery_date.date()
            if date not in daily_scores:
                daily_scores[date] = []
            daily_scores[date].append(self.calculate_risk_score(vuln))
            
        for finding in nuclei_findings:
            date = finding.discovery_date.date()
            if date not in daily_scores:
                daily_scores[date] = []
            daily_scores[date].append(self.calculate_risk_score(finding))
            
        # Calculate average daily scores
        trend_data = []
        for date in sorted(daily_scores.keys()):
            trend_data.append({
                'date': date.isoformat(),
                'average_risk_score': round(sum(daily_scores[date]) / len(daily_scores[date]), 2),
                'findings_count': len(daily_scores[date])
            })
            
        return {
            'target': target,
            'period_days': days,
            'trend_data': trend_data
        }from django.http import JsonResponse
from django.views import View
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator

from vulnerability.correlation import VulnerabilityCorrelator


from .unified_scanner import UnifiedVulnerabilityScanner
from .models import Vulnerability
import json
import logging
from django.db.models import Count
from django.db import transaction

from django.http import JsonResponse
from django.views import View
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from .unified_scanner import UnifiedVulnerabilityScanner
import json
import logging

logger = logging.getLogger(__name__)  # Create logger at module level

# File to update: vulnerability/views.py

@method_decorator(csrf_exempt, name='dispatch')
class VulnerabilityScanView(View):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.scanner = UnifiedVulnerabilityScanner()
        self.logger = logger  # Assign module logger to instance

    def post(self, request):
        try:
            data = json.loads(request.body)
            
            # Extract parameters with defaults
            target = data.get('target')
            scan_type = data.get('scan_type', 'standard')
            include_zap = data.get('include_zap', True)
            include_nuclei = data.get('include_nuclei', True)
            nuclei_scan_type = data.get('nuclei_scan_type', 'basic')
            # New parameter for advanced correlation
            use_advanced_correlation = data.get('use_advanced_correlation', True)

            # Validate required parameters
            if not target:
                return JsonResponse({
                    'status': 'error',
                    'error': 'Target is required'
                }, status=400)

            # Validate nuclei_scan_type
            if nuclei_scan_type not in ['basic', 'advanced']:
                return JsonResponse({
                    'status': 'error',
                    'error': 'nuclei_scan_type must be either "basic" or "advanced"'
                }, status=400)

            # Log scan configuration
            self.logger.info(f"Starting vulnerability scan for {target}")
            self.logger.info(
                f"Scan configuration - Type: {scan_type}, "
                f"ZAP: {include_zap}, "
                f"Nuclei: {include_nuclei} ({nuclei_scan_type}), "
                f"Advanced Correlation: {use_advanced_correlation}"
            )
            
            # Run the scan
            results = self.scanner.scan_target(
                target=target,
                scan_type=scan_type,
                include_zap=include_zap,
                include_nuclei=include_nuclei,
                nuclei_scan_type=nuclei_scan_type,
                use_advanced_correlation=use_advanced_correlation
            )
            
            return JsonResponse(results)

        except json.JSONDecodeError:
            self.logger.error("Invalid JSON in request body")
            return JsonResponse({
                'status': 'error',
                'error': 'Invalid JSON data'
            }, status=400)
        except Exception as e:
            self.logger.exception(f"Error during vulnerability scan: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)

class VulnerabilityListView(View):
    def get(self, request):
        try:
            # Get filter parameters
            target = request.GET.get('target')
            severity = request.GET.get('severity')
            vuln_type = request.GET.get('type')
            source = request.GET.get('source')
            include_fixed = request.GET.get('include_fixed', 'false').lower() == 'true'

            # Build query
            query = Vulnerability.objects.all()
            if target:
                query = query.filter(target=target)
            if severity:
                query = query.filter(severity=severity.upper())
            if vuln_type:
                query = query.filter(vuln_type=vuln_type)
            if source:
                query = query.filter(source=source)
            if not include_fixed:
                query = query.filter(is_fixed=False)

            # Get vulnerabilities with selected fields
            vulnerabilities = query.values(
                'id', 'target', 'name', 'description', 'severity',
                'vuln_type', 'evidence', 'source', 'confidence',
                'discovery_date', 'is_fixed', 'fix_date', 'cvss_score'
            ).order_by('-discovery_date')

            return JsonResponse({
                'status': 'success',
                'count': len(vulnerabilities),
                'vulnerabilities': list(vulnerabilities)
            })

        except Exception as e:
            logger.error(f"Error listing vulnerabilities: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)

@method_decorator(csrf_exempt, name='dispatch')
class VulnerabilityUpdateView(View):
    def post(self, request, vuln_id):
        try:
            vulnerability = Vulnerability.objects.get(id=vuln_id)
            data = json.loads(request.body)

            # Update fields
            if 'is_fixed' in data:
                vulnerability.is_fixed = data['is_fixed']
            if 'notes' in data:
                vulnerability.notes = data['notes']
            if 'severity' in data:
                vulnerability.severity = data['severity'].upper()

            vulnerability.save()

            return JsonResponse({
                'status': 'success',
                'vulnerability': {
                    'id': vulnerability.id,
                    'name': vulnerability.name,
                    'is_fixed': vulnerability.is_fixed,
                    'notes': vulnerability.notes,
                    'severity': vulnerability.severity
                }
            })

        except Vulnerability.DoesNotExist:
            return JsonResponse({
                'status': 'error',
                'error': 'Vulnerability not found'
            }, status=404)
        except Exception as e:
            logger.error(f"Error updating vulnerability: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)

class ScannerStatusView(View):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.scanner = UnifiedVulnerabilityScanner()

    def get(self, request):
        try:
            status = self.scanner.get_scanner_status()
            return JsonResponse({
                'status': 'success',
                'scanners': status
            })
        except Exception as e:
            logger.error(f"Error getting scanner status: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)
            
# vulnerability/views.py (Add these views to your existing views.py)

from django.http import JsonResponse
from django.views import View
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from .nuclei_scanner import NucleiScanner
from .models import NucleiFinding
import json
import logging

logger = logging.getLogger(__name__)

@method_decorator(csrf_exempt, name='dispatch')
class NucleiScanView(View):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.scanner = NucleiScanner()

    def post(self, request):
        try:
            data = json.loads(request.body)
            target = data.get('target')
            scan_type = data.get('scan_type', 'basic')  # Default to basic scan
            scan_options = data.get('options', {})

            if not target:
                return JsonResponse({
                    'status': 'error',
                    'error': 'Target is required'
                }, status=400)

            # Choose scan type
            if scan_type == 'basic':
                logger.info(f"Starting basic Nuclei scan for {target}")
                scan_results = self.scanner.run_basic_scan(target)
            elif scan_type == 'advanced':
                logger.info(f"Starting advanced Nuclei scan for {target}")
                scan_results = self.scanner.run_advanced_scan(target, scan_options)
            else:
                logger.info(f"Starting custom Nuclei scan for {target}")
                scan_results = self.scanner.scan_target(target, scan_options)
            
            if scan_results.get('status') == 'success':
                # Prepare response with summary and limited findings
                findings = scan_results.get('findings', [])
                # Limit findings in response to first 10
                limited_findings = findings[:10] if len(findings) > 10 else findings
                
                response_data = {
                    'status': 'success',
                    'message': f'Nuclei scan completed for {target}',
                    'scan_id': scan_results.get('scan_id'),
                    'findings_count': len(findings),
                    'findings': limited_findings,  # Limit to first 10 for response size
                    'summary': scan_results.get('summary'),
                    'note': "Response limited to 10 findings" if len(findings) > 10 else ""
                }
                
                # Save findings to database
                saved_findings = []
                for finding in findings:
                    try:
                        nuclei_finding = NucleiFinding.objects.create(
                            template_id=finding.get('template_id', ''),
                            name=finding.get('name', ''),
                            severity=finding.get('severity', 'unknown').upper(),
                            finding_type=finding.get('type', ''),
                            host=finding.get('host', ''),
                            matched_at=finding.get('matched', ''),
                            description=finding.get('description', ''),
                            tags=finding.get('tags', []),
                            references=finding.get('references', []),
                            cwe=finding.get('cwe', ''),
                            cvss_score=finding.get('cvss_score'),
                            scan_id=scan_results.get('scan_id', ''),
                            target=target
                        )
                        saved_findings.append({
                            'id': nuclei_finding.id,
                            'name': nuclei_finding.name,
                            'severity': nuclei_finding.severity
                        })
                    except Exception as e:
                        logger.error(f"Error saving finding: {str(e)}")
                
                return JsonResponse(response_data)
            else:
                return JsonResponse({
                    'status': 'error',
                    'error': scan_results.get('error', 'Unknown error during scan')
                }, status=500)

        except json.JSONDecodeError:
            return JsonResponse({
                'status': 'error',
                'error': 'Invalid JSON data'
            }, status=400)
        except Exception as e:
            logger.error(f"Error during Nuclei scan: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)
            
class NucleiTemplatesView(View):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.scanner = NucleiScanner()

    def get(self, request):
        """Get information about available Nuclei templates"""
        try:
            template_info = self.scanner.get_template_info()
            return JsonResponse(template_info)
        except Exception as e:
            logger.error(f"Error getting template info: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)

    def post(self, request):
        """Update Nuclei templates"""
        try:
            update_result = self.scanner.update_templates()
            return JsonResponse(update_result)
        except Exception as e:
            logger.error(f"Error updating templates: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)

class NucleiFindingsView(View):
    def get(self, request):
        """Get Nuclei findings with optional filters"""
        try:
            target = request.GET.get('target')
            severity = request.GET.get('severity')
            template_id = request.GET.get('template_id')
            
            query = NucleiFinding.objects.all()
            if target:
                query = query.filter(target=target)
            if severity:
                query = query.filter(severity=severity.upper())
            if template_id:
                query = query.filter(template_id=template_id)
                
            findings = query.values(
                'id', 'template_id', 'name', 'severity',
                'finding_type', 'host', 'matched_at',
                'discovery_date', 'cvss_score'
            ).order_by('-discovery_date')
            
            return JsonResponse({
                'status': 'success',
                'count': len(findings),
                'findings': list(findings)
            })
            
        except Exception as e:
            logger.error(f"Error retrieving Nuclei findings: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)
            
            
# Add to vulnerability/views.py

# Imports needed at the top of views.py
from django.http import JsonResponse
from django.views import View
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from django.db.models import Count
from django.db import transaction
import json
import logging

from .correlation import VulnerabilityCorrelator
from .models import Vulnerability

# Then the CorrelationView class as provided previously
@method_decorator(csrf_exempt, name='dispatch')
class CorrelationView(View):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.correlator = VulnerabilityCorrelator()
        self.logger = logging.getLogger(__name__)

    def post(self, request):
        """Correlate vulnerabilities from different sources"""
        try:
            data = json.loads(request.body)
            
            # Extract parameters
            target = data.get('target')
            
            # Extract findings from different scanners if provided
            internal_results = data.get('internal_results', [])
            zap_results = data.get('zap_results', [])
            nuclei_results = data.get('nuclei_results', [])
            openvas_results = data.get('openvas_results', [])
            manual_results = data.get('manual_results', [])
            
            # Whether to save results to database
            save_to_db = data.get('save_to_db', True)
            
            # Whether to deduplicate existing entries for this target
            deduplicate = data.get('deduplicate', True)
            
            # Validate that at least one scanner has results
            if not any([internal_results, zap_results, nuclei_results, openvas_results, manual_results]):
                return JsonResponse({
                    'status': 'error',
                    'error': 'At least one scanner must provide results'
                }, status=400)

            # Run correlation
            self.logger.info(f"Starting vulnerability correlation for {target}")
            correlation_results = self.correlator.correlate_findings(
                internal_results=internal_results,
                zap_results=zap_results,
                nuclei_results=nuclei_results,
                openvas_results=openvas_results,
                manual_results=manual_results,
                target=target if save_to_db else None  # Only save if requested
            )
            
            # If requested, deduplicate existing entries for this target
            if deduplicate and target and save_to_db:
                self._deduplicate_vulnerabilities(target)
            
            return JsonResponse(correlation_results)

        except json.JSONDecodeError:
            self.logger.error("Invalid JSON in request body")
            return JsonResponse({
                'status': 'error',
                'error': 'Invalid JSON data'
            }, status=400)
        except Exception as e:
            self.logger.exception(f"Error during correlation: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)
            
    def get(self, request):
        """Get correlated vulnerabilities for a target with enhanced filtering"""
        try:
            target = request.GET.get('target')
            if not target:
                return JsonResponse({
                    'status': 'error',
                    'error': 'Target parameter is required'
                }, status=400)
                
            # Additional filter parameters
            severity = request.GET.get('severity')
            source = request.GET.get('source')
            show_fixed = request.GET.get('show_fixed', 'false').lower() == 'true'
            correlated_only = request.GET.get('correlated_only', 'false').lower() == 'true'
            deduplicate = request.GET.get('deduplicate', 'true').lower() == 'true'
            
            # Deduplicate vulnerabilities if requested
            if deduplicate:
                deduplication_result = self._deduplicate_vulnerabilities(target)
                
            # Build query
            query = Vulnerability.objects.filter(target=target)
            
            if severity:
                query = query.filter(severity=severity.upper())
                
            if source:
                # Filter for vulnerabilities that contain this source
                query = query.filter(source__icontains=source)
                
            if not show_fixed:
                query = query.filter(is_fixed=False)
                
            # Get all matching vulnerabilities
            vulnerabilities = list(query)
            
            # Post-process for correlated findings if requested
            if correlated_only:
                vulnerabilities = [v for v in vulnerabilities if ',' in v.source]
            
            # Generate correlation statistics
            correlated_count = sum(1 for v in vulnerabilities if ',' in v.source)
            
            # Count by source
            source_stats = {}
            for vuln in vulnerabilities:
                for source in vuln.source.split(','):
                    source_stats[source] = source_stats.get(source, 0) + 1
            
            # Count by severity
            severity_stats = {
                'CRITICAL': sum(1 for v in vulnerabilities if v.severity == 'CRITICAL'),
                'HIGH': sum(1 for v in vulnerabilities if v.severity == 'HIGH'),
                'MEDIUM': sum(1 for v in vulnerabilities if v.severity == 'MEDIUM'),
                'LOW': sum(1 for v in vulnerabilities if v.severity == 'LOW')
            }
            
            # Serialize vulnerabilities for response
            vuln_data = []
            for vuln in vulnerabilities:
                vuln_data.append({
                    'id': vuln.id,
                    'name': vuln.name,
                    'description': vuln.description,
                    'severity': vuln.severity,
                    'vuln_type': vuln.vuln_type,
                    'source': vuln.source,
                    'sources': vuln.source.split(','),  # Parsed as list for convenience
                    'confidence': vuln.confidence,
                    'discovery_date': vuln.discovery_date.isoformat(),
                    'is_fixed': vuln.is_fixed,
                    'cvss_score': vuln.cvss_score,
                    'is_correlated': ',' in vuln.source
                })
            
            return JsonResponse({
                'status': 'success',
                'target': target,
                'total_vulnerabilities': len(vulnerabilities),
                'correlated_vulnerabilities': correlated_count,
                'source_statistics': source_stats,
                'severity_statistics': severity_stats,
                'filters_applied': {
                    'severity': severity,
                    'source': source,
                    'show_fixed': show_fixed,
                    'correlated_only': correlated_only
                },
                'vulnerabilities': vuln_data
            })
            
        except Exception as e:
            self.logger.error(f"Error retrieving correlated vulnerabilities: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)
    
    def _deduplicate_vulnerabilities(self, target):
        """
        Deduplicate vulnerabilities for a target by merging duplicates
        Returns statistics about the deduplication
        """
        try:
            start_count = Vulnerability.objects.filter(target=target).count()
            self.logger.info(f"Starting deduplication for {target} with {start_count} vulnerabilities")
            
            # Process batch by batch to avoid memory issues with large datasets
            # Group by name, vuln_type and severity
            deduplication_count = 0
            
            with transaction.atomic():
                # Get all distinct vulnerability signatures
                vulnerability_groups = Vulnerability.objects.filter(
                    target=target
                ).values('name', 'vuln_type', 'severity').annotate(
                    count=Count('id')
                ).filter(count__gt=1)  # Only process groups with duplicates
                
                for group in vulnerability_groups:
                    # Get all vulnerabilities matching this signature
                    duplicates = Vulnerability.objects.filter(
                        target=target,
                        name=group['name'],
                        vuln_type=group['vuln_type'],
                        severity=group['severity']
                    ).order_by('discovery_date')
                    
                    # Skip if only one vulnerability (should never happen due to our filter above)
                    if duplicates.count() <= 1:
                        continue
                        
                    # Keep the first vulnerability (oldest) and merge sources from the rest
                    base_vuln = duplicates.first()
                    sources = set(base_vuln.source.split(','))
                    references = set(base_vuln.references)
                    all_metadata = base_vuln.metadata.copy() if base_vuln.metadata else {}
                    
                    # Track all merged IDs for logging
                    merged_ids = []
                    
                    # Process all but the first vulnerability
                    for dup in duplicates[1:]:
                        # Add sources
                        for source in dup.source.split(','):
                            sources.add(source)
                        
                        # Add references
                        for ref in dup.references:
                            references.add(ref)
                        
                        # Merge metadata
                        if dup.metadata:
                            for key, value in dup.metadata.items():
                                if key not in all_metadata:
                                    all_metadata[key] = value
                                elif key == 'merged_ids':
                                    # Combine merged_ids lists
                                    all_metadata[key] = list(set(all_metadata[key] + value))
                        
                        # Track this ID as merged
                        merged_ids.append(dup.id)
                        
                        # Delete the duplicate
                        dup.delete()
                        deduplication_count += 1
                    
                    # Update the base vulnerability with merged data
                    base_vuln.source = ','.join(sorted(sources))
                    base_vuln.references = list(references)
                    
                    # Add merged_ids to metadata
                    if 'merged_ids' not in all_metadata:
                        all_metadata['merged_ids'] = []
                    all_metadata['merged_ids'].extend(merged_ids)
                    all_metadata['merged_ids'] = list(set(all_metadata['merged_ids']))
                    
                    # Update metadata
                    base_vuln.metadata = all_metadata
                    
                    # Save changes
                    base_vuln.save()
            
            end_count = Vulnerability.objects.filter(target=target).count()
            reduction = start_count - end_count
            
            self.logger.info(f"Deduplication completed: {reduction} duplicates merged")
            
            return {
                'status': 'success',
                'target': target,
                'original_count': start_count,
                'final_count': end_count,
                'duplicates_merged': reduction
            }
            
        except Exception as e:
            self.logger.error(f"Error during deduplication: {str(e)}")
            return {
                'status': 'error',
                'error': str(e)
            }from django.db import models

from django.db import models

class Subdomain(models.Model):
    domain = models.CharField(max_length=255)
    subdomain = models.CharField(max_length=255)
    ip_address = models.GenericIPAddressField(null=True)
    discovered_date = models.DateTimeField(auto_now_add=True)
    is_active = models.BooleanField(default=True)

    class Meta:
        ordering = ['-discovered_date']
        unique_together = ['domain', 'subdomain']
        indexes = [
            models.Index(fields=['domain']),
            models.Index(fields=['subdomain']),
        ]

    def __str__(self):
        return self.subdomain

    def save(self, *args, **kwargs):
        # Update instead of error on duplicate
        try:
            super().save(*args, **kwargs)
        except:
            existing = Subdomain.objects.get(domain=self.domain, subdomain=self.subdomain)
            existing.ip_address = self.ip_address
            existing.is_active = self.is_active
            existing.save()

class Service(models.Model):
    RISK_LEVELS = [
        ('LOW', 'Low'),
        ('MEDIUM', 'Medium'),
        ('HIGH', 'High'),
    ]

    CATEGORIES = [
        ('web', 'Web Services'),
        ('database', 'Database Services'),
        ('mail', 'Mail Services'),
        ('file_transfer', 'File Transfer'),
        ('remote_access', 'Remote Access'),
        ('domain_services', 'Domain Services'),
        ('monitoring', 'Monitoring'),
        ('security', 'Security Services'),
        ('other', 'Other'),
    ]

    PROTOCOLS = [
        ('tcp', 'TCP'),
        ('udp', 'UDP'),
        ('sctp', 'SCTP'),
    ]

    host = models.CharField(max_length=255)
    port = models.IntegerField()
    protocol = models.CharField(max_length=10, choices=PROTOCOLS, default='tcp')
    name = models.CharField(max_length=100)
    product = models.CharField(max_length=100, blank=True)
    version = models.CharField(max_length=100, blank=True)
    extra_info = models.TextField(blank=True)
    category = models.CharField(max_length=50, choices=CATEGORIES)
    risk_level = models.CharField(max_length=10, choices=RISK_LEVELS)
    cpe = models.JSONField(default=list)
    scan_date = models.DateTimeField(auto_now_add=True)
    last_seen = models.DateTimeField(auto_now=True)
    is_active = models.BooleanField(default=True)

    class Meta:
        unique_together = ('host', 'port', 'protocol')
        ordering = ['-scan_date']
        indexes = [
            models.Index(fields=['host', 'port']),
            models.Index(fields=['category']),
            models.Index(fields=['risk_level']),
        ]

    def __str__(self):
        return f"{self.host}:{self.port} - {self.name}"

class PortScan(models.Model):
    STATES = [
        ('pending', 'Pending'),
        ('in_progress', 'In Progress'), 
        ('open', 'Open'),
        ('closed', 'Closed'),
        ('filtered', 'Filtered'),
        ('unfiltered', 'Unfiltered'),
        ('error', 'Error'),
        ('completed', 'Completed')
    ]

    SCAN_STATUS = [
        ('pending', 'Pending'),
        ('in_progress', 'In Progress'),
        ('completed', 'Completed'),
        ('error', 'Error')
    ]

    host = models.CharField(max_length=255)
    port = models.IntegerField()
    service = models.CharField(max_length=100)
    state = models.CharField(max_length=50, choices=STATES)
    scan_status = models.CharField(max_length=50, choices=SCAN_STATUS, default='pending')
    scan_date = models.DateTimeField(auto_now_add=True)
    protocol = models.CharField(max_length=10, choices=Service.PROTOCOLS, default='tcp')
    banner = models.TextField(blank=True)
    notes = models.TextField(blank=True)
    scan_type = models.CharField(max_length=50, default='quick')
    error_message = models.TextField(blank=True)

    class Meta:
        ordering = ['-scan_date']
        indexes = [
            models.Index(fields=['host', 'port']),
            models.Index(fields=['state']),
            models.Index(fields=['scan_date']),
            models.Index(fields=['scan_status'])
        ]

    def __str__(self):
        return f"{self.host}:{self.port} - {self.state}"
class SystemLogEntry(models.Model):
    """Model for system logs admin interface"""
    class Meta:
        managed = False
        verbose_name_plural = 'System Logs'
        default_permissions = ('view',)import nmap
from typing import Dict, Any
from enum import Enum

class ScanType(Enum):
    QUICK = "quick"       # Fast scan of most common ports
    PARTIAL = "partial"   # Standard scan with version detection
    COMPLETE = "complete" # Comprehensive scan of all ports with version detection
    FULL = "full"        # Intensive scan with all possible features

class PortScanner:
    def __init__(self):
        self.scanner = nmap.PortScanner()
        
    def get_scan_config(self, scan_type: str) -> Dict[str, str]:
        scan_configs = {
            ScanType.QUICK.value: {
                'ports': '21-23,25,80,443,3306,8080',
                'arguments': '-sV -T4 --version-intensity 0'  # Fast scan
            },
            ScanType.PARTIAL.value: {
                'ports': '1-1000',
                'arguments': '-sV -T4 -sC --version-intensity 5'  # Standard scan
            },
            ScanType.COMPLETE.value: {
                'ports': '1-65535',
                'arguments': '-sV -T4 -sC -O --version-intensity 7'  # All ports
            },
            ScanType.FULL.value: {
                'ports': '1-65535',
                'arguments': '-sV -T4 -sC -O -A --version-intensity 9 --script=vuln'  # Everything
            }
        }
        return scan_configs.get(scan_type, scan_configs[ScanType.QUICK.value])

    def scan(self, target: str, scan_type: str = "quick") -> Dict[str, Any]:
        try:
            config = self.get_scan_config(scan_type)
            self.scanner.scan(target, config['ports'], config['arguments'])
            
            scan_results = []
            scan_info = {
                'scan_type': scan_type,
                'command_line': self.scanner.command_line(),
                'scan_time': self.scanner.scanstats().get('elapsed', ''),
                'total_hosts': len(self.scanner.all_hosts())
            }
            
            for host in self.scanner.all_hosts():
                host_data = {
                    'host': host,
                    'state': self.scanner[host].state(),
                    'ports': []
                }
                
                for proto in self.scanner[host].all_protocols():
                    ports = self.scanner[host][proto].keys()
                    for port in ports:
                        port_info = self.scanner[host][proto][port]
                        port_data = {
                            'port': port,
                            'state': port_info['state'],
                            'service': port_info.get('name', ''),
                            'version': port_info.get('version', ''),
                            'product': port_info.get('product', ''),
                            'extrainfo': port_info.get('extrainfo', ''),
                            'reason': port_info.get('reason', ''),
                            'cpe': port_info.get('cpe', '')
                        }
                        host_data['ports'].append(port_data)
                
                if 'osmatch' in self.scanner[host]:
                    host_data['os_matches'] = self.scanner[host]['osmatch']
                
                scan_results.append(host_data)
            
            return {
                'status': 'success',
                'scan_info': scan_info,
                'results': scan_results
            }
            
        except Exception as e:
            return {
                'status': 'error',
                'error': str(e)
            }

    def get_available_scan_types(self) -> Dict[str, str]:
        return {
            ScanType.QUICK.value: "Fast scan of most common ports (21-23,25,80,443,3306,8080)",
            ScanType.PARTIAL.value: "Standard scan of first 1000 ports with version detection",
            ScanType.COMPLETE.value: "Comprehensive scan of all ports with version and OS detection",
            ScanType.FULL.value: "Intensive scan with all features including vulnerability detection"
        }import dns.resolver
import dns.zone
import socket
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
from typing import List, Dict
import requests
import re

class SubdomainEnumerator:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.resolver = dns.resolver.Resolver()
        self.resolver.timeout = 1
        self.resolver.lifetime = 1
        
        # Common subdomain prefixes
        self.common_subdomains = [
            'www', 'mail', 'ftp', 'smtp', 'pop', 'ns1', 'ns2', 'dns1', 'dns2',
            'webmail', 'admin', 'secure', 'vpn', 'remote', 'test', 'dev', 'host',
            'support', 'api', 'dev', 'staging', 'app', 'portal', 'beta'
        ]

    def enumerate_subdomains(self, target: str) -> List[Dict]:
        """Main subdomain enumeration method combining multiple techniques"""
        # Clean target - remove protocol and path to get just the domain
        domain = self._extract_domain(target)
        if not domain:
            self.logger.error(f"Invalid domain provided: {target}")
            return []
            
        self.logger.info(f"Starting subdomain enumeration for domain: {domain}")
        discovered_subdomains = set()
        results = []

        # 1. DNS enumeration
        dns_results = self._dns_enumeration(domain)
        for subdomain in dns_results:
            discovered_subdomains.add(subdomain)

        # 2. Brute force common subdomains
        brute_results = self._brute_force_subdomains(domain)
        for subdomain in brute_results:
            discovered_subdomains.add(subdomain)

        # Process and validate all discovered subdomains
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_subdomain = {
                executor.submit(self._validate_subdomain, subdomain): subdomain 
                for subdomain in discovered_subdomains
            }
            
            for future in as_completed(future_to_subdomain):
                subdomain = future_to_subdomain[future]
                try:
                    result = future.result()
                    if result:
                        results.append(result)
                except Exception as e:
                    self.logger.error(f"Error validating {subdomain}: {str(e)}")

        return results

    def _extract_domain(self, url: str) -> str:
        """Extract root domain from a URL or domain string"""
        # Remove protocol if present
        if '://' in url:
            url = url.split('://', 1)[1]
            
        # Remove path, query params, and fragment
        url = url.split('/', 1)[0]
        url = url.split('?', 1)[0]
        url = url.split('#', 1)[0]
        
        # Remove port if present
        url = url.split(':', 1)[0]
        
        # Validate domain format
        domain_pattern = r'^([a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]{2,}$'
        if re.match(domain_pattern, url):
            return url
        
        return None

    def _dns_enumeration(self, domain: str) -> set:
        """Enumerate subdomains using DNS queries"""
        discovered = set()
        
        try:
            # Try zone transfer first
            ns_records = self.resolver.resolve(domain, 'NS')
            for ns in ns_records:
                try:
                    zone = dns.zone.from_xfr(dns.query.xfr(str(ns), domain))
                    for name, _ in zone.nodes.items():
                        subdomain = str(name) + '.' + domain
                        discovered.add(subdomain)
                except:
                    continue

            # Try to get common DNS records
            for record_type in ['A', 'AAAA', 'CNAME', 'MX', 'NS', 'TXT']:
                try:
                    answers = self.resolver.resolve(domain, record_type)
                    for rdata in answers:
                        if record_type == 'MX':
                            discovered.add(str(rdata.exchange).rstrip('.'))
                        elif record_type == 'NS':
                            discovered.add(str(rdata).rstrip('.'))
                        elif record_type == 'CNAME':
                            discovered.add(str(rdata.target).rstrip('.'))
                except:
                    continue

        except Exception as e:
            self.logger.error(f"Error in DNS enumeration for {domain}: {str(e)}")

        return discovered

    def _brute_force_subdomains(self, domain: str) -> set:
        """Brute force subdomains using common prefixes"""
        discovered = set()
        
        with ThreadPoolExecutor(max_workers=20) as executor:
            future_to_subdomain = {
                executor.submit(self._check_subdomain, f"{prefix}.{domain}"): prefix 
                for prefix in self.common_subdomains
            }
            
            for future in as_completed(future_to_subdomain):
                try:
                    result = future.result()
                    if result:
                        discovered.add(result)
                except Exception as e:
                    continue

        return discovered

    def _check_subdomain(self, subdomain: str) -> str:
        """Check if a subdomain exists"""
        try:
            self.resolver.resolve(subdomain, 'A')
            return subdomain
        except:
            return None

    def _validate_subdomain(self, subdomain: str) -> Dict:
        """Validate and get information about a subdomain"""
        try:
            ip_address = socket.gethostbyname(subdomain)
            
            # Basic HTTP check
            is_http = False
            http_status = None
            try:
                response = requests.get(f"http://{subdomain}", timeout=3)
                is_http = True
                http_status = response.status_code
            except:
                try:
                    response = requests.get(f"https://{subdomain}", timeout=3)
                    is_http = True
                    http_status = response.status_code
                except:
                    pass

            return {
                'subdomain': subdomain,
                'ip_address': ip_address,
                'is_http': is_http,
                'http_status': http_status,
                'status': 'active'
            }
        except Exception as e:
            return Noneimport nmap
from typing import Dict, List, Optional
import logging
from datetime import datetime
import socket
import requests
from urllib.parse import urlparse
import concurrent.futures
import threading
from django.conf import settings

class ServiceIdentifier:
    def __init__(self):
        self.scanner = nmap.PortScanner()
        self.logger = logging.getLogger(__name__)
        self.timeout = getattr(settings, 'SERVICE_SCAN_TIMEOUT', 180)  # 3 minutes default
        self._stop_event = threading.Event()

    def identify_services(self, target: str, scan_type: str = 'standard') -> Dict:
        """Comprehensive service identification with timeout handling"""
        try:
            # Parse target
            parsed_target = urlparse(target)
            target_host = parsed_target.netloc or parsed_target.path
            target_host = target_host.split(':')[0]

            scan_config = self._get_scan_config(scan_type)
            self.logger.info(f"Starting {scan_type} service scan for {target_host}")

            # Use ThreadPoolExecutor for timeout handling
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(self._run_service_scan, target_host, scan_config)
                try:
                    result = future.result(timeout=self.timeout)
                    return result
                except concurrent.futures.TimeoutError:
                    self._stop_event.set()
                    return {
                        'status': 'error',
                        'error': 'Scan timeout',
                        'details': f'Service scan exceeded {self.timeout} seconds'
                    }

        except Exception as e:
            self.logger.error(f"Service scan failed for {target}: {str(e)}")
            return {
                'status': 'error',
                'error': str(e),
                'details': 'Service scan failed'
            }
            
    def _get_scan_config(self, scan_type: str) -> Dict:
        """Get scan configuration based on scan type"""
        configs = {
            'quick': {
                'ports': '21-23,25,80,443,3306,8080',
                'arguments': '-sV -sT -Pn -T4 --version-light'  # Fast scan
            },
            'standard': {
                'ports': '1-1000',
                'arguments': '-sV -sT -Pn -T4 --version-all'  # Standard scan
            },
            'full': {
                'ports': '1-65535',
                'arguments': '-sV -sT -Pn -T4 -A --version-all'  # Full scan
            },
            'stealth': {
                'ports': '1-1000',
                'arguments': '-sV -sS -Pn -T2 --version-all'  # Stealth scan
            }
        }
        return configs.get(scan_type, configs['standard'])

    def _run_service_scan(self, target: str, config: Dict) -> Dict:
        """Execute the actual service scan with interrupt handling"""
        try:
            # Initial port discovery
            self.logger.info(f"Starting port discovery for {target}")
            open_ports = self._discover_ports(target, config)
            
            if not open_ports:
                return {
                    'status': 'success',
                    'services': [],
                    'total_services': 0,
                    'scan_stats': {'open_ports': 0}
                }

            # Detailed service scanning
            self.logger.info(f"Starting service detection on {len(open_ports)} ports")
            scan_result = self.scanner.scan(
                target,
                ports=','.join(map(str, open_ports)),
                arguments=config['arguments']
            )

            # Debug logging
            self.logger.info(f"Raw scan result structure: {list(scan_result.keys())}")
            self.logger.info(f"Scan hosts: {self.scanner.all_hosts()}")
            if target in self.scanner.all_hosts():
                self.logger.info(f"Protocols for {target}: {self.scanner[target].all_protocols()}")
                for proto in self.scanner[target].all_protocols():
                    self.logger.info(f"Ports for {proto}: {list(self.scanner[target][proto].keys())}")

            if self._stop_event.is_set():
                return {
                    'status': 'error',
                    'error': 'Scan interrupted',
                    'details': 'Service scan was interrupted'
                }

            # Process scan results
            services = []
            for host in self.scanner.all_hosts():
                for proto in self.scanner[host].all_protocols():
                    for port in self.scanner[host][proto].keys():
                        service_info = self.scanner[host][proto][port]
                        if service_info['state'] == 'open':
                            service_detail = {
                                'port': port,
                                'protocol': proto,
                                'state': service_info['state'],
                                'service': {
                                    'name': service_info.get('name', 'unknown'),
                                    'product': service_info.get('product', ''),
                                    'version': service_info.get('version', ''),
                                    'extrainfo': service_info.get('extrainfo', ''),
                                    'cpe': service_info.get('cpe', [])
                                },
                                'category': self._categorize_service(service_info),
                                'risk_level': self._assess_risk_level(service_info)
                            }
                            services.append(service_detail)

            # Enhance service information
            enhanced_services = []
            for service in services:
                try:
                    enhanced_service = self._enhance_service_info(target, service.copy())
                    if enhanced_service:
                        enhanced_services.append(enhanced_service)
                    else:
                        enhanced_services.append(service)
                except Exception as e:
                    self.logger.error(f"Error enhancing service {service.get('port')}: {str(e)}")
                    enhanced_services.append(service)

            scan_time = scan_result.get('nmap', {}).get('scanstats', {}).get('elapsed', 'unknown')
            self.logger.info(f"Scan completed in {scan_time} seconds")
            
            # Log found services
            for service in enhanced_services:
                self.logger.info(f"Found service: Port {service['port']} - {service['service']['name']} ({service['category']})")

            return {
                'status': 'success',
                'target': target,
                'timestamp': datetime.now().isoformat(),
                'services': enhanced_services,
                'total_services': len(enhanced_services),
                'scan_stats': {
                    'open_ports': len(open_ports),
                    'scan_time': scan_time
                }
            }

        except Exception as e:
            self.logger.error(f"Error in service scan: {str(e)}")
            return {
                'status': 'error',
                'error': str(e),
                'details': 'Error during service scan execution'
            }

    def _discover_ports(self, target: str, config: Dict) -> List[int]:
        """Initial port discovery with timeout"""
        open_ports = set()
        ports_to_scan = self._parse_ports(config['ports'])
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
            future_to_port = {
                executor.submit(self._check_port, target, port): port 
                for port in ports_to_scan
            }
            
            for future in concurrent.futures.as_completed(future_to_port):
                if not self._stop_event.is_set():
                    try:
                        is_open = future.result()
                        if is_open:
                            open_ports.add(future_to_port[future])
                    except:
                        continue
                else:
                    break

        return list(sorted(open_ports))

    def _check_port(self, target: str, port: int) -> bool:
        """Check if a port is open"""
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                sock.settimeout(1)
                result = sock.connect_ex((target, port))
                return result == 0
        except:
            return False

    def _parse_ports(self, ports_str: str) -> List[int]:
        """Parse ports string into list of port numbers"""
        ports = set()
        for part in ports_str.split(','):
            if '-' in part:
                start, end = map(int, part.split('-'))
                ports.update(range(start, end + 1))
            else:
                ports.add(int(part))
        return list(ports)

    def _enhance_service_info(self, target: str, service: Dict) -> Dict:
        """Enhance service information with additional checks"""
        try:
            if service['service']['name'] == 'http':
                self._enhance_web_service(target, service)
            elif service['service']['name'] in ['ssh', 'ftp', 'smtp']:
                self._enhance_common_service(target, service)
            return service
        except:
            return service

    def _enhance_web_service(self, target: str, service: Dict) -> None:
        """Enhance web service information"""
        try:
            port = service['port']
            url = f"http{'s' if port == 443 else ''}://{target}:{port}"
            response = requests.get(url, timeout=3, verify=False)
            
            service['service']['headers'] = dict(response.headers)
            service['service']['status_code'] = response.status_code
            service['service']['technologies'] = self._detect_technologies(response)
        except:
            pass

    def _enhance_common_service(self, target: str, service: Dict) -> None:
        """Enhance common service information with banner grabbing"""
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                sock.settimeout(2)
                sock.connect((target, service['port']))
                banner = sock.recv(1024).decode('utf-8', errors='ignore')
                service['service']['banner'] = banner
        except:
            pass

    def _detect_technologies(self, response) -> List[str]:
        """Detect web technologies from response headers"""
        technologies = []
        headers = response.headers
        
        tech_headers = {
            'X-Powered-By': None,
            'Server': None,
            'X-AspNet-Version': 'ASP.NET',
            'X-Runtime': 'Ruby'
        }
        
        for header, tech in tech_headers.items():
            if header in headers:
                technologies.append(tech or headers[header])
                
        return technologies

    def _process_scan_results(self, target: str) -> List[Dict]:
        """Process scan results and categorize services"""
        services = []
        
        if target not in self.scanner.all_hosts():
            return services

        for proto in self.scanner[target].all_protocols():
            ports = self.scanner[target][proto].keys()
            
            for port in ports:
                service_info = self.scanner[target][proto][port]
                service_detail = {
                    'port': port,
                    'protocol': proto,
                    'state': service_info['state'],
                    'service': {
                        'name': service_info.get('name', 'unknown'),
                        'product': service_info.get('product', ''),
                        'version': service_info.get('version', ''),
                        'extrainfo': service_info.get('extrainfo', ''),
                        'cpe': service_info.get('cpe', [])
                    },
                    'category': self._categorize_service(service_info),
                    'risk_level': self._assess_risk_level(service_info)
                }
                services.append(service_detail)
        
        return services

    def _categorize_service(self, service_info: Dict) -> str:
        """Categorize service based on name and product"""
        service_categories = {
            'web': ['http', 'https', 'nginx', 'apache'],
            'database': ['mysql', 'postgresql', 'mongodb'],
            'mail': ['smtp', 'pop3', 'imap'],
            'file_transfer': ['ftp', 'sftp'],
            'remote_access': ['ssh', 'telnet', 'rdp'],
            'dns': ['dns', 'domain']
        }

        service_name = service_info.get('name', '').lower()
        
        for category, services in service_categories.items():
            if any(s in service_name for s in services):
                return category
        return 'other'

    def _assess_risk_level(self, service_info: Dict) -> str:
        """Basic risk assessment of services"""
        high_risk = ['telnet', 'ftp']
        medium_risk = ['smtp', 'pop3']
        
        service_name = service_info.get('name', '').lower()
        
        if any(service in service_name for service in high_risk):
            return 'HIGH'
        elif any(service in service_name for service in medium_risk):
            return 'MEDIUM'
        return 'LOW'

    def _log_service_details(self, target: str, services: List[Dict]) -> None:
        """Log detailed information about identified services"""
        self.logger.info(f"Service identification completed for {target}")
        self.logger.info(f"Total services identified: {len(services)}")
        
        for service in services:
            log_message = (
                f"Port {service['port']}/{service['protocol']}: "
                f"{service['service']['name']} "
                f"({service['category']}, Risk: {service['risk_level']})"
            )
            if service['service']['version']:
                log_message += f" Version: {service['service']['version']}"
            
            self.logger.info(log_message)from django.http import JsonResponse
from django.views import View
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from django.db.models import Count, Q
from django.utils import timezone
from .models import Service, Subdomain, PortScan
from .service_identifier import ServiceIdentifier
import json
import logging
from .subdomain_enumerator import SubdomainEnumerator
from .scanner import PortScanner, ScanType
logger = logging.getLogger(__name__)

@method_decorator(csrf_exempt, name='dispatch')
class SubdomainScanView(View):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.enumerator = SubdomainEnumerator()
        
    def get(self, request):
        try:
            target = request.GET.get('target')
            if not target:
                return JsonResponse({'error': 'Target parameter is required'}, status=400)
                
            subdomains = Subdomain.objects.filter(domain=target).values(
                'subdomain', 'ip_address', 'discovered_date', 'is_active'
            )
            
            return JsonResponse({
                'status': 'success',
                'target': target,
                'subdomains': list(subdomains)
            })
        except Exception as e:
            logger.error(f"Error retrieving subdomains: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

    def post(self, request):
        try:
            data = json.loads(request.body)
            domain = data.get('domain')
            
            if not domain:
                return JsonResponse({'error': 'Domain is required'}, status=400)
            
            # Perform actual subdomain enumeration
            discovered = self.enumerator.enumerate_subdomains(domain)
            
            # Save results to database
            saved_subdomains = []
            for subdomain_data in discovered:
                subdomain, created = Subdomain.objects.update_or_create(
                    domain=domain,
                    subdomain=subdomain_data['subdomain'],
                    defaults={
                        'ip_address': subdomain_data['ip_address'],
                        'is_active': True
                    }
                )
                saved_subdomains.append({
                    'id': subdomain.id,
                    'subdomain': subdomain.subdomain,
                    'ip_address': subdomain.ip_address,
                    'status': 'created' if created else 'updated'
                })
            
            return JsonResponse({
                'status': 'success',
                'message': 'Subdomain scan completed',
                'domain': domain,
                'total_subdomains': len(saved_subdomains),
                'subdomains': saved_subdomains
            })
        except Exception as e:
            logger.error(f"Subdomain scan error: {str(e)}")
            return JsonResponse({
                'error': str(e)
            }, status=500)

class SubdomainListView(View):
    def get(self, request):
        try:
            domain = request.GET.get('domain')
            query = Subdomain.objects.all()
            
            if domain:
                query = query.filter(domain=domain)
            
            subdomains = query.values('id', 'domain', 'subdomain', 
                                    'ip_address', 'discovered_date')
            
            return JsonResponse({
                'status': 'success',
                'subdomains': list(subdomains)
            })
        except Exception as e:
            logger.error(f"Error listing subdomains: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

class SubdomainDetailView(View):
    def get(self, request, subdomain_id):
        try:
            subdomain = Subdomain.objects.get(id=subdomain_id)
            return JsonResponse({
                'status': 'success',
                'subdomain': {
                    'id': subdomain.id,
                    'domain': subdomain.domain,
                    'subdomain': subdomain.subdomain,
                    'ip_address': subdomain.ip_address,
                    'discovered_date': subdomain.discovered_date.isoformat(),
                    'is_active': subdomain.is_active
                }
            })
        except Subdomain.DoesNotExist:
            return JsonResponse({'error': 'Subdomain not found'}, status=404)
        except Exception as e:
            logger.error(f"Error retrieving subdomain details: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

@method_decorator(csrf_exempt, name='dispatch')
class PortScanView(View):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.scanner = PortScanner()
        self.logger = logging.getLogger(__name__)

    def post(self, request):
        try:
            data = json.loads(request.body)
            target = data.get('target')
            scan_type = data.get('scan_type', 'quick')
            custom_ports = data.get('ports')

            if not target:
                return JsonResponse({'error': 'Target is required'}, status=400)

            if scan_type not in ScanType.__members__ and not custom_ports:
                return JsonResponse({
                    'error': f'Invalid scan type. Available types: {", ".join(ScanType.__members__.keys())}'
                }, status=400)

            # Start the scan
            self.logger.info(f"Starting {scan_type} port scan for {target}")
            scan_result = self.scanner.scan(target, scan_type)

            if scan_result['status'] == 'success':
                saved_ports = []
                
                # Process and save results
                for host in scan_result['results']:
                    for port_data in host['ports']:
                        scan = PortScan.objects.create(
                            host=target,
                            port=port_data['port'],
                            service=port_data['service'],
                            state=port_data['state'],
                            protocol='tcp',
                            scan_status='completed',
                            scan_type=scan_type,
                            banner=port_data.get('extrainfo', ''),
                            notes=f"Version: {port_data.get('version', 'unknown')}"
                        )
                        saved_ports.append({
                            'port': scan.port,
                            'state': scan.state,
                            'service': scan.service
                        })

                return JsonResponse({
                    'status': 'success',
                    'message': f'Port scan completed for {target}',
                    'target': target,
                    'scan_type': scan_type,
                    'total_ports': len(saved_ports),
                    'open_ports': len([p for p in saved_ports if p['state'] == 'open']),
                    'ports': saved_ports,
                    'scan_time': scan_result['scan_info']['scan_time']
                })
            else:
                return JsonResponse({
                    'status': 'error',
                    'error': scan_result.get('error', 'Unknown error during scan')
                }, status=500)

        except json.JSONDecodeError:
            return JsonResponse({'error': 'Invalid JSON data'}, status=400)
        except Exception as e:
            self.logger.error(f"Port scan error: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

    def get(self, request):
        """Get scan results for a target"""
        try:
            target = request.GET.get('target')
            if not target:
                return JsonResponse({'error': 'Target parameter is required'}, status=400)

            scans = PortScan.objects.filter(
                host=target, 
                scan_status='completed'
            ).values(
                'port', 'service', 'state', 'protocol', 
                'banner', 'scan_date'
            ).order_by('port')

            return JsonResponse({
                'status': 'success',
                'target': target,
                'total_ports': len(scans),
                'open_ports': scans.filter(state='open').count(),
                'ports': list(scans)
            })

        except Exception as e:
            self.logger.error(f"Error retrieving port scan results: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

    def _estimate_scan_time(self, scan_type, ports):
        """Estimate scan time based on scan type and ports"""
        if scan_type == 'quick':
            return "1-2 minutes"
        elif scan_type == 'partial':
            return "5-10 minutes"
        elif scan_type == 'complete':
            return "30-60 minutes"
        elif scan_type == 'full':
            return "1-2 hours"
        else:
            # Custom port range
            port_count = len(ports.split(','))
            if port_count < 100:
                return "1-5 minutes"
            elif port_count < 1000:
                return "5-15 minutes"
            else:
                return "15+ minutes"

class PortScanListView(View):
    def get(self, request):
        try:
            host = request.GET.get('host')
            state = request.GET.get('state')
            
            query = PortScan.objects.all()
            if host:
                query = query.filter(host=host)
            if state:
                query = query.filter(state=state)
                
            scans = query.values('id', 'host', 'port', 'service', 
                               'state', 'protocol', 'scan_date')
            
            return JsonResponse({
                'status': 'success',
                'scans': list(scans)
            })
        except Exception as e:
            logger.error(f"Error listing port scans: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

class PortScanDetailView(View):
    def get(self, request, scan_id):
        try:
            scan = PortScan.objects.get(id=scan_id)
            return JsonResponse({
                'status': 'success',
                'scan': {
                    'id': scan.id,
                    'host': scan.host,
                    'port': scan.port,
                    'service': scan.service,
                    'state': scan.state,
                    'protocol': scan.protocol,
                    'scan_date': scan.scan_date.isoformat(),
                    'banner': scan.banner,
                    'notes': scan.notes
                }
            })
        except PortScan.DoesNotExist:
            return JsonResponse({'error': 'Scan not found'}, status=404)
        except Exception as e:
            logger.error(f"Error retrieving scan details: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

@method_decorator(csrf_exempt, name='dispatch')
class ServiceScanView(View):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.identifier = ServiceIdentifier()
        self.logger = logging.getLogger(__name__)

    def post(self, request):
        try:
            data = json.loads(request.body)
            target = data.get('target')
            scan_type = data.get('scan_type', 'standard')

            if not target:
                return JsonResponse({'error': 'Target is required'}, status=400)

            # Start service identification
            self.logger.info(f"Starting {scan_type} service scan for {target}")
            results = self.identifier.identify_services(target, scan_type)

            if results['status'] == 'success':
                # Save discovered services
                saved_services = []
                for service_data in results['services']:
                    try:
                        service, created = Service.objects.update_or_create(
                            host=target,
                            port=service_data['port'],
                            protocol=service_data['protocol'],
                            defaults={
                                'name': service_data['service']['name'],
                                'product': service_data['service']['product'],
                                'version': service_data['service']['version'],
                                'extra_info': service_data['service'].get('extrainfo', ''),
                                'category': service_data['category'],
                                'risk_level': service_data['risk_level'],
                                'cpe': service_data['service'].get('cpe', [])
                            }
                        )
                        saved_services.append({
                            'id': service.id,
                            'port': service.port,
                            'name': service.name,
                            'version': service.version,
                            'risk_level': service.risk_level,
                            'status': 'created' if created else 'updated'
                        })
                    except Exception as e:
                        self.logger.error(f"Error saving service: {str(e)}")

                return JsonResponse({
                    'status': 'success',
                    'message': f"Service scan completed for {target}",
                    'target': target,
                    'total_services': len(saved_services),
                    'services': saved_services,
                    'scan_stats': results['scan_stats']
                })

            return JsonResponse({
                'status': 'error',
                'error': results.get('error', 'Unknown error'),
                'details': results.get('details', '')
            }, status=500)

        except Exception as e:
            self.logger.error(f"Service scan error: {str(e)}")
            return JsonResponse({
                'error': str(e)
            }, status=500)

    def get(self, request):
        try:
            target = request.GET.get('target')
            if not target:
                return JsonResponse({'error': 'Target parameter is required'}, status=400)
            
            # Get all services for target
            services = Service.objects.filter(host=target).values(
                'id', 'port', 'protocol', 'name', 'product',
                'version', 'category', 'risk_level', 'last_seen'
            )

            # Group by risk level
            risk_summary = {
                'HIGH': services.filter(risk_level='HIGH').count(),
                'MEDIUM': services.filter(risk_level='MEDIUM').count(),
                'LOW': services.filter(risk_level='LOW').count()
            }

            return JsonResponse({
                'status': 'success',
                'target': target,
                'services': list(services),
                'risk_summary': risk_summary,
                'total_services': len(services)
            })

        except Exception as e:
            self.logger.error(f"Error retrieving services: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

class ServiceListView(View):
    def get(self, request):
        try:
            category = request.GET.get('category')
            risk_level = request.GET.get('risk_level')
            
            query = Service.objects.all()
            if category:
                query = query.filter(category=category)
            if risk_level:
                query = query.filter(risk_level=risk_level)
                
            services = query.values('id', 'host', 'port', 'name', 
                                  'category', 'risk_level', 'last_seen')
            
            return JsonResponse({
                'status': 'success',
                'services': list(services)
            })
        except Exception as e:
            logger.error(f"Error listing services: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

class ServiceDetailView(View):
    def get(self, request, service_id):
        try:
            service = Service.objects.get(id=service_id)
            return JsonResponse({
                'status': 'success',
                'service': {
                    'id': service.id,
                    'host': service.host,
                    'port': service.port,
                    'protocol': service.protocol,
                    'name': service.name,
                    'product': service.product,
                    'version': service.version,
                    'category': service.category,
                    'risk_level': service.risk_level,
                    'extra_info': service.extra_info,
                    'cpe': service.cpe,
                    'last_seen': service.last_seen.isoformat()
                }
            })
        except Service.DoesNotExist:
            return JsonResponse({'error': 'Service not found'}, status=404)
        except Exception as e:
            logger.error(f"Error retrieving service details: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

class ScanStatisticsView(View):
    def get(self, request):
        try:
            # Get time range from request
            days = int(request.GET.get('days', 7))
            time_threshold = timezone.now() - timezone.timedelta(days=days)
            
            # Collect statistics
            stats = {
                'total_subdomains': Subdomain.objects.count(),
                'active_subdomains': Subdomain.objects.filter(is_active=True).count(),
                'total_services': Service.objects.count(),
                'services_by_risk': {
                    level: Service.objects.filter(risk_level=level).count()
                    for level, _ in Service.RISK_LEVELS
                },
                'services_by_category': {
                    category: Service.objects.filter(category=category).count()
                    for category, _ in Service.CATEGORIES
                },
                'recent_scans': {
                    'port_scans': PortScan.objects.filter(
                        scan_date__gte=time_threshold).count(),
                    'service_scans': Service.objects.filter(
                        scan_date__gte=time_threshold).count(),
                }
            }
            
            return JsonResponse({
                'status': 'success',
                'statistics': stats
            })
        except Exception as e:
            logger.error(f"Error generating statistics: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

class HostSummaryView(View):
    def get(self, request, host):
        try:
            # Collect host information
            summary = {
                'subdomains': list(Subdomain.objects.filter(
                    domain=host).values('subdomain', 'ip_address')),
                'services': list(Service.objects.filter(
                    host=host).values('port', 'name', 'risk_level')),
                'port_scans': list(PortScan.objects.filter(
                    host=host).values('port', 'state', 'service')),
                'risk_assessment': {
                    'high_risk_services': Service.objects.filter(
                        host=host, risk_level='HIGH').count(),
                    'open_ports': PortScan.objects.filter(
                        host=host, state='open').count(),
                }
            }
            
            return JsonResponse({
                'status': 'success',
                'host': host,
                'summary': summary
            })
            

        except Exception as e:
            logger.error(f"Error generating host summary: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)
        
class ResultsView(View):
    def get(self, request):
        """Get all scan results for a target"""
        try:
            target = request.GET.get('target')
            if not target:
                return JsonResponse({'error': 'Target parameter is required'}, status=400)

            # Get subdomain results
            subdomains = Subdomain.objects.filter(domain=target).values(
                'subdomain', 'ip_address', 'discovered_date', 'is_active'
            )

            # Get port scan results
            ports = PortScan.objects.filter(host=target).values(
                'port', 'service', 'state', 'protocol', 'banner'
            )

            # Get service results
            services = Service.objects.filter(host=target).values(
                'port', 'name', 'product', 'version', 'category', 'risk_level'
            )

            return JsonResponse({
                'status': 'success',
                'target': target,
                'results': {
                    'subdomains': list(subdomains),
                    'ports': list(ports),
                    'services': list(services),
                    'summary': {
                        'total_subdomains': len(subdomains),
                        'open_ports': ports.filter(state='open').count(),
                        'high_risk_services': services.filter(risk_level='HIGH').count()
                    }
                }
            })

        except Exception as e:
            logger.error(f"Error retrieving results: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)"""
Django settings for security_automation project.

Generated by 'django-admin startproject' using Django 4.2.11.

For more information on this file, see
https://docs.djangoproject.com/en/4.2/topics/settings/

For the full list of settings and their values, see
https://docs.djangoproject.com/en/4.2/ref/settings/
"""

from pathlib import Path

# Build paths inside the project like this: BASE_DIR / 'subdir'.
BASE_DIR = Path(__file__).resolve().parent.parent


# Quick-start development settings - unsuitable for production
# See https://docs.djangoproject.com/en/4.2/howto/deployment/checklist/

# SECURITY WARNING: keep the secret key used in production secret!
SECRET_KEY = 'django-insecure-7u1-7x*e*z3&3d#1f1ktuo#zuu6f($58^=yuz=b4n7(n)6w4kl'

# SECURITY WARNING: don't run with debug turned on in production!
DEBUG = True

ALLOWED_HOSTS = ['localhost', '127.0.0.1']


# Application definition

INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'network_visualization',
    'automation',
    'reconnaissance.apps.ReconnaissanceConfig',
    'vulnerability.apps.VulnerabilityConfig',
    'reporting.apps.ReportingConfig',
    
]

# Update DATABASES in settings.py
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': BASE_DIR / 'db.sqlite3',
        'TIMEOUT': 20,  # Seconds
        'OPTIONS': {
            'timeout': 20,
            'check_same_thread': False,
        }
    }
}

MIDDLEWARE = [
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
    'automation.middleware.AutomationProcessorMiddleware'

]
# OpenVAS Configuration
# In security_automation/settings.py
# Update the OpenVAS configuration section





# Security Settings
SECURE_BROWSER_XSS_FILTER = True
SECURE_CONTENT_TYPE_NOSNIFF = True
X_FRAME_OPTIONS = 'DENY'
SECURE_HSTS_SECONDS = 31536000
SECURE_HSTS_INCLUDE_SUBDOMAINS = True
SECURE_HSTS_PRELOAD = True

# Session Security
SESSION_COOKIE_SECURE = True
CSRF_COOKIE_SECURE = True
SESSION_COOKIE_HTTPONLY = True
CSRF_COOKIE_HTTPONLY = True

# Rate Limiting
RATELIMIT_ENABLE = True
RATELIMIT_USE_CACHE = 'default'
RATELIMIT_DEFAULT_RATES = ['100/h']  # Default rate limit

# Logging Configuration
# Add this to your settings.py



# Logging Configuration

# Logging Configuration
import os
from pathlib import Path

# Build paths inside the project
BASE_DIR = Path(__file__).resolve().parent.parent

# Create logs directory
LOGS_DIR = os.path.join(BASE_DIR, 'logs')
os.makedirs(LOGS_DIR, exist_ok=True)

# ZAP Scanner Configuration
# These settings control the behavior of the OWASP ZAP integration
ZAP_SETTINGS = {
    'API_KEY': 'change_me_please',  # Change this in production
    'HOST': 'localhost',
    'PORT': 8080,
    'TIMEOUT': 300,  # 5 minutes timeout for ZAP operations
    'DEBUG': DEBUG,  # Tie ZAP debug mode to Django's debug setting
    'MAX_RETRIES': 3,  # Maximum number of connection retry attempts
    'RETRY_DELAY': 5,  # Seconds to wait between retries
}

# Logging Configuration
# This defines how the application handles different types of logs
LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'verbose': {
            'format': '[{asctime}] {levelname} {module} {process:d} {thread:d} {message}',
            'style': '{',
            'datefmt': '%Y-%m-%d %H:%M:%S'
        },
        'simple': {
            'format': '[{asctime}] {levelname} {message}',
            'style': '{',
            'datefmt': '%Y-%m-%d %H:%M:%S'
        },
    },
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',
            'formatter': 'simple',
            'level': 'DEBUG' if DEBUG else 'INFO',
        },
        'file': {
            'class': 'logging.FileHandler',
            'filename': os.path.join(LOGS_DIR, 'debug.log'),
            'formatter': 'verbose',
            'level': 'DEBUG',
        },
        'service_file': {
            'class': 'logging.FileHandler',
            'filename': os.path.join(LOGS_DIR, 'services.log'),
            'formatter': 'verbose',
            'level': 'INFO',
        },
        'error_file': {
            'class': 'logging.FileHandler',
            'filename': os.path.join(LOGS_DIR, 'error.log'),
            'formatter': 'verbose',
            'level': 'ERROR',
        },
        'security_file': {
            'class': 'logging.FileHandler',
            'filename': os.path.join(LOGS_DIR, 'security.log'),
            'formatter': 'verbose',
            'level': 'INFO',
        }
    },
    'loggers': {
        'django': {
            'handlers': ['console', 'file'],
            'level': 'INFO',
            'propagate': True,
        },
        'reconnaissance': {
            'handlers': ['console', 'service_file', 'error_file'],
            'level': 'INFO',
            'propagate': True,
        },
        'vulnerability': {
            'handlers': ['console', 'service_file', 'error_file', 'security_file'],
            'level': 'INFO',
            'propagate': True,
        },
        'zap': {
            'handlers': ['console', 'service_file', 'error_file', 'security_file'],
            'level': 'DEBUG' if DEBUG else 'INFO',
            'propagate': True,
        },
        'scanner': {
            'handlers': ['console', 'service_file', 'error_file', 'security_file'],
            'level': 'DEBUG' if DEBUG else 'INFO',
            'propagate': True,
        }
    },
}

ROOT_URLCONF = 'security_automation.urls'

import os

# Build paths inside the project
BASE_DIR = Path(__file__).resolve().parent.parent

TEMPLATES = [
    {
        'BACKEND': 'django.template.backends.django.DjangoTemplates',
        'DIRS': [os.path.join(BASE_DIR, 'templates')],  # Add this line
        'APP_DIRS': True,
        'OPTIONS': {
            'context_processors': [
                'django.template.context_processors.debug',
                'django.template.context_processors.request',
                'django.contrib.auth.context_processors.auth',
                'django.contrib.messages.context_processors.messages',
            ],
        },
    },
]

WSGI_APPLICATION = 'security_automation.wsgi.application'


# Database
# https://docs.djangoproject.com/en/4.2/ref/settings/#databases



# Password validation
# https://docs.djangoproject.com/en/4.2/ref/settings/#auth-password-validators

AUTH_PASSWORD_VALIDATORS = [
    {
        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',
    },
]


# Internationalization
# https://docs.djangoproject.com/en/4.2/topics/i18n/

LANGUAGE_CODE = 'en-us'

TIME_ZONE = 'UTC'

USE_I18N = True

USE_TZ = True

SECRET_KEY = 'p6IOStJ0i0azQfySy0mSCIht7VouYg9RGggv6iDgI4MQUSgADoTcy7SG2Z4kjDboM6Q'  # Remember to use environment variables in production
DEBUG = True 

# Static files (CSS, JavaScript, Images)
# https://docs.djangoproject.com/en/4.2/howto/static-files/

STATIC_URL = 'static/'

# Default primary key field type
# https://docs.djangoproject.com/en/4.2/ref/settings/#default-auto-field

DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'


# ZAP Settings
ZAP_SETTINGS = {
    'API_KEY': 'change_me_please',
    'HOST': 'localhost',
    'PORT': 8080,
    'TIMEOUT': 300,
    'MAX_RETRIES': 3,
    'SPIDER_TIMEOUT': 600,
    'ACTIVE_SCAN_TIMEOUT': 1200
}
# Nuclei Settings
NUCLEI_SETTINGS = {
    'TEMPLATES_DIR': 'nuclei-templates',
    'RESULTS_DIR': 'nuclei-results',
    'DEFAULT_SEVERITY': 'critical,high,medium',
    'RATE_LIMIT': 150,
    'TIMEOUT': 300,
    'GO_PATH': '/usr/local/go/bin',  # Adjust this based on your Go installation
    'BINARY_PATH': str(Path.home() / "go" / "bin" / "nuclei"),  # Default Go installation path
}

# Email Settings
# Email Settings
EMAIL_BACKEND = 'django.core.mail.backends.smtp.EmailBackend'
EMAIL_HOST = 'smtp.gmail.com'  # Changed from example.com to Gmail's SMTP server
EMAIL_PORT = 587
EMAIL_USE_TLS = True
EMAIL_HOST_USER = 'harmeeksingh729@gmail.com' 
EMAIL_HOST_PASSWORD = 'lyel vzou qbuh fvcl'  # App password or regular password
DEFAULT_FROM_EMAIL = 'harmeeksingh729@gmail.com'  # Should match EMAIL_HOST_USER

AUTOMATION_AUTOSTART = True  # Automatically start the processor on application startup
AUTOMATION_PROCESSING_INTERVAL = 60  # Seconds between processing cycles# Updated portion of security_automation/urls.py

from django.contrib import admin
from django.urls import path, include

urlpatterns = [
    path('admin/', admin.site.urls),
    path('recon/', include('reconnaissance.urls')),
    path('vulnerability/', include('vulnerability.urls')),
    path('reporting/', include('reporting.urls')),
    path('network/', include('network_visualization.urls')),
    path('automation/', include('automation.urls')),  # Add this line
]