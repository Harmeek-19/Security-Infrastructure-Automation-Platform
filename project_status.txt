"""
Django settings for security_automation project.

Generated by 'django-admin startproject' using Django 4.2.11.

For more information on this file, see
https://docs.djangoproject.com/en/4.2/topics/settings/

For the full list of settings and their values, see
https://docs.djangoproject.com/en/4.2/ref/settings/
"""

from pathlib import Path

# Build paths inside the project like this: BASE_DIR / 'subdir'.
BASE_DIR = Path(__file__).resolve().parent.parent


# Quick-start development settings - unsuitable for production
# See https://docs.djangoproject.com/en/4.2/howto/deployment/checklist/

# SECURITY WARNING: keep the secret key used in production secret!
SECRET_KEY = 'django-insecure-7u1-7x*e*z3&3d#1f1ktuo#zuu6f($58^=yuz=b4n7(n)6w4kl'

# SECURITY WARNING: don't run with debug turned on in production!
DEBUG = True

ALLOWED_HOSTS = ['localhost', '127.0.0.1']


# Application definition

INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'network_visualization',
    'automation',
    'exploit_manager', 
    'reconnaissance.apps.ReconnaissanceConfig',
    'vulnerability.apps.VulnerabilityConfig',
    'reporting.apps.ReportingConfig',
    'manual_exploitation.apps.ManualExploitationConfig', 
    
]

# Update DATABASES in settings.py
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': BASE_DIR / 'db.sqlite3',
        'TIMEOUT': 20,  # Seconds
        'OPTIONS': {
            'timeout': 20,
            'check_same_thread': False,
        }
    }
}

MIDDLEWARE = [
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
    'automation.middleware.AutomationProcessorMiddleware'

]
# OpenVAS Configuration
# In security_automation/settings.py
# Update the OpenVAS configuration section





# Security Settings
SECURE_BROWSER_XSS_FILTER = True
SECURE_CONTENT_TYPE_NOSNIFF = True
X_FRAME_OPTIONS = 'DENY'
SECURE_HSTS_SECONDS = 31536000
SECURE_HSTS_INCLUDE_SUBDOMAINS = True
SECURE_HSTS_PRELOAD = True

# Session Security
SESSION_COOKIE_SECURE = True
CSRF_COOKIE_SECURE = True
SESSION_COOKIE_HTTPONLY = True
CSRF_COOKIE_HTTPONLY = True

# Rate Limiting
RATELIMIT_ENABLE = True
RATELIMIT_USE_CACHE = 'default'
RATELIMIT_DEFAULT_RATES = ['100/h']  # Default rate limit

# Logging Configuration
# Add this to your settings.py



# Logging Configuration

# Logging Configuration
import os
from pathlib import Path

# Build paths inside the project
BASE_DIR = Path(__file__).resolve().parent.parent

# Create logs directory
LOGS_DIR = os.path.join(BASE_DIR, 'logs')
os.makedirs(LOGS_DIR, exist_ok=True)

# ZAP Scanner Configuration
# These settings control the behavior of the OWASP ZAP integration
ZAP_SETTINGS = {
    'API_KEY': 'change_me_please',  # Change this in production
    'HOST': 'localhost',
    'PORT': 8080,
    'TIMEOUT': 300,  # 5 minutes timeout for ZAP operations
    'DEBUG': DEBUG,  # Tie ZAP debug mode to Django's debug setting
    'MAX_RETRIES': 3,  # Maximum number of connection retry attempts
    'RETRY_DELAY': 5,  # Seconds to wait between retries
}

# Logging Configuration
# This defines how the application handles different types of logs
LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'verbose': {
            'format': '[{asctime}] {levelname} {module} {process:d} {thread:d} {message}',
            'style': '{',
            'datefmt': '%Y-%m-%d %H:%M:%S'
        },
        'simple': {
            'format': '[{asctime}] {levelname} {message}',
            'style': '{',
            'datefmt': '%Y-%m-%d %H:%M:%S'
        },
    },
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',
            'formatter': 'simple',
            'level': 'DEBUG' if DEBUG else 'INFO',
        },
        'file': {
            'class': 'logging.FileHandler',
            'filename': os.path.join(LOGS_DIR, 'debug.log'),
            'formatter': 'verbose',
            'level': 'DEBUG',
        },
        'service_file': {
            'class': 'logging.FileHandler',
            'filename': os.path.join(LOGS_DIR, 'services.log'),
            'formatter': 'verbose',
            'level': 'INFO',
        },
        'error_file': {
            'class': 'logging.FileHandler',
            'filename': os.path.join(LOGS_DIR, 'error.log'),
            'formatter': 'verbose',
            'level': 'ERROR',
        },
        'security_file': {
            'class': 'logging.FileHandler',
            'filename': os.path.join(LOGS_DIR, 'security.log'),
            'formatter': 'verbose',
            'level': 'INFO',
        }
    },
    'loggers': {
        'django': {
            'handlers': ['console', 'file'],
            'level': 'INFO',
            'propagate': True,
        },
        'reconnaissance': {
            'handlers': ['console', 'service_file', 'error_file'],
            'level': 'INFO',
            'propagate': True,
        },
        'vulnerability': {
            'handlers': ['console', 'service_file', 'error_file', 'security_file'],
            'level': 'INFO',
            'propagate': True,
        },
        'zap': {
            'handlers': ['console', 'service_file', 'error_file', 'security_file'],
            'level': 'DEBUG' if DEBUG else 'INFO',
            'propagate': True,
        },
        'scanner': {
            'handlers': ['console', 'service_file', 'error_file', 'security_file'],
            'level': 'DEBUG' if DEBUG else 'INFO',
            'propagate': True,
        }
    },
}

ROOT_URLCONF = 'security_automation.urls'

import os

# Build paths inside the project
BASE_DIR = Path(__file__).resolve().parent.parent

TEMPLATES = [
    {
        'BACKEND': 'django.template.backends.django.DjangoTemplates',
        'DIRS': [os.path.join(BASE_DIR, 'templates')],  # Add this line
        'APP_DIRS': True,
        'OPTIONS': {
            'context_processors': [
                'django.template.context_processors.debug',
                'django.template.context_processors.request',
                'django.contrib.auth.context_processors.auth',
                'django.contrib.messages.context_processors.messages',
            ],
        },
    },
]

WSGI_APPLICATION = 'security_automation.wsgi.application'


# Database
# https://docs.djangoproject.com/en/4.2/ref/settings/#databases



# Password validation
# https://docs.djangoproject.com/en/4.2/ref/settings/#auth-password-validators

AUTH_PASSWORD_VALIDATORS = [
    {
        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',
    },
]


# Internationalization
# https://docs.djangoproject.com/en/4.2/topics/i18n/

LANGUAGE_CODE = 'en-us'

TIME_ZONE = 'UTC'

USE_I18N = True

USE_TZ = True

SECRET_KEY = 'p6IOStJ0i0azQfySy0mSCIht7VouYg9RGggv6iDgI4MQUSgADoTcy7SG2Z4kjDboM6Q'  # Remember to use environment variables in production
DEBUG = True 

# Static files (CSS, JavaScript, Images)
# https://docs.djangoproject.com/en/4.2/howto/static-files/

STATIC_URL = 'static/'

# Default primary key field type
# https://docs.djangoproject.com/en/4.2/ref/settings/#default-auto-field

DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'


# ZAP Settings
ZAP_SETTINGS = {
    'API_KEY': 'change_me_please',
    'HOST': 'localhost',
    'PORT': 8080,
    'TIMEOUT': 300,
    'MAX_RETRIES': 3,
    'SPIDER_TIMEOUT': 600,
    'ACTIVE_SCAN_TIMEOUT': 1200
}
# Nuclei Settings
NUCLEI_SETTINGS = {
    'TEMPLATES_DIR': 'nuclei-templates',
    'RESULTS_DIR': 'nuclei-results',
    'DEFAULT_SEVERITY': 'critical,high,medium',
    'RATE_LIMIT': 150,
    'TIMEOUT': 300,
    'GO_PATH': '/usr/local/go/bin',  # Adjust this based on your Go installation
    'BINARY_PATH': str(Path.home() / "go" / "bin" / "nuclei"),  # Default Go installation path
}

# Email Settings
# Email Settings
EMAIL_BACKEND = 'django.core.mail.backends.smtp.EmailBackend'
EMAIL_HOST = 'smtp.gmail.com'  # Changed from example.com to Gmail's SMTP server
EMAIL_PORT = 587
EMAIL_USE_TLS = True
EMAIL_HOST_USER = 'harmeeksingh729@gmail.com' 
EMAIL_HOST_PASSWORD = 'lyel vzou qbuh fvcl'  # App password or regular password
DEFAULT_FROM_EMAIL = 'harmeeksingh729@gmail.com'  # Should match EMAIL_HOST_USER

AUTOMATION_AUTOSTART = True  # Automatically start the processor on application startup
AUTOMATION_PROCESSING_INTERVAL = 60  # Seconds between processing cycles# File: security_automation/urls.py
from django.contrib import admin
from django.urls import path, include

urlpatterns = [
    path('admin/', admin.site.urls),
    path('recon/', include('reconnaissance.urls')),
    path('vulnerability/', include('vulnerability.urls')),
    path('reporting/', include('reporting.urls')),
    path('network/', include('network_visualization.urls')),
    path('automation/', include('automation.urls')),
    path('exploits/', include('exploit_manager.urls')),
    path('exploitation/', include('manual_exploitation.urls')),# Add this line
]from django.db import models

from django.db import models

class Subdomain(models.Model):
    domain = models.CharField(max_length=255)
    subdomain = models.CharField(max_length=255)
    ip_address = models.GenericIPAddressField(null=True)
    discovered_date = models.DateTimeField(auto_now_add=True)
    is_active = models.BooleanField(default=True)

    class Meta:
        ordering = ['-discovered_date']
        unique_together = ['domain', 'subdomain']
        indexes = [
            models.Index(fields=['domain']),
            models.Index(fields=['subdomain']),
        ]

    def __str__(self):
        return self.subdomain

    def save(self, *args, **kwargs):
        # Update instead of error on duplicate
        try:
            super().save(*args, **kwargs)
        except:
            existing = Subdomain.objects.get(domain=self.domain, subdomain=self.subdomain)
            existing.ip_address = self.ip_address
            existing.is_active = self.is_active
            existing.save()

class Service(models.Model):
    RISK_LEVELS = [
        ('LOW', 'Low'),
        ('MEDIUM', 'Medium'),
        ('HIGH', 'High'),
    ]

    CATEGORIES = [
        ('web', 'Web Services'),
        ('database', 'Database Services'),
        ('mail', 'Mail Services'),
        ('file_transfer', 'File Transfer'),
        ('remote_access', 'Remote Access'),
        ('domain_services', 'Domain Services'),
        ('monitoring', 'Monitoring'),
        ('security', 'Security Services'),
        ('other', 'Other'),
    ]

    PROTOCOLS = [
        ('tcp', 'TCP'),
        ('udp', 'UDP'),
        ('sctp', 'SCTP'),
    ]

    host = models.CharField(max_length=255)
    port = models.IntegerField()
    protocol = models.CharField(max_length=10, choices=PROTOCOLS, default='tcp')
    name = models.CharField(max_length=100)
    product = models.CharField(max_length=100, blank=True)
    version = models.CharField(max_length=100, blank=True)
    extra_info = models.TextField(blank=True)
    category = models.CharField(max_length=50, choices=CATEGORIES)
    risk_level = models.CharField(max_length=10, choices=RISK_LEVELS)
    cpe = models.JSONField(default=list)
    scan_date = models.DateTimeField(auto_now_add=True)
    last_seen = models.DateTimeField(auto_now=True)
    is_active = models.BooleanField(default=True)

    class Meta:
        unique_together = ('host', 'port', 'protocol')
        ordering = ['-scan_date']
        indexes = [
            models.Index(fields=['host', 'port']),
            models.Index(fields=['category']),
            models.Index(fields=['risk_level']),
        ]

    def __str__(self):
        return f"{self.host}:{self.port} - {self.name}"

class PortScan(models.Model):
    STATES = [
        ('pending', 'Pending'),
        ('in_progress', 'In Progress'), 
        ('open', 'Open'),
        ('closed', 'Closed'),
        ('filtered', 'Filtered'),
        ('unfiltered', 'Unfiltered'),
        ('error', 'Error'),
        ('completed', 'Completed')
    ]

    SCAN_STATUS = [
        ('pending', 'Pending'),
        ('in_progress', 'In Progress'),
        ('completed', 'Completed'),
        ('error', 'Error')
    ]

    host = models.CharField(max_length=255)
    port = models.IntegerField()
    service = models.CharField(max_length=100)
    state = models.CharField(max_length=50, choices=STATES)
    scan_status = models.CharField(max_length=50, choices=SCAN_STATUS, default='pending')
    scan_date = models.DateTimeField(auto_now_add=True)
    protocol = models.CharField(max_length=10, choices=Service.PROTOCOLS, default='tcp')
    banner = models.TextField(blank=True)
    notes = models.TextField(blank=True)
    scan_type = models.CharField(max_length=50, default='quick')
    error_message = models.TextField(blank=True)

    class Meta:
        ordering = ['-scan_date']
        indexes = [
            models.Index(fields=['host', 'port']),
            models.Index(fields=['state']),
            models.Index(fields=['scan_date']),
            models.Index(fields=['scan_status'])
        ]

    def __str__(self):
        return f"{self.host}:{self.port} - {self.state}"
class SystemLogEntry(models.Model):
    """Model for system logs admin interface"""
    class Meta:
        managed = False
        verbose_name_plural = 'System Logs'
        default_permissions = ('view',)from django.http import JsonResponse
from django.views import View
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from django.db.models import Count, Q
from django.utils import timezone
from .models import Service, Subdomain, PortScan
from .service_identifier import ServiceIdentifier
import json
import logging
from .subdomain_enumerator import SubdomainEnumerator
from .scanner import PortScanner, ScanType
logger = logging.getLogger(__name__)

@method_decorator(csrf_exempt, name='dispatch')
class SubdomainScanView(View):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.enumerator = SubdomainEnumerator()
        
    def get(self, request):
        try:
            target = request.GET.get('target')
            if not target:
                return JsonResponse({'error': 'Target parameter is required'}, status=400)
                
            subdomains = Subdomain.objects.filter(domain=target).values(
                'subdomain', 'ip_address', 'discovered_date', 'is_active'
            )
            
            return JsonResponse({
                'status': 'success',
                'target': target,
                'subdomains': list(subdomains)
            })
        except Exception as e:
            logger.error(f"Error retrieving subdomains: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

    def post(self, request):
        try:
            data = json.loads(request.body)
            domain = data.get('domain')
            
            if not domain:
                return JsonResponse({'error': 'Domain is required'}, status=400)
            
            # Perform actual subdomain enumeration
            discovered = self.enumerator.enumerate_subdomains(domain)
            
            # Save results to database
            saved_subdomains = []
            for subdomain_data in discovered:
                subdomain, created = Subdomain.objects.update_or_create(
                    domain=domain,
                    subdomain=subdomain_data['subdomain'],
                    defaults={
                        'ip_address': subdomain_data['ip_address'],
                        'is_active': True
                    }
                )
                saved_subdomains.append({
                    'id': subdomain.id,
                    'subdomain': subdomain.subdomain,
                    'ip_address': subdomain.ip_address,
                    'status': 'created' if created else 'updated'
                })
            
            return JsonResponse({
                'status': 'success',
                'message': 'Subdomain scan completed',
                'domain': domain,
                'total_subdomains': len(saved_subdomains),
                'subdomains': saved_subdomains
            })
        except Exception as e:
            logger.error(f"Subdomain scan error: {str(e)}")
            return JsonResponse({
                'error': str(e)
            }, status=500)

class SubdomainListView(View):
    def get(self, request):
        try:
            domain = request.GET.get('domain')
            query = Subdomain.objects.all()
            
            if domain:
                query = query.filter(domain=domain)
            
            subdomains = query.values('id', 'domain', 'subdomain', 
                                    'ip_address', 'discovered_date')
            
            return JsonResponse({
                'status': 'success',
                'subdomains': list(subdomains)
            })
        except Exception as e:
            logger.error(f"Error listing subdomains: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

class SubdomainDetailView(View):
    def get(self, request, subdomain_id):
        try:
            subdomain = Subdomain.objects.get(id=subdomain_id)
            return JsonResponse({
                'status': 'success',
                'subdomain': {
                    'id': subdomain.id,
                    'domain': subdomain.domain,
                    'subdomain': subdomain.subdomain,
                    'ip_address': subdomain.ip_address,
                    'discovered_date': subdomain.discovered_date.isoformat(),
                    'is_active': subdomain.is_active
                }
            })
        except Subdomain.DoesNotExist:
            return JsonResponse({'error': 'Subdomain not found'}, status=404)
        except Exception as e:
            logger.error(f"Error retrieving subdomain details: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

@method_decorator(csrf_exempt, name='dispatch')
class PortScanView(View):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.scanner = PortScanner()
        self.logger = logging.getLogger(__name__)

    def post(self, request):
        try:
            data = json.loads(request.body)
            target = data.get('target')
            scan_type = data.get('scan_type', 'quick')
            custom_ports = data.get('ports')

            if not target:
                return JsonResponse({'error': 'Target is required'}, status=400)

            if scan_type not in ScanType.__members__ and not custom_ports:
                return JsonResponse({
                    'error': f'Invalid scan type. Available types: {", ".join(ScanType.__members__.keys())}'
                }, status=400)

            # Start the scan
            self.logger.info(f"Starting {scan_type} port scan for {target}")
            scan_result = self.scanner.scan(target, scan_type)

            if scan_result['status'] == 'success':
                saved_ports = []
                
                # Process and save results
                for host in scan_result['results']:
                    for port_data in host['ports']:
                        scan = PortScan.objects.create(
                            host=target,
                            port=port_data['port'],
                            service=port_data['service'],
                            state=port_data['state'],
                            protocol='tcp',
                            scan_status='completed',
                            scan_type=scan_type,
                            banner=port_data.get('extrainfo', ''),
                            notes=f"Version: {port_data.get('version', 'unknown')}"
                        )
                        saved_ports.append({
                            'port': scan.port,
                            'state': scan.state,
                            'service': scan.service
                        })

                return JsonResponse({
                    'status': 'success',
                    'message': f'Port scan completed for {target}',
                    'target': target,
                    'scan_type': scan_type,
                    'total_ports': len(saved_ports),
                    'open_ports': len([p for p in saved_ports if p['state'] == 'open']),
                    'ports': saved_ports,
                    'scan_time': scan_result['scan_info']['scan_time']
                })
            else:
                return JsonResponse({
                    'status': 'error',
                    'error': scan_result.get('error', 'Unknown error during scan')
                }, status=500)

        except json.JSONDecodeError:
            return JsonResponse({'error': 'Invalid JSON data'}, status=400)
        except Exception as e:
            self.logger.error(f"Port scan error: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

    def get(self, request):
        """Get scan results for a target"""
        try:
            target = request.GET.get('target')
            if not target:
                return JsonResponse({'error': 'Target parameter is required'}, status=400)

            scans = PortScan.objects.filter(
                host=target, 
                scan_status='completed'
            ).values(
                'port', 'service', 'state', 'protocol', 
                'banner', 'scan_date'
            ).order_by('port')

            return JsonResponse({
                'status': 'success',
                'target': target,
                'total_ports': len(scans),
                'open_ports': scans.filter(state='open').count(),
                'ports': list(scans)
            })

        except Exception as e:
            self.logger.error(f"Error retrieving port scan results: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

    def _estimate_scan_time(self, scan_type, ports):
        """Estimate scan time based on scan type and ports"""
        if scan_type == 'quick':
            return "1-2 minutes"
        elif scan_type == 'partial':
            return "5-10 minutes"
        elif scan_type == 'complete':
            return "30-60 minutes"
        elif scan_type == 'full':
            return "1-2 hours"
        else:
            # Custom port range
            port_count = len(ports.split(','))
            if port_count < 100:
                return "1-5 minutes"
            elif port_count < 1000:
                return "5-15 minutes"
            else:
                return "15+ minutes"

class PortScanListView(View):
    def get(self, request):
        try:
            host = request.GET.get('host')
            state = request.GET.get('state')
            
            query = PortScan.objects.all()
            if host:
                query = query.filter(host=host)
            if state:
                query = query.filter(state=state)
                
            scans = query.values('id', 'host', 'port', 'service', 
                               'state', 'protocol', 'scan_date')
            
            return JsonResponse({
                'status': 'success',
                'scans': list(scans)
            })
        except Exception as e:
            logger.error(f"Error listing port scans: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

class PortScanDetailView(View):
    def get(self, request, scan_id):
        try:
            scan = PortScan.objects.get(id=scan_id)
            return JsonResponse({
                'status': 'success',
                'scan': {
                    'id': scan.id,
                    'host': scan.host,
                    'port': scan.port,
                    'service': scan.service,
                    'state': scan.state,
                    'protocol': scan.protocol,
                    'scan_date': scan.scan_date.isoformat(),
                    'banner': scan.banner,
                    'notes': scan.notes
                }
            })
        except PortScan.DoesNotExist:
            return JsonResponse({'error': 'Scan not found'}, status=404)
        except Exception as e:
            logger.error(f"Error retrieving scan details: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

@method_decorator(csrf_exempt, name='dispatch')
class ServiceScanView(View):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.identifier = ServiceIdentifier()
        self.logger = logging.getLogger(__name__)

    def post(self, request):
        try:
            data = json.loads(request.body)
            target = data.get('target')
            scan_type = data.get('scan_type', 'standard')

            if not target:
                return JsonResponse({'error': 'Target is required'}, status=400)

            # Start service identification
            self.logger.info(f"Starting {scan_type} service scan for {target}")
            results = self.identifier.identify_services(target, scan_type)

            if results['status'] == 'success':
                # Save discovered services
                saved_services = []
                for service_data in results['services']:
                    try:
                        service, created = Service.objects.update_or_create(
                            host=target,
                            port=service_data['port'],
                            protocol=service_data['protocol'],
                            defaults={
                                'name': service_data['service']['name'],
                                'product': service_data['service']['product'],
                                'version': service_data['service']['version'],
                                'extra_info': service_data['service'].get('extrainfo', ''),
                                'category': service_data['category'],
                                'risk_level': service_data['risk_level'],
                                'cpe': service_data['service'].get('cpe', [])
                            }
                        )
                        saved_services.append({
                            'id': service.id,
                            'port': service.port,
                            'name': service.name,
                            'version': service.version,
                            'risk_level': service.risk_level,
                            'status': 'created' if created else 'updated'
                        })
                    except Exception as e:
                        self.logger.error(f"Error saving service: {str(e)}")

                return JsonResponse({
                    'status': 'success',
                    'message': f"Service scan completed for {target}",
                    'target': target,
                    'total_services': len(saved_services),
                    'services': saved_services,
                    'scan_stats': results['scan_stats']
                })

            return JsonResponse({
                'status': 'error',
                'error': results.get('error', 'Unknown error'),
                'details': results.get('details', '')
            }, status=500)

        except Exception as e:
            self.logger.error(f"Service scan error: {str(e)}")
            return JsonResponse({
                'error': str(e)
            }, status=500)

    def get(self, request):
        try:
            target = request.GET.get('target')
            if not target:
                return JsonResponse({'error': 'Target parameter is required'}, status=400)
            
            # Get all services for target
            services = Service.objects.filter(host=target).values(
                'id', 'port', 'protocol', 'name', 'product',
                'version', 'category', 'risk_level', 'last_seen'
            )

            # Group by risk level
            risk_summary = {
                'HIGH': services.filter(risk_level='HIGH').count(),
                'MEDIUM': services.filter(risk_level='MEDIUM').count(),
                'LOW': services.filter(risk_level='LOW').count()
            }

            return JsonResponse({
                'status': 'success',
                'target': target,
                'services': list(services),
                'risk_summary': risk_summary,
                'total_services': len(services)
            })

        except Exception as e:
            self.logger.error(f"Error retrieving services: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

class ServiceListView(View):
    def get(self, request):
        try:
            category = request.GET.get('category')
            risk_level = request.GET.get('risk_level')
            
            query = Service.objects.all()
            if category:
                query = query.filter(category=category)
            if risk_level:
                query = query.filter(risk_level=risk_level)
                
            services = query.values('id', 'host', 'port', 'name', 
                                  'category', 'risk_level', 'last_seen')
            
            return JsonResponse({
                'status': 'success',
                'services': list(services)
            })
        except Exception as e:
            logger.error(f"Error listing services: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

class ServiceDetailView(View):
    def get(self, request, service_id):
        try:
            service = Service.objects.get(id=service_id)
            return JsonResponse({
                'status': 'success',
                'service': {
                    'id': service.id,
                    'host': service.host,
                    'port': service.port,
                    'protocol': service.protocol,
                    'name': service.name,
                    'product': service.product,
                    'version': service.version,
                    'category': service.category,
                    'risk_level': service.risk_level,
                    'extra_info': service.extra_info,
                    'cpe': service.cpe,
                    'last_seen': service.last_seen.isoformat()
                }
            })
        except Service.DoesNotExist:
            return JsonResponse({'error': 'Service not found'}, status=404)
        except Exception as e:
            logger.error(f"Error retrieving service details: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

class ScanStatisticsView(View):
    def get(self, request):
        try:
            # Get time range from request
            days = int(request.GET.get('days', 7))
            time_threshold = timezone.now() - timezone.timedelta(days=days)
            
            # Collect statistics
            stats = {
                'total_subdomains': Subdomain.objects.count(),
                'active_subdomains': Subdomain.objects.filter(is_active=True).count(),
                'total_services': Service.objects.count(),
                'services_by_risk': {
                    level: Service.objects.filter(risk_level=level).count()
                    for level, _ in Service.RISK_LEVELS
                },
                'services_by_category': {
                    category: Service.objects.filter(category=category).count()
                    for category, _ in Service.CATEGORIES
                },
                'recent_scans': {
                    'port_scans': PortScan.objects.filter(
                        scan_date__gte=time_threshold).count(),
                    'service_scans': Service.objects.filter(
                        scan_date__gte=time_threshold).count(),
                }
            }
            
            return JsonResponse({
                'status': 'success',
                'statistics': stats
            })
        except Exception as e:
            logger.error(f"Error generating statistics: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)

class HostSummaryView(View):
    def get(self, request, host):
        try:
            # Collect host information
            summary = {
                'subdomains': list(Subdomain.objects.filter(
                    domain=host).values('subdomain', 'ip_address')),
                'services': list(Service.objects.filter(
                    host=host).values('port', 'name', 'risk_level')),
                'port_scans': list(PortScan.objects.filter(
                    host=host).values('port', 'state', 'service')),
                'risk_assessment': {
                    'high_risk_services': Service.objects.filter(
                        host=host, risk_level='HIGH').count(),
                    'open_ports': PortScan.objects.filter(
                        host=host, state='open').count(),
                }
            }
            
            return JsonResponse({
                'status': 'success',
                'host': host,
                'summary': summary
            })
            

        except Exception as e:
            logger.error(f"Error generating host summary: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)
        
class ResultsView(View):
    def get(self, request):
        """Get all scan results for a target"""
        try:
            target = request.GET.get('target')
            if not target:
                return JsonResponse({'error': 'Target parameter is required'}, status=400)

            # Get subdomain results
            subdomains = Subdomain.objects.filter(domain=target).values(
                'subdomain', 'ip_address', 'discovered_date', 'is_active'
            )

            # Get port scan results
            ports = PortScan.objects.filter(host=target).values(
                'port', 'service', 'state', 'protocol', 'banner'
            )

            # Get service results
            services = Service.objects.filter(host=target).values(
                'port', 'name', 'product', 'version', 'category', 'risk_level'
            )

            return JsonResponse({
                'status': 'success',
                'target': target,
                'results': {
                    'subdomains': list(subdomains),
                    'ports': list(ports),
                    'services': list(services),
                    'summary': {
                        'total_subdomains': len(subdomains),
                        'open_ports': ports.filter(state='open').count(),
                        'high_risk_services': services.filter(risk_level='HIGH').count()
                    }
                }
            })

        except Exception as e:
            logger.error(f"Error retrieving results: {str(e)}")
            return JsonResponse({'error': str(e)}, status=500)import dns.resolver
import dns.zone
import socket
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
from typing import List, Dict
import requests
import re

class SubdomainEnumerator:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.resolver = dns.resolver.Resolver()
        self.resolver.timeout = 1
        self.resolver.lifetime = 1
        
        # Common subdomain prefixes
        self.common_subdomains = [
            'www', 'mail', 'ftp', 'smtp', 'pop', 'ns1', 'ns2', 'dns1', 'dns2',
            'webmail', 'admin', 'secure', 'vpn', 'remote', 'test', 'dev', 'host',
            'support', 'api', 'dev', 'staging', 'app', 'portal', 'beta'
        ]

# File: reconnaissance/subdomain_enumerator.py
# Updates needed in enumerate_subdomains method

    def enumerate_subdomains(self, target: str) -> List[Dict]:
        """Main subdomain enumeration method combining multiple techniques"""
        # Clean target - remove protocol and path to get just the domain
        domain = self._extract_domain(target)
        if not domain:
            self.logger.error(f"Invalid domain provided: {target}")
            return []
            
        self.logger.info(f"Starting subdomain enumeration for domain: {domain}")
        
        # Add a basic check to ensure the domain is valid
        try:
            socket.gethostbyname(domain)
        except socket.gaierror:
            # Add the domain itself as a subdomain if we can't resolve it
            # This allows the workflow to continue
            self.logger.warning(f"Domain {domain} could not be resolved, but continuing enumeration")
        
        discovered_subdomains = set()
        results = []

        # Always add the domain itself to results
        try:
            main_domain_ip = socket.gethostbyname(domain)
            discovered_subdomains.add(domain)
            results.append({
                'subdomain': domain,
                'ip_address': main_domain_ip,
                'is_http': True,  # Assume main domain has HTTP
                'http_status': None,
                'status': 'active'
            })
        except Exception as e:
            self.logger.warning(f"Couldn't resolve main domain {domain}: {str(e)}")
        
        # 1. DNS enumeration
        dns_results = self._dns_enumeration(domain)
        for subdomain in dns_results:
            discovered_subdomains.add(subdomain)

        # 2. Brute force common subdomains
        brute_results = self._brute_force_subdomains(domain)
        for subdomain in brute_results:
            discovered_subdomains.add(subdomain)

        # Process and validate all discovered subdomains
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_subdomain = {
                executor.submit(self._validate_subdomain, subdomain): subdomain 
                for subdomain in discovered_subdomains
            }
            
            for future in as_completed(future_to_subdomain):
                subdomain = future_to_subdomain[future]
                try:
                    result = future.result()
                    if result:
                        results.append(result)
                except Exception as e:
                    self.logger.error(f"Error validating {subdomain}: {str(e)}")

        # Ensure we have at least the main domain in results
        if not results and domain:
            results.append({
                'subdomain': domain,
                'ip_address': None,
                'is_http': None,
                'http_status': None,
                'status': 'unknown'
            })

        return results

    def _extract_domain(self, url: str) -> str:
        """Extract root domain from a URL or domain string"""
        # Remove protocol if present
        if '://' in url:
            url = url.split('://', 1)[1]
            
        # Remove path, query params, and fragment
        url = url.split('/', 1)[0]
        url = url.split('?', 1)[0]
        url = url.split('#', 1)[0]
        
        # Remove port if present
        url = url.split(':', 1)[0]
        
        # Validate domain format
        domain_pattern = r'^([a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]{2,}$'
        if re.match(domain_pattern, url):
            return url
        
        return None

    def _dns_enumeration(self, domain: str) -> set:
        """Enumerate subdomains using DNS queries"""
        discovered = set()
        
        try:
            # Try zone transfer first
            try:
                ns_records = self.resolver.resolve(domain, 'NS')
                for ns in ns_records:
                    try:
                        zone = dns.zone.from_xfr(dns.query.xfr(str(ns), domain))
                        for name, _ in zone.nodes.items():
                            subdomain = str(name) + '.' + domain
                            discovered.add(subdomain)
                    except Exception as zone_error:
                        # Zone transfers often fail due to security restrictions, this is expected
                        continue
            except dns.resolver.NoAnswer:
                self.logger.info(f"No NS records found for {domain} - this is normal for many domains")
            except dns.resolver.NXDOMAIN:
                self.logger.info(f"Domain {domain} does not exist in DNS")
            except Exception as e:
                self.logger.info(f"NS record query failed for {domain}: {str(e)}")

            # Try to get common DNS records
            for record_type in ['A', 'AAAA', 'CNAME', 'MX', 'NS', 'TXT']:
                try:
                    answers = self.resolver.resolve(domain, record_type)
                    for rdata in answers:
                        if record_type == 'MX':
                            discovered.add(str(rdata.exchange).rstrip('.'))
                        elif record_type == 'NS':
                            discovered.add(str(rdata).rstrip('.'))
                        elif record_type == 'CNAME':
                            discovered.add(str(rdata.target).rstrip('.'))
                except dns.resolver.NoAnswer:
                    # This is normal - not all record types exist for all domains
                    continue
                except Exception:
                    # Other DNS errors are also common and shouldn't stop enumeration
                    continue

        except Exception as e:
            # Only log as warning since this is one of multiple enumeration techniques
            self.logger.warning(f"DNS enumeration had issues for {domain}: {str(e)}")

        return discovered

    def _brute_force_subdomains(self, domain: str) -> set:
        """Brute force subdomains using common prefixes"""
        discovered = set()
        
        with ThreadPoolExecutor(max_workers=20) as executor:
            future_to_subdomain = {
                executor.submit(self._check_subdomain, f"{prefix}.{domain}"): prefix 
                for prefix in self.common_subdomains
            }
            
            for future in as_completed(future_to_subdomain):
                try:
                    result = future.result()
                    if result:
                        discovered.add(result)
                except Exception as e:
                    continue

        return discovered

    def _check_subdomain(self, subdomain: str) -> str:
        """Check if a subdomain exists"""
        try:
            self.resolver.resolve(subdomain, 'A')
            return subdomain
        except:
            return None

    def _validate_subdomain(self, subdomain: str) -> Dict:
        """Validate and get information about a subdomain"""
        try:
            ip_address = socket.gethostbyname(subdomain)
            
            # Basic HTTP check
            is_http = False
            http_status = None
            try:
                response = requests.get(f"http://{subdomain}", timeout=3)
                is_http = True
                http_status = response.status_code
            except:
                try:
                    response = requests.get(f"https://{subdomain}", timeout=3)
                    is_http = True
                    http_status = response.status_code
                except:
                    pass

            return {
                'subdomain': subdomain,
                'ip_address': ip_address,
                'is_http': is_http,
                'http_status': http_status,
                'status': 'active'
            }
        except Exception as e:
            return Noneimport nmap
from typing import Dict, Any, List
from enum import Enum
import os
import logging
import time
import threading
import socket

logger = logging.getLogger(__name__)

class ScanType(Enum):
    QUICK = "quick"       # Fast scan of most common ports
    PARTIAL = "partial"   # Standard scan with version detection
    COMPLETE = "complete" # Comprehensive scan of all ports with version detection
    FULL = "full"        # Intensive scan with all possible features

class PortScanner:
    def __init__(self):
        self.scanner = nmap.PortScanner()
        # Check if running as root
        self.is_root = os.geteuid() == 0 if hasattr(os, 'geteuid') else False
        # Set default timeout
        self.default_timeout = 300  # 5 minutes default
        # Logger for class instance
        self.logger = logging.getLogger(__name__)
        
    def get_scan_config(self, scan_type: str) -> Dict[str, str]:
        # Base configurations without OS detection
        base_configs = {
            ScanType.QUICK.value: {
                'ports': '21-23,25,80,443,3306,8080',
                'arguments': '-sV -T4 --version-intensity 0',  # Fast scan
                'timeout': 120  # 2 minutes timeout
            },
            ScanType.PARTIAL.value: {
                'ports': '1-1000',
                'arguments': '-sV -T4 -sC --version-intensity 5',  # Standard scan
                'timeout': 300  # 5 minutes timeout
            },
            ScanType.COMPLETE.value: {
                'ports': '1-65535',
                'arguments': '-sV -T4 -sC --version-intensity 7',  # All ports
                'timeout': 600  # 10 minutes timeout
            },
            ScanType.FULL.value: {
                'ports': '1-10000',  # Reduced port range for full scan
                'arguments': '-sV -T4 --version-intensity 9',  # Simplified for full scan
                'timeout': 900  # 15 minutes timeout
            }
        }
        
        # Add OS detection flags only if running as root
        if self.is_root:
            base_configs[ScanType.COMPLETE.value]['arguments'] += ' -O'
            base_configs[ScanType.FULL.value]['arguments'] += ' -O -A'
        
        config = base_configs.get(scan_type, base_configs[ScanType.QUICK.value])
        logger.info(f"Scan configuration for {scan_type}: {config}")
        return config

    def scan(self, target: str, scan_type: str = "quick") -> Dict[str, Any]:
        try:
            config = self.get_scan_config(scan_type)
            logger.info(f"Starting {scan_type} scan for {target} with args: {config['arguments']}")
            
            # Always do a manual port check first - this is more reliable
            responsive_ports = self._check_responsive_ports(target)
            
            # If we found ports in our manual check, create a manual result immediately
            if responsive_ports:
                logger.info(f"Manual check found open ports on {target}: {responsive_ports}")
                manual_result = self._create_manual_result(target, responsive_ports)
                
                # Only try nmap if it's not a full scan (which seems problematic)
                if scan_type != 'full':
                    try:
                        # Still run nmap for better service detection
                        scan_result = self.scanner.scan(target, config['ports'], config['arguments'])
                        
                        # Check if nmap found any open ports
                        nmap_ports = self._extract_open_ports_from_nmap(scan_result)
                        
                        if nmap_ports:
                            logger.info(f"Nmap found {len(nmap_ports)} open ports")
                            # Process the nmap results normally
                            return self._process_nmap_results(scan_result, scan_type)
                    except Exception as e:
                        logger.warning(f"Nmap scan failed: {str(e)}, using manual results")
                
                # Return our manual results
                return manual_result
            
            # If no ports were found by manual check, still try nmap
            try:
                scan_result = self.scanner.scan(target, config['ports'], config['arguments'])
                # Process the nmap results
                return self._process_nmap_results(scan_result, scan_type)
            except Exception as e:
                logger.error(f"Nmap scan error: {str(e)}")
                return {
                    'status': 'error',
                    'error': str(e)
                }
            
        except Exception as e:
            logger.error(f"Scan error: {str(e)}")
            return {
                'status': 'error',
                'error': str(e)
            }
    
    def _process_nmap_results(self, scan_result, scan_type):
        """Process nmap scan results into our standard format"""
        scan_results = []
        scan_info = {
            'scan_type': scan_type,
            'command_line': self.scanner.command_line(),
            'scan_time': self.scanner.scanstats().get('elapsed', ''),
            'total_hosts': len(self.scanner.all_hosts())
        }
        
        vulnerabilities = []  # Create a list to store detected vulnerabilities
        
        for host in self.scanner.all_hosts():
            host_data = {
                'host': host,
                'state': self.scanner[host].state(),
                'ports': []
            }
            
            # Flag to check if we found any open ports
            found_open_ports = False
            open_ports = []  # List to track open ports for vulnerability detection
            
            for proto in self.scanner[host].all_protocols():
                ports = self.scanner[host][proto].keys()
                for port in ports:
                    port_info = self.scanner[host][proto][port]
                    # Normalize state to 'open', 'closed', or 'filtered'
                    port_state = port_info['state']
                    
                    if port_state == 'open':
                        found_open_ports = True
                        open_ports.append(int(port))  # Add to open ports list
                            
                    port_data = {
                        'port': port,
                        'state': port_state,
                        'service': port_info.get('name', ''),
                        'version': port_info.get('version', ''),
                        'product': port_info.get('product', ''),
                        'extrainfo': port_info.get('extrainfo', ''),
                        'reason': port_info.get('reason', ''),
                        'cpe': port_info.get('cpe', '')
                    }
                    host_data['ports'].append(port_data)
            
            # Add a flag to indicate if any open ports were found
            host_data['has_open_ports'] = found_open_ports
            
            # Process risky open ports and create vulnerabilities
            if open_ports:
                RISKY_PORTS = {
                    80: {'name': 'HTTP Server', 'severity': 'LOW'},
                    443: {'name': 'HTTPS Server', 'severity': 'LOW'},
                    21: {'name': 'FTP Server', 'severity': 'HIGH'},
                    22: {'name': 'SSH Server', 'severity': 'LOW'},
                    23: {'name': 'Telnet Server', 'severity': 'HIGH'},
                    25: {'name': 'SMTP Server', 'severity': 'MEDIUM'},
                    53: {'name': 'DNS Server', 'severity': 'LOW'},
                    3306: {'name': 'MySQL Database', 'severity': 'MEDIUM'},
                    5432: {'name': 'PostgreSQL Database', 'severity': 'MEDIUM'},
                    8080: {'name': 'HTTP Alternate Port', 'severity': 'MEDIUM'}
                }
                
                for port in open_ports:
                    if port in RISKY_PORTS:
                        port_info = RISKY_PORTS[port]
                        vuln_data = {
                            'type': 'open_port',
                            'name': f"Open Port {port} ({port_info['name']})",
                            'description': f"Port {port} is open, running {port_info['name']}. This may present a security risk depending on configuration.",
                            'severity': port_info['severity'],
                            'evidence': f"Port {port} is open and accessible",
                            'confidence': 'high',
                            'cvss': 5.0,  # Default CVSS score for open port
                            'host': host  # Add the host for database storage
                        }
                        
                        # Add to vulnerabilities list
                        vulnerabilities.append(vuln_data)
            
            # Only try to include OS data if it exists
            if 'osmatch' in self.scanner[host]:
                host_data['os_matches'] = self.scanner[host]['osmatch']
            
            scan_results.append(host_data)
        
        # After scanning, save any detected port vulnerabilities to database
        from vulnerability.models import Vulnerability
        from django.utils import timezone
        
        # Get the target from the first host (assuming all hosts are for the same target)
        if scan_results and vulnerabilities:
            target = scan_results[0]['host']
            
            for vuln in vulnerabilities:
                try:
                    # Create vulnerability record in database
                    Vulnerability.objects.get_or_create(
                        target=target,
                        name=vuln['name'],
                        defaults={
                            'description': vuln['description'],
                            'severity': vuln['severity'],
                            'vuln_type': 'open_port',
                            'evidence': vuln['evidence'],
                            'source': 'port_scan',
                            'confidence': vuln['confidence'],
                            'cvss_score': vuln['cvss'],
                            'is_fixed': False,
                            'discovery_date': timezone.now()
                        }
                    )
                except Exception as e:
                    logger.error(f"Error creating vulnerability for open port: {str(e)}")
        
        return {
            'status': 'success',
            'scan_info': scan_info,
            'results': scan_results,
            'open_ports_found': any(host.get('has_open_ports', False) for host in scan_results),
            'vulnerabilities': vulnerabilities  # Include vulnerabilities in result
        }
    
    def _extract_open_ports_from_nmap(self, scan_result) -> List[int]:
        """Extract list of open ports from nmap scan result"""
        open_ports = []
        for host in self.scanner.all_hosts():
            for proto in self.scanner[host].all_protocols():
                ports = self.scanner[host][proto].keys()
                for port in ports:
                    port_info = self.scanner[host][proto][port]
                    if port_info['state'] == 'open':
                        open_ports.append(int(port))
        return open_ports
        
    def _check_responsive_ports(self, target: str) -> List[int]:
        """Check if common ports are responsive and return list of open ports"""
        common_ports = [80, 443, 22, 21, 8080, 8443, 3306, 3389, 7001, 8081, 8000]
        open_ports = []
        
        for port in common_ports:
            try:
                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                    sock.settimeout(1)
                    result = sock.connect_ex((target, port))
                    if result == 0:
                        open_ports.append(port)
                        logger.info(f"Target {target} is responsive on port {port}")
            except:
                pass
        
        return open_ports
    
    def _guess_service_name(self, port: int) -> str:
        """Guess service name based on common port numbers"""
        service_map = {
            21: 'ftp',
            22: 'ssh',
            23: 'telnet',
            25: 'smtp',
            53: 'domain',
            80: 'http',
            110: 'pop3',
            139: 'netbios-ssn',
            143: 'imap',
            443: 'https', 
            445: 'microsoft-ds',
            993: 'imaps',
            995: 'pop3s',
            1723: 'pptp',
            3306: 'mysql',
            3389: 'ms-wbt-server',
            5900: 'vnc',
            7001: 'weblogic',
            8000: 'http-alt',
            8080: 'http-proxy',
            8081: 'http-alt',
            8443: 'https-alt'
        }
        return service_map.get(port, 'unknown')
    

    def get_available_scan_types(self) -> Dict[str, str]:
        return {
            ScanType.QUICK.value: "Fast scan of most common ports (21-23,25,80,443,3306,8080)",
            ScanType.PARTIAL.value: "Standard scan of first 1000 ports with version detection",
            ScanType.COMPLETE.value: "Comprehensive scan of all ports with version detection",
            ScanType.FULL.value: "Intensive scan with all features including vulnerability detection"
        }
        
    # In PortScanner class in scanner.py
    def _create_manual_result(self, target: str, open_ports: List[int]) -> Dict[str, Any]:
        """Create scan result dictionary from manually detected open ports"""
        ports_data = []
        vulnerabilities = []  # Add a list for vulnerabilities
        
        for port in open_ports:
            service_name = self._guess_service_name(port)
            ports_data.append({
                'port': port,
                'state': 'open',
                'service': service_name,
                'version': '',
                'product': '',
                'extrainfo': 'Detected by manual scan',
                'reason': 'syn-ack',
                'cpe': ''
            })
            
            # Create vulnerability data for open ports
            # Define risk levels for different services
            RISKY_PORTS = {
                80: {'name': 'HTTP Server', 'severity': 'LOW'},
                443: {'name': 'HTTPS Server', 'severity': 'LOW'},
                21: {'name': 'FTP Server', 'severity': 'HIGH'},
                22: {'name': 'SSH Server', 'severity': 'LOW'},
                23: {'name': 'Telnet Server', 'severity': 'HIGH'},
                25: {'name': 'SMTP Server', 'severity': 'MEDIUM'},
                53: {'name': 'DNS Server', 'severity': 'LOW'},
                3306: {'name': 'MySQL Database', 'severity': 'MEDIUM'},
                5432: {'name': 'PostgreSQL Database', 'severity': 'MEDIUM'},
                8080: {'name': 'HTTP Alternate Port', 'severity': 'MEDIUM'}
            }
            
            # Add vulnerability for the detected port
            if port in RISKY_PORTS:
                port_info = RISKY_PORTS[port]
                vuln_data = {
                    'type': 'open_port',
                    'name': f"Open Port {port} ({port_info['name']})",
                    'description': f"Port {port} is open, running {port_info['name']}. This may present a security risk depending on configuration.",
                    'severity': port_info['severity'],
                    'evidence': f"Port {port} is open and accessible",
                    'confidence': 'high',
                    'cvss': 5.0,  # Default CVSS score for open port
                    'host': target  # Add the host for database storage
                }
                vulnerabilities.append(vuln_data)
        
        # Add vulnerabilities to database
        self._save_port_vulnerabilities(target, vulnerabilities)
        
        return {
            'status': 'success',
            'scan_info': {
                'scan_type': 'manual',
                'command_line': 'Manual socket scan',
                'scan_time': '0',
                'total_hosts': 1
            },
            'results': [{
                'host': target,
                'state': 'up',
                'ports': ports_data
            }],
            'manual_detected': True,
            'vulnerabilities': vulnerabilities  # Include vulnerabilities in result
        }

    def _save_port_vulnerabilities(self, target: str, vulnerabilities: List[Dict]) -> None:
        """Save port vulnerabilities to database"""
        from vulnerability.models import Vulnerability
        from django.utils import timezone
        
        for vuln in vulnerabilities:
            try:
                # Create vulnerability record in database
                Vulnerability.objects.get_or_create(
                    target=target,
                    name=vuln['name'],
                    defaults={
                        'description': vuln['description'],
                        'severity': vuln['severity'],
                        'vuln_type': 'open_port',
                        'evidence': vuln['evidence'],
                        'source': 'port_scan',
                        'confidence': vuln['confidence'],
                        'cvss_score': vuln['cvss'],
                        'is_fixed': False,
                        'discovery_date': timezone.now()
                    }
                )
            except Exception as e:
                self.logger.error(f"Error creating vulnerability for open port: {str(e)}")
        
    # Add to port scanning results processing
import nmap
from typing import Dict, List, Optional
import logging
from datetime import datetime
import socket
import requests
from urllib.parse import urlparse
import concurrent.futures
import threading
import time
from django.conf import settings
import os

class ServiceIdentifier:
    def __init__(self):
        self.scanner = nmap.PortScanner()
        self.logger = logging.getLogger(__name__)
        self.timeout = getattr(settings, 'SERVICE_SCAN_TIMEOUT', 180)  # 3 minutes default
        self._stop_event = threading.Event()
        # Use shorter timeouts for various operations
        self.discovery_timeout = 60  # 1 minute for port discovery
        self.connection_timeout = 2  # 2 seconds for individual connections

    def identify_services(self, target: str, scan_type: str = 'standard') -> Dict:
        """Comprehensive service identification with improved timeout handling"""
        try:
            # Parse target
            parsed_target = urlparse(target)
            target_host = parsed_target.netloc or parsed_target.path
            target_host = target_host.split(':')[0]

            scan_config = self._get_scan_config(scan_type)
            self.logger.info(f"Starting {scan_type} service scan for {target_host}")

            # Use a simpler approach with manual timeout handling
            self._stop_event.clear()
            
            # Start a timer to enforce overall timeout
            start_time = time.time()
            
            # Set timeout based on scan type
            effective_timeout = self.timeout
            if scan_type == 'quick':
                effective_timeout = min(self.timeout, 120)  # 2 minutes max for quick
            elif scan_type == 'full':
                effective_timeout = self.timeout  # Full timeout for full scan
            
            self.logger.info(f"Using timeout of {effective_timeout} seconds for {scan_type} scan")
            
            # Discover open ports with a shorter timeout
            open_ports = self._discover_ports_with_timeout(target_host, scan_config, 
                                                          timeout=self.discovery_timeout)
            
            # Check if we should stop
            elapsed = time.time() - start_time
            if elapsed > effective_timeout * 0.8:  # 80% of timeout used
                self.logger.warning(f"Port discovery took {elapsed:.1f}s, approaching timeout")
                return {
                    'status': 'success',
                    'services': [],
                    'total_services': 0,
                    'scan_stats': {'open_ports': len(open_ports)}
                }
            
            if not open_ports:
                return {
                    'status': 'success',
                    'services': [],
                    'total_services': 0,
                    'scan_stats': {'open_ports': 0}
                }

            # Perform limited service detection directly without nmap
            self.logger.info(f"Starting basic service detection on {len(open_ports)} ports")
            services = self._perform_basic_service_detection(target_host, open_ports)
            
            # Only do nmap service detection if we have time and found less than 10 ports
            time_left = effective_timeout - (time.time() - start_time)
            if time_left > 60 and len(open_ports) < 10 and not self._stop_event.is_set():
                try:
                    # Try nmap service detection with a short timeout
                    self.logger.info(f"Starting nmap service detection (time left: {time_left:.1f}s)")
                    nmap_services = self._run_nmap_service_detection(
                        target_host, open_ports, scan_config, timeout=min(60, time_left)
                    )
                    
                    # Add any services found by nmap
                    if nmap_services:
                        services.extend(nmap_services)
                except Exception as e:
                    self.logger.error(f"Nmap service detection failed: {str(e)}")
                    # Continue with basic services
            
            # Remove duplicates by port
            service_dict = {}
            for service in services:
                port = service['port']
                # Keep the one with more information
                if port not in service_dict or len(str(service)) > len(str(service_dict[port])):
                    service_dict[port] = service
            
            unique_services = list(service_dict.values())
            
            return {
                'status': 'success',
                'target': target,
                'timestamp': datetime.now().isoformat(),
                'services': unique_services,
                'total_services': len(unique_services),
                'scan_stats': {
                    'open_ports': len(open_ports),
                    'scan_time': f"{time.time() - start_time:.1f}s"
                }
            }

        except Exception as e:
            self.logger.error(f"Service scan failed for {target}: {str(e)}")
            return {
                'status': 'error',
                'error': str(e),
                'details': 'Service scan failed'
            }
    
    def _discover_ports_with_timeout(self, target: str, config: Dict, timeout: int = 60) -> List[int]:
        """Discover open ports with a strict timeout"""
        self.logger.info(f"Starting port discovery with {timeout}s timeout")
        open_ports = set()
        
        # Parse ports to scan
        ports_to_scan = self._parse_ports(config['ports'])
        
        # Use a smaller subset of ports for quicker scanning
        if len(ports_to_scan) > 100:
            # If many ports, prioritize common ones
            common_ports = [21, 22, 23, 25, 53, 80, 110, 139, 143, 443, 445, 993, 995, 1723, 3306, 3389, 5900, 8080, 8443]
            subset = [p for p in ports_to_scan if p in common_ports]
            subset.extend(sorted(list(set(ports_to_scan) - set(subset)))[:100-len(subset)])
            self.logger.info(f"Scanning subset of {len(subset)} ports out of {len(ports_to_scan)}")
            ports_to_scan = subset
        
        start_time = time.time()
        
        # Check ports in parallel with a maximum time limit
        with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
            future_to_port = {
                executor.submit(self._check_port, target, port, 1): port 
                for port in ports_to_scan
            }
            
            for future in concurrent.futures.as_completed(future_to_port):
                if time.time() - start_time > timeout:
                    self.logger.warning(f"Port discovery reached timeout of {timeout}s")
                    break
                    
                if self._stop_event.is_set():
                    break
                    
                try:
                    is_open = future.result()
                    if is_open:
                        port = future_to_port[future]
                        open_ports.add(port)
                        self.logger.info(f"Found open port {port} on {target}")
                except Exception as e:
                    self.logger.debug(f"Error checking port: {str(e)}")
                    continue

        self.logger.info(f"Port discovery completed in {time.time() - start_time:.1f}s, found {len(open_ports)} open ports")
        return sorted(list(open_ports))
    
    def _perform_basic_service_detection(self, target: str, open_ports: List[int]) -> List[Dict]:
        """Perform basic service detection without nmap"""
        services = []
        
        # Common service port mappings
        common_services = {
            21: {'name': 'ftp', 'category': 'file_transfer', 'risk_level': 'HIGH'},
            22: {'name': 'ssh', 'category': 'remote_access', 'risk_level': 'LOW'},
            23: {'name': 'telnet', 'category': 'remote_access', 'risk_level': 'HIGH'},
            25: {'name': 'smtp', 'category': 'mail', 'risk_level': 'MEDIUM'},
            53: {'name': 'domain', 'category': 'dns', 'risk_level': 'LOW'},
            80: {'name': 'http', 'category': 'web', 'risk_level': 'MEDIUM'},
            110: {'name': 'pop3', 'category': 'mail', 'risk_level': 'MEDIUM'},
            139: {'name': 'netbios-ssn', 'category': 'file_transfer', 'risk_level': 'HIGH'},
            143: {'name': 'imap', 'category': 'mail', 'risk_level': 'MEDIUM'},
            443: {'name': 'https', 'category': 'web', 'risk_level': 'LOW'},
            445: {'name': 'microsoft-ds', 'category': 'file_transfer', 'risk_level': 'HIGH'},
            993: {'name': 'imaps', 'category': 'mail', 'risk_level': 'LOW'},
            995: {'name': 'pop3s', 'category': 'mail', 'risk_level': 'LOW'},
            1723: {'name': 'pptp', 'category': 'vpn', 'risk_level': 'MEDIUM'},
            3306: {'name': 'mysql', 'category': 'database', 'risk_level': 'HIGH'},
            3389: {'name': 'ms-wbt-server', 'category': 'remote_access', 'risk_level': 'HIGH'},
            5900: {'name': 'vnc', 'category': 'remote_access', 'risk_level': 'HIGH'},
            8080: {'name': 'http-proxy', 'category': 'web', 'risk_level': 'MEDIUM'},
            8443: {'name': 'https-alt', 'category': 'web', 'risk_level': 'LOW'}
        }
        
        for port in open_ports:
            # Start with defaults
            service_info = common_services.get(port, {
                'name': 'unknown',
                'category': 'other',
                'risk_level': 'MEDIUM'
            })
            
            # Try basic banner grabbing with short timeout
            banner = self._grab_banner(target, port)
            
            # For HTTP ports, try to get more info
            if port in [80, 443, 8080, 8443] or banner and ('HTTP' in banner or 'html' in banner.lower()):
                http_info = self._get_http_info(target, port)
                if http_info:
                    service_detail = {
                        'port': port,
                        'protocol': 'tcp',
                        'state': 'open',
                        'service': {
                            'name': 'http' if port != 443 and port != 8443 else 'https',
                            'product': http_info.get('server', ''),
                            'version': '',
                            'extrainfo': '',
                            'banner': banner
                        },
                        'category': 'web',
                        'risk_level': 'MEDIUM',
                        'http_info': http_info
                    }
                    services.append(service_detail)
                    continue
            
            # For other ports, use the basic info
            service_detail = {
                'port': port,
                'protocol': 'tcp',
                'state': 'open',
                'service': {
                    'name': service_info.get('name', 'unknown'),
                    'product': '',
                    'version': '',
                    'extrainfo': '',
                    'banner': banner
                },
                'category': service_info.get('category', 'other'),
                'risk_level': service_info.get('risk_level', 'MEDIUM')
            }
            services.append(service_detail)
        
        return services
    
    def _grab_banner(self, target: str, port: int) -> str:
        """Grab service banner with timeout"""
        banner = ""
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                sock.settimeout(self.connection_timeout)
                sock.connect((target, port))
                sock.send(b"HEAD / HTTP/1.0\r\n\r\n")
                banner = sock.recv(1024).decode('utf-8', errors='ignore')
        except:
            # Try one more time with no data sent (for non-HTTP services)
            try:
                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                    sock.settimeout(self.connection_timeout)
                    sock.connect((target, port))
                    banner = sock.recv(1024).decode('utf-8', errors='ignore')
            except:
                pass
        return banner
    
    def _get_http_info(self, target: str, port: int) -> Dict:
        """Get HTTP service information"""
        protocol = 'https' if port == 443 or port == 8443 else 'http'
        url = f"{protocol}://{target}:{port}"
        try:
            response = requests.get(
                url, 
                timeout=self.connection_timeout,
                verify=False,
                headers={'User-Agent': 'Mozilla/5.0 SecurityScan'}
            )
            return {
                'status': response.status_code,
                'server': response.headers.get('Server', ''),
                'title': self._extract_title(response.text),
                'headers': dict(response.headers)
            }
        except:
            return None
    
    def _extract_title(self, html: str) -> str:
        """Extract title from HTML"""
        import re
        title_match = re.search(r'<title>(.*?)</title>', html, re.IGNORECASE | re.DOTALL)
        if title_match:
            return title_match.group(1).strip()
        return ""
    
    def _run_nmap_service_detection(self, target: str, open_ports: List[int], config: Dict, timeout: int = 60) -> List[Dict]:
        """Run nmap service detection with timeout"""
        if not open_ports:
            return []
        
        # Create a minimal scan config
        args = "-sV -T4 --version-all"
        ports_str = ','.join(map(str, open_ports))
        
        try:
            # Check if nmap is available and working
            try:
                self.scanner.scan('127.0.0.1', '22', "-sV -T5")
            except:
                self.logger.warning("Nmap test scan failed, skipping nmap service detection")
                return []
            
            # Run the scan with manual timeout using threading
            result_holder = []
            
            def run_scan():
                try:
                    scan_result = self.scanner.scan(target, ports_str, args)
                    result_holder.append(scan_result)
                except Exception as e:
                    self.logger.error(f"Nmap scan error in thread: {str(e)}")
            
            # Start thread
            scan_thread = threading.Thread(target=run_scan)
            scan_thread.daemon = True
            scan_thread.start()
            
            # Wait with timeout
            scan_thread.join(timeout)
            
            if not result_holder:
                self.logger.warning(f"Nmap scan timed out or failed after {timeout}s")
                return []
            
            # Process results
            services = []
            for host in self.scanner.all_hosts():
                for proto in self.scanner[host].all_protocols():
                    for port in self.scanner[host][proto].keys():
                        service_info = self.scanner[host][proto][port]
                        if service_info['state'] == 'open':
                            service_detail = {
                                'port': port,
                                'protocol': proto,
                                'state': service_info['state'],
                                'service': {
                                    'name': service_info.get('name', 'unknown'),
                                    'product': service_info.get('product', ''),
                                    'version': service_info.get('version', ''),
                                    'extrainfo': service_info.get('extrainfo', ''),
                                    'cpe': service_info.get('cpe', [])
                                },
                                'category': self._categorize_service(service_info),
                                'risk_level': self._assess_risk_level(service_info)
                            }
                            services.append(service_detail)
            
            return services
        except Exception as e:
            self.logger.error(f"Error in nmap service detection: {str(e)}")
            return []
            
    def _get_scan_config(self, scan_type: str) -> Dict:
        """Get scan configuration based on scan type"""
        configs = {
            'quick': {
                'ports': '21-23,25,80,443,3306,8080',
                'arguments': '-sV -sT -Pn -T4 --version-light'  # Fast scan
            },
            'standard': {
                'ports': '1-1000',
                'arguments': '-sV -sT -Pn -T4 --version-all'  # Standard scan
            },
            'full': {
                'ports': '1-65535',
                'arguments': '-sV -sT -Pn -T4 --version-all'  # Full scan
            },
            'stealth': {
                'ports': '1-1000',
                'arguments': '-sV -sS -Pn -T2 --version-all'  # Stealth scan
            }
        }
        return configs.get(scan_type, configs['standard'])

    def _check_port(self, target: str, port: int, timeout: int = 1) -> bool:
        """Check if a port is open with timeout"""
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                sock.settimeout(timeout)
                result = sock.connect_ex((target, port))
                return result == 0
        except:
            return False

    def _parse_ports(self, ports_str: str) -> List[int]:
        """Parse ports string into list of port numbers"""
        ports = set()
        for part in ports_str.split(','):
            if '-' in part:
                start, end = map(int, part.split('-'))
                ports.update(range(start, end + 1))
            else:
                ports.add(int(part))
        return list(ports)

    def _categorize_service(self, service_info: Dict) -> str:
        """Categorize service based on name and product"""
        service_categories = {
            'web': ['http', 'https', 'nginx', 'apache'],
            'database': ['mysql', 'postgresql', 'mongodb'],
            'mail': ['smtp', 'pop3', 'imap'],
            'file_transfer': ['ftp', 'sftp'],
            'remote_access': ['ssh', 'telnet', 'rdp'],
            'dns': ['dns', 'domain']
        }

        service_name = service_info.get('name', '').lower()
        
        for category, services in service_categories.items():
            if any(s in service_name for s in services):
                return category
        return 'other'

    def _assess_risk_level(self, service_info: Dict) -> str:
        """Basic risk assessment of services"""
        high_risk = ['telnet', 'ftp']
        medium_risk = ['smtp', 'pop3']
        
        service_name = service_info.get('name', '').lower()
        
        if any(service in service_name for service in high_risk):
            return 'HIGH'
        elif any(service in service_name for service in medium_risk):
            return 'MEDIUM'
        return 'LOW'from django.db import models
from django.core.validators import MinValueValidator, MaxValueValidator
import logging
from django.db import transaction

logger = logging.getLogger(__name__)

class Vulnerability(models.Model):
    SEVERITY_CHOICES = [
        ('LOW', 'Low'),
        ('MEDIUM', 'Medium'),
        ('HIGH', 'High'),
        ('CRITICAL', 'Critical'),
    ]

    # Updated source choices to include more options
    SOURCE_CHOICES = [
        ('internal', 'Internal Scanner'),
        ('zap', 'OWASP ZAP'),
        ('nuclei', 'Nuclei Scanner'),
        ('openvas', 'OpenVAS Scanner'),
        ('manual', 'Manual Entry'),
        ('multiple', 'Multiple Sources')  # For correlated findings
    ]

    # Basic Information
    target = models.CharField(max_length=255, db_index=True)
    name = models.CharField(max_length=255)
    description = models.TextField()
    severity = models.CharField(max_length=10, choices=SEVERITY_CHOICES, db_index=True)
    vuln_type = models.CharField(max_length=50, db_index=True)
    
    # Evidence and Details
    evidence = models.TextField()
    solution = models.TextField(blank=True)
    references = models.JSONField(default=list)
    
    # Source and Confidence - increased max_length to accommodate combined sources
    source = models.CharField(max_length=100)  # Removed choices constraint and increased length
    confidence = models.CharField(max_length=50, default='medium')
    
    # Status and Tracking
    discovery_date = models.DateTimeField(auto_now_add=True)
    is_fixed = models.BooleanField(default=False, db_index=True)
    fix_date = models.DateTimeField(null=True, blank=True)
    notes = models.TextField(blank=True)
    
    # Additional Metadata
    cwe = models.CharField(max_length=50, blank=True)
    cvss_score = models.FloatField(
        null=True, 
        blank=True,
        validators=[MinValueValidator(0.0), MaxValueValidator(10.0)]
    )
    metadata = models.JSONField(default=dict)  # Added metadata field
    
    def __str__(self):
        return f"{self.target} - {self.name} ({self.severity})"

# In the Vulnerability model in vulnerability/models.py

# In the Vulnerability model in vulnerability/models.py

    def save(self, *args, **kwargs):
        # Extract our custom parameter before passing to Django save
        skip_deduplication = kwargs.pop('skip_deduplication', False) if 'skip_deduplication' in kwargs else False
        
        # Normalize target before saving
        self.target = self.__class__.normalize_target(self.target)
        
        # Ensure severity is uppercase
        if self.severity:
            self.severity = self.severity.upper()
        
        # Set fix_date when vulnerability is marked as fixed
        if self.is_fixed and not self.fix_date:
            from django.utils import timezone
            self.fix_date = timezone.now()
        
        # Handle source field for multiple sources
        if ',' in self.source:
            # Store original sources in metadata for reference
            if not self.metadata:
                self.metadata = {}
            self.metadata['original_sources'] = self.source.split(',')
            
        # Call save with clean kwargs
        super().save(*args, **kwargs)
        
        # Skip deduplication if explicitly requested (to avoid infinite recursion)
        # or schedule deduplication to run after save
        if not skip_deduplication:
            # Store target to use in lambda
            target = self.target
            # Use raw lambda to avoid potential reference issues
            transaction.on_commit(lambda t=target: self.__class__.deduplicate_vulnerabilities(t))

    @classmethod
    def normalize_target(cls, target_str):
        """Class method to normalize target URLs"""
        if not target_str:
            return target_str
            
        # Remove protocol prefix
        if '://' in target_str:
            target_str = target_str.split('://', 1)[1]
        
        # Remove path, trailing slash, etc.
        if '/' in target_str:
            target_str = target_str.split('/', 1)[0]
            
        # Remove port if present
        if ':' in target_str:
            target_str = target_str.split(':', 1)[0]
            
        # Remove 'www.' prefix if present
        if target_str.startswith('www.'):
            target_str = target_str[4:]
            
        return target_str.lower()

    @classmethod
    def deduplicate_vulnerabilities(cls, target):
        """
        Deduplicate vulnerabilities for a target by merging duplicates
        
        Args:
            target: The target hostname/domain
            
        Returns:
            dict: Statistics about deduplication
        """
        from django.db.models import Count
        from django.db import transaction
        
        # Use the class method to normalize target
        normalized_target = cls.normalize_target(target)
        
        # Find all targets that might be the same after normalization
        potential_targets = []
        for t in cls.objects.values_list('target', flat=True).distinct():
            if cls.normalize_target(t) == normalized_target:
                potential_targets.append(t)
        
        if not potential_targets:
            return {
                'original_count': 0,
                'merged_count': 0,
                'final_count': 0,
                'message': 'No matching targets found'
            }
        
        # Keep track of statistics
        stats = {
            'original_count': cls.objects.filter(target__in=potential_targets).count(),
            'merged_count': 0,
            'final_count': 0
        }
        
        try:
            with transaction.atomic():
                # First normalize all targets
                for pt in potential_targets:
                    if pt != normalized_target:
                        # Update target to normalized version
                        cls.objects.filter(target=pt).update(target=normalized_target)
                
                # Then deduplicate within the normalized target
                duplicate_groups = cls.objects.filter(target=normalized_target).values(
                    'name', 'vuln_type', 'severity'
                ).annotate(
                    count=Count('id')
                ).filter(count__gt=1)
                
                # Process each group
                for group in duplicate_groups:
                    duplicates = cls.objects.filter(
                        target=normalized_target,
                        name=group['name'],
                        vuln_type=group['vuln_type'],
                        severity=group['severity']
                    ).order_by('discovery_date')
                    
                    # Keep the first (oldest) vulnerability as the canonical one
                    if duplicates.count() > 1:
                        primary_vuln = duplicates.first()
                        
                        # Process other duplicates
                        for dup in duplicates[1:]:
                            # Combine sources if different
                            sources = set(primary_vuln.source.split(','))
                            for source in dup.source.split(','):
                                sources.add(source)
                            primary_vuln.source = ','.join(sorted(sources))
                            
                            # Use the higher CVSS score if available
                            if dup.cvss_score and (not primary_vuln.cvss_score or dup.cvss_score > primary_vuln.cvss_score):
                                primary_vuln.cvss_score = dup.cvss_score
                            
                            # Take the most recent evidence if available
                            if dup.evidence and len(dup.evidence) > len(primary_vuln.evidence):
                                primary_vuln.evidence = dup.evidence
                                
                            # Merge references
                            primary_refs = set(primary_vuln.references)
                            for ref in dup.references:
                                primary_refs.add(ref)
                            primary_vuln.references = list(primary_refs)
                            
                            # Track the merged duplicate in metadata
                            if not primary_vuln.metadata:
                                primary_vuln.metadata = {}
                            if 'merged_duplicates' not in primary_vuln.metadata:
                                primary_vuln.metadata['merged_duplicates'] = []
                            primary_vuln.metadata['merged_duplicates'].append({
                                'id': dup.id,
                                'discovery_date': dup.discovery_date.isoformat(),
                                'source': dup.source
                            })
                            
                            # Delete the duplicate
                            dup.delete()
                            stats['merged_count'] += 1
                        
                        # Save the updated primary vulnerability with skip_deduplication
                        # to avoid infinite recursion
                        primary_vuln.save(skip_deduplication=True)
            
            # Calculate final counts
            stats['final_count'] = cls.objects.filter(target=normalized_target).count()
            return stats
            
        except Exception as e:
            logger.error(f"Error deduplicating vulnerabilities: {str(e)}")
            return {'error': str(e)}    

    @classmethod
    @transaction.atomic
    def remove_duplicates(cls, target=None):
        """
        Remove duplicate vulnerabilities for a target
        
        Args:
            target: Optional target to limit deduplication
            
        Returns:
            int: Number of vulnerabilities removed
        """
        from django.db.models import Count
        
        try:
            # Build query based on if target is provided
            if target:
                # Normalize target first
                normalized_target = cls.normalize_target(target)
                # Find duplicates for specific target
                duplicates = cls.objects.filter(target=normalized_target)
            else:
                # Find duplicates across all targets
                duplicates = cls.objects.all()
            
            # Group by name and target to find duplicates
            duplicate_groups = duplicates.values('name', 'target').annotate(
                count=Count('id')
            ).filter(count__gt=1)
            
            removed_count = 0
            
            # Process each duplicate group
            for group in duplicate_groups:
                # Get all duplicates in this group, ordered by ID (oldest first)
                dupes = cls.objects.filter(
                    name=group['name'],
                    target=group['target']
                ).order_by('id')
                
                # Keep the first one, delete the rest
                keep = dupes.first()
                to_delete = dupes.exclude(id=keep.id)
                
                # Count how many we're deleting
                delete_count = to_delete.count()
                
                # Delete duplicates
                to_delete.delete()
                removed_count += delete_count
            
            return removed_count
            
        except Exception as e:
            logger.error(f"Error removing duplicates: {str(e)}")
            return 0
    
    @property
    def age_in_days(self):
        from django.utils import timezone
        return (timezone.now() - self.discovery_date).days
    
    # Add this method to the Vulnerability model


    @property
    def risk_score(self):
        """Calculate risk score based on CVSS and age"""
        base_score = self.cvss_score if self.cvss_score else {
            'CRITICAL': 9.0,
            'HIGH': 7.0,
            'MEDIUM': 5.0,
            'LOW': 3.0
        }.get(self.severity, 1.0)
        
        # Age factor: 1.0 - 2.0 based on age (caps at 90 days)
        age_factor = min(1 + (self.age_in_days / 90), 2.0)
        
        return base_score * age_factor
    
    @property
    def source_list(self):
        """Return the source as a list for easier filtering"""
        return self.source.split(',')

class NucleiFinding(models.Model):
    SEVERITY_CHOICES = [
        ('CRITICAL', 'Critical'),
        ('HIGH', 'High'),
        ('MEDIUM', 'Medium'),
        ('LOW', 'Low'),
        ('INFO', 'Info'),
    ]

    template_id = models.CharField(max_length=255)
    name = models.CharField(max_length=255)
    severity = models.CharField(max_length=10, choices=SEVERITY_CHOICES)
    finding_type = models.CharField(max_length=50)
    host = models.CharField(max_length=255)
    matched_at = models.URLField(max_length=500)
    description = models.TextField()
    tags = models.JSONField(default=list)
    references = models.JSONField(default=list)
    cwe = models.CharField(max_length=50, blank=True, null=True)
    cvss_score = models.FloatField(null=True, blank=True)
    discovery_date = models.DateTimeField(auto_now_add=True)
    scan_id = models.CharField(max_length=100)
    target = models.CharField(max_length=255)
    
    class Meta:
        indexes = [
            models.Index(fields=['template_id']),
            models.Index(fields=['severity']),
            models.Index(fields=['discovery_date']),
            models.Index(fields=['target']),
        ]import re
from django.http import JsonResponse
from django.views import View
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator

from vulnerability.correlation import VulnerabilityCorrelator


from .unified_scanner import UnifiedVulnerabilityScanner
from .models import Vulnerability
import json
import logging
from django.db.models import Count
from django.db import transaction

from django.http import JsonResponse
from django.views import View
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from .unified_scanner import UnifiedVulnerabilityScanner
import json
import logging

logger = logging.getLogger(__name__)  # Create logger at module level

# File to update: vulnerability/views.py

@method_decorator(csrf_exempt, name='dispatch')
class VulnerabilityScanView(View):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.scanner = UnifiedVulnerabilityScanner()
        self.logger = logger  # Assign module logger to instance

    def post(self, request):
        try:
            data = json.loads(request.body)
            
            # Extract parameters with defaults
            target = data.get('target')
            scan_type = data.get('scan_type', 'standard')
            include_zap = data.get('include_zap', True)
            include_nuclei = data.get('include_nuclei', True)
            nuclei_scan_type = data.get('nuclei_scan_type', 'basic')
            # New parameter for advanced correlation
            use_advanced_correlation = data.get('use_advanced_correlation', True)

            # Validate required parameters
            if not target:
                return JsonResponse({
                    'status': 'error',
                    'error': 'Target is required'
                }, status=400)

            # Validate nuclei_scan_type
            if nuclei_scan_type not in ['basic', 'advanced']:
                return JsonResponse({
                    'status': 'error',
                    'error': 'nuclei_scan_type must be either "basic" or "advanced"'
                }, status=400)

            # Log scan configuration
            self.logger.info(f"Starting vulnerability scan for {target}")
            self.logger.info(
                f"Scan configuration - Type: {scan_type}, "
                f"ZAP: {include_zap}, "
                f"Nuclei: {include_nuclei} ({nuclei_scan_type}), "
                f"Advanced Correlation: {use_advanced_correlation}"
            )
            
            # Run the scan
            results = self.scanner.scan_target(
                target=target,
                scan_type=scan_type,
                include_zap=include_zap,
                include_nuclei=include_nuclei,
                nuclei_scan_type=nuclei_scan_type,
                use_advanced_correlation=use_advanced_correlation
            )
            
            # Deduplicate descriptions to reduce response size
            results = self._deduplicate_descriptions(results)
            
            return JsonResponse(results)

        except json.JSONDecodeError:
            self.logger.error("Invalid JSON in request body")
            return JsonResponse({
                'status': 'error',
                'error': 'Invalid JSON data'
            }, status=400)
        except Exception as e:
            self.logger.exception(f"Error during vulnerability scan: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)
            
    # Add this method to the VulnerabilityScanView class
    def _deduplicate_descriptions(self, results):
        """Remove duplicate paragraphs from vulnerability descriptions"""
        # Check if vulnerabilities field exists
        if 'vulnerabilities' in results:
            for vuln in results['vulnerabilities']:
                if 'description' in vuln and vuln['description']:
                    vuln['description'] = self._deduplicate_text(vuln['description'])
        
        # Also check for findings field
        if 'findings' in results:
            for finding in results['findings']:
                if 'description' in finding and finding['description']:
                    finding['description'] = self._deduplicate_text(finding['description'])
        
        return results

    def _deduplicate_text(self, text):
        """Remove duplicate content from a text string with improved handling for ZAP outputs"""
        if not text or len(text) < 100:  # Only process longer texts
            return text
        
        # Handle ZAP-specific patterns
        if text.startswith('zap: ') and '\n\nzap: ' in text:
            # Split by ZAP marker
            parts = text.split('\n\nzap: ')
            # Prepend 'zap: ' to all but the first part (which already has it)
            parts[0] = parts[0].removeprefix('zap: ')
            
            # Keep only unique parts
            unique_parts = []
            seen = set()
            
            for part in parts:
                # Skip empty parts
                if not part.strip():
                    continue
                    
                # Normalize for comparison
                norm_part = re.sub(r'\s+', ' ', part).strip().lower()
                if norm_part and norm_part not in seen:
                    seen.add(norm_part)
                    unique_parts.append(part)
            
            # Rebuild the text
            if unique_parts:
                return 'zap: ' + '\n\nzap: '.join(unique_parts)
        
        # Default handling for other text    
        if '\n\n' in text:
            separator = '\n\n'
        else:
            separator = '\n'
                
        # Split into paragraphs or lines
        parts = text.split(separator)
        
        # No need to process if very few parts
        if len(parts) <= 2:
            return text
                
        # Keep only unique parts
        unique_parts = []
        seen = set()
        
        for part in parts:
            # Skip empty parts
            if not part.strip():
                continue
                    
            # Normalize for comparison (strip whitespace, lowercase, remove extra spaces)
            norm_part = re.sub(r'\s+', ' ', part).strip().lower()
            if norm_part and norm_part not in seen:
                seen.add(norm_part)
                unique_parts.append(part)
        
        # If we got duplicates, rebuild the text
        if len(unique_parts) < len(parts):
            return separator.join(unique_parts)
        
        # No duplicates found
        return text

class VulnerabilityListView(View):
    def get(self, request):
        try:
            # Get filter parameters
            target = request.GET.get('target')
            severity = request.GET.get('severity')
            vuln_type = request.GET.get('type')
            source = request.GET.get('source')
            include_fixed = request.GET.get('include_fixed', 'false').lower() == 'true'

            # Build query
            query = Vulnerability.objects.all()
            if target:
                query = query.filter(target=target)
            if severity:
                query = query.filter(severity=severity.upper())
            if vuln_type:
                query = query.filter(vuln_type=vuln_type)
            if source:
                query = query.filter(source=source)
            if not include_fixed:
                query = query.filter(is_fixed=False)

            # Get vulnerabilities with selected fields
            vulnerabilities = query.values(
                'id', 'target', 'name', 'description', 'severity',
                'vuln_type', 'evidence', 'source', 'confidence',
                'discovery_date', 'is_fixed', 'fix_date', 'cvss_score'
            ).order_by('-discovery_date')

            return JsonResponse({
                'status': 'success',
                'count': len(vulnerabilities),
                'vulnerabilities': list(vulnerabilities)
            })

        except Exception as e:
            logger.error(f"Error listing vulnerabilities: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)

@method_decorator(csrf_exempt, name='dispatch')
class VulnerabilityUpdateView(View):
    def post(self, request, vuln_id):
        try:
            vulnerability = Vulnerability.objects.get(id=vuln_id)
            data = json.loads(request.body)

            # Update fields
            if 'is_fixed' in data:
                vulnerability.is_fixed = data['is_fixed']
            if 'notes' in data:
                vulnerability.notes = data['notes']
            if 'severity' in data:
                vulnerability.severity = data['severity'].upper()

            vulnerability.save()

            return JsonResponse({
                'status': 'success',
                'vulnerability': {
                    'id': vulnerability.id,
                    'name': vulnerability.name,
                    'is_fixed': vulnerability.is_fixed,
                    'notes': vulnerability.notes,
                    'severity': vulnerability.severity
                }
            })

        except Vulnerability.DoesNotExist:
            return JsonResponse({
                'status': 'error',
                'error': 'Vulnerability not found'
            }, status=404)
        except Exception as e:
            logger.error(f"Error updating vulnerability: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)

class ScannerStatusView(View):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.scanner = UnifiedVulnerabilityScanner()

    def get(self, request):
        try:
            status = self.scanner.get_scanner_status()
            return JsonResponse({
                'status': 'success',
                'scanners': status
            })
        except Exception as e:
            logger.error(f"Error getting scanner status: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)
            
# vulnerability/views.py (Add these views to your existing views.py)

from django.http import JsonResponse
from django.views import View
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from .nuclei_scanner import NucleiScanner
from .models import NucleiFinding
import json
import logging

logger = logging.getLogger(__name__)

@method_decorator(csrf_exempt, name='dispatch')
class NucleiScanView(View):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.scanner = NucleiScanner()

    def post(self, request):
        try:
            data = json.loads(request.body)
            target = data.get('target')
            scan_type = data.get('scan_type', 'basic')  # Default to basic scan
            scan_options = data.get('options', {})

            if not target:
                return JsonResponse({
                    'status': 'error',
                    'error': 'Target is required'
                }, status=400)

            # Choose scan type
            if scan_type == 'basic':
                logger.info(f"Starting basic Nuclei scan for {target}")
                scan_results = self.scanner.run_basic_scan(target)
            elif scan_type == 'advanced':
                logger.info(f"Starting advanced Nuclei scan for {target}")
                scan_results = self.scanner.run_advanced_scan(target, scan_options)
            else:
                logger.info(f"Starting custom Nuclei scan for {target}")
                scan_results = self.scanner.scan_target(target, scan_options)
            
            if scan_results.get('status') == 'success':
                # Prepare response with summary and limited findings
                findings = scan_results.get('findings', [])
                # Limit findings in response to first 10
                limited_findings = findings[:10] if len(findings) > 10 else findings
                
                response_data = {
                    'status': 'success',
                    'message': f'Nuclei scan completed for {target}',
                    'scan_id': scan_results.get('scan_id'),
                    'findings_count': len(findings),
                    'findings': limited_findings,  # Limit to first 10 for response size
                    'summary': scan_results.get('summary'),
                    'note': "Response limited to 10 findings" if len(findings) > 10 else ""
                }
                
                # Save findings to database
                saved_findings = []
                for finding in findings:
                    try:
                        nuclei_finding = NucleiFinding.objects.create(
                            template_id=finding.get('template_id', ''),
                            name=finding.get('name', ''),
                            severity=finding.get('severity', 'unknown').upper(),
                            finding_type=finding.get('type', ''),
                            host=finding.get('host', ''),
                            matched_at=finding.get('matched', ''),
                            description=finding.get('description', ''),
                            tags=finding.get('tags', []),
                            references=finding.get('references', []),
                            cwe=finding.get('cwe', ''),
                            cvss_score=finding.get('cvss_score'),
                            scan_id=scan_results.get('scan_id', ''),
                            target=target
                        )
                        saved_findings.append({
                            'id': nuclei_finding.id,
                            'name': nuclei_finding.name,
                            'severity': nuclei_finding.severity
                        })
                    except Exception as e:
                        logger.error(f"Error saving finding: {str(e)}")
                
                return JsonResponse(response_data)
            else:
                return JsonResponse({
                    'status': 'error',
                    'error': scan_results.get('error', 'Unknown error during scan')
                }, status=500)

        except json.JSONDecodeError:
            return JsonResponse({
                'status': 'error',
                'error': 'Invalid JSON data'
            }, status=400)
        except Exception as e:
            logger.error(f"Error during Nuclei scan: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)
            
class NucleiTemplatesView(View):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.scanner = NucleiScanner()

    def get(self, request):
        """Get information about available Nuclei templates"""
        try:
            template_info = self.scanner.get_template_info()
            return JsonResponse(template_info)
        except Exception as e:
            logger.error(f"Error getting template info: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)

    def post(self, request):
        """Update Nuclei templates"""
        try:
            update_result = self.scanner.update_templates()
            return JsonResponse(update_result)
        except Exception as e:
            logger.error(f"Error updating templates: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)

class NucleiFindingsView(View):
    def get(self, request):
        """Get Nuclei findings with optional filters"""
        try:
            target = request.GET.get('target')
            severity = request.GET.get('severity')
            template_id = request.GET.get('template_id')
            
            query = NucleiFinding.objects.all()
            if target:
                query = query.filter(target=target)
            if severity:
                query = query.filter(severity=severity.upper())
            if template_id:
                query = query.filter(template_id=template_id)
                
            findings = query.values(
                'id', 'template_id', 'name', 'severity',
                'finding_type', 'host', 'matched_at',
                'discovery_date', 'cvss_score'
            ).order_by('-discovery_date')
            
            return JsonResponse({
                'status': 'success',
                'count': len(findings),
                'findings': list(findings)
            })
            
        except Exception as e:
            logger.error(f"Error retrieving Nuclei findings: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)
            
            
# Add to vulnerability/views.py

# Imports needed at the top of views.py
from django.http import JsonResponse
from django.views import View
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from django.db.models import Count
from django.db import transaction
import json
import logging

from .correlation import VulnerabilityCorrelator
from .models import Vulnerability

# Then the CorrelationView class as provided previously
@method_decorator(csrf_exempt, name='dispatch')
class CorrelationView(View):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.correlator = VulnerabilityCorrelator()
        self.logger = logging.getLogger(__name__)

    def post(self, request):
        """Correlate vulnerabilities from different sources"""
        try:
            data = json.loads(request.body)
            
            # Extract parameters
            target = data.get('target')
            
            # Extract findings from different scanners if provided
            internal_results = data.get('internal_results', [])
            zap_results = data.get('zap_results', [])
            nuclei_results = data.get('nuclei_results', [])
            openvas_results = data.get('openvas_results', [])
            manual_results = data.get('manual_results', [])
            
            # Whether to save results to database
            save_to_db = data.get('save_to_db', True)
            
            # Whether to deduplicate existing entries for this target
            deduplicate = data.get('deduplicate', True)
            
            # Validate that at least one scanner has results
            if not any([internal_results, zap_results, nuclei_results, openvas_results, manual_results]):
                return JsonResponse({
                    'status': 'error',
                    'error': 'At least one scanner must provide results'
                }, status=400)

            # Run correlation
            self.logger.info(f"Starting vulnerability correlation for {target}")
            correlation_results = self.correlator.correlate_findings(
                internal_results=internal_results,
                zap_results=zap_results,
                nuclei_results=nuclei_results,
                openvas_results=openvas_results,
                manual_results=manual_results,
                target=target if save_to_db else None  # Only save if requested
            )
            
            # If requested, deduplicate existing entries for this target
            if deduplicate and target and save_to_db:
                self._deduplicate_vulnerabilities(target)
            
            return JsonResponse(correlation_results)

        except json.JSONDecodeError:
            self.logger.error("Invalid JSON in request body")
            return JsonResponse({
                'status': 'error',
                'error': 'Invalid JSON data'
            }, status=400)
        except Exception as e:
            self.logger.exception(f"Error during correlation: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)
            
    def get(self, request):
        """Get correlated vulnerabilities for a target with enhanced filtering"""
        try:
            target = request.GET.get('target')
            if not target:
                return JsonResponse({
                    'status': 'error',
                    'error': 'Target parameter is required'
                }, status=400)
                
            # Additional filter parameters
            severity = request.GET.get('severity')
            source = request.GET.get('source')
            show_fixed = request.GET.get('show_fixed', 'false').lower() == 'true'
            correlated_only = request.GET.get('correlated_only', 'false').lower() == 'true'
            deduplicate = request.GET.get('deduplicate', 'true').lower() == 'true'
            
            # Deduplicate vulnerabilities if requested
            if deduplicate:
                deduplication_result = self._deduplicate_vulnerabilities(target)
                
            # Build query
            query = Vulnerability.objects.filter(target=target)
            
            if severity:
                query = query.filter(severity=severity.upper())
                
            if source:
                # Filter for vulnerabilities that contain this source
                query = query.filter(source__icontains=source)
                
            if not show_fixed:
                query = query.filter(is_fixed=False)
                
            # Get all matching vulnerabilities
            vulnerabilities = list(query)
            
            # Post-process for correlated findings if requested
            if correlated_only:
                vulnerabilities = [v for v in vulnerabilities if ',' in v.source]
            
            # Generate correlation statistics
            correlated_count = sum(1 for v in vulnerabilities if ',' in v.source)
            
            # Count by source
            source_stats = {}
            for vuln in vulnerabilities:
                for source in vuln.source.split(','):
                    source_stats[source] = source_stats.get(source, 0) + 1
            
            # Count by severity
            severity_stats = {
                'CRITICAL': sum(1 for v in vulnerabilities if v.severity == 'CRITICAL'),
                'HIGH': sum(1 for v in vulnerabilities if v.severity == 'HIGH'),
                'MEDIUM': sum(1 for v in vulnerabilities if v.severity == 'MEDIUM'),
                'LOW': sum(1 for v in vulnerabilities if v.severity == 'LOW')
            }
            
            # Serialize vulnerabilities for response
            vuln_data = []
            for vuln in vulnerabilities:
                vuln_data.append({
                    'id': vuln.id,
                    'name': vuln.name,
                    'description': vuln.description,
                    'severity': vuln.severity,
                    'vuln_type': vuln.vuln_type,
                    'source': vuln.source,
                    'sources': vuln.source.split(','),  # Parsed as list for convenience
                    'confidence': vuln.confidence,
                    'discovery_date': vuln.discovery_date.isoformat(),
                    'is_fixed': vuln.is_fixed,
                    'cvss_score': vuln.cvss_score,
                    'is_correlated': ',' in vuln.source
                })
            
            return JsonResponse({
                'status': 'success',
                'target': target,
                'total_vulnerabilities': len(vulnerabilities),
                'correlated_vulnerabilities': correlated_count,
                'source_statistics': source_stats,
                'severity_statistics': severity_stats,
                'filters_applied': {
                    'severity': severity,
                    'source': source,
                    'show_fixed': show_fixed,
                    'correlated_only': correlated_only
                },
                'vulnerabilities': vuln_data
            })
            
        except Exception as e:
            self.logger.error(f"Error retrieving correlated vulnerabilities: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)
    
    def _deduplicate_vulnerabilities(self, target):
        """
        Deduplicate vulnerabilities for a target by merging duplicates
        Returns statistics about the deduplication
        """
        try:
            start_count = Vulnerability.objects.filter(target=target).count()
            self.logger.info(f"Starting deduplication for {target} with {start_count} vulnerabilities")
            
            # Process batch by batch to avoid memory issues with large datasets
            # Group by name, vuln_type and severity
            deduplication_count = 0
            
            with transaction.atomic():
                # Get all distinct vulnerability signatures
                vulnerability_groups = Vulnerability.objects.filter(
                    target=target
                ).values('name', 'vuln_type', 'severity').annotate(
                    count=Count('id')
                ).filter(count__gt=1)  # Only process groups with duplicates
                
                for group in vulnerability_groups:
                    # Get all vulnerabilities matching this signature
                    duplicates = Vulnerability.objects.filter(
                        target=target,
                        name=group['name'],
                        vuln_type=group['vuln_type'],
                        severity=group['severity']
                    ).order_by('discovery_date')
                    
                    # Skip if only one vulnerability (should never happen due to our filter above)
                    if duplicates.count() <= 1:
                        continue
                        
                    # Keep the first vulnerability (oldest) and merge sources from the rest
                    base_vuln = duplicates.first()
                    sources = set(base_vuln.source.split(','))
                    references = set(base_vuln.references)
                    all_metadata = base_vuln.metadata.copy() if base_vuln.metadata else {}
                    
                    # Track all merged IDs for logging
                    merged_ids = []
                    
                    # Process all but the first vulnerability
                    for dup in duplicates[1:]:
                        # Add sources
                        for source in dup.source.split(','):
                            sources.add(source)
                        
                        # Add references
                        for ref in dup.references:
                            references.add(ref)
                        
                        # Merge metadata
                        if dup.metadata:
                            for key, value in dup.metadata.items():
                                if key not in all_metadata:
                                    all_metadata[key] = value
                                elif key == 'merged_ids':
                                    # Combine merged_ids lists
                                    all_metadata[key] = list(set(all_metadata[key] + value))
                        
                        # Track this ID as merged
                        merged_ids.append(dup.id)
                        
                        # Delete the duplicate
                        dup.delete()
                        deduplication_count += 1
                    
                    # Update the base vulnerability with merged data
                    base_vuln.source = ','.join(sorted(sources))
                    base_vuln.references = list(references)
                    
                    # Add merged_ids to metadata
                    if 'merged_ids' not in all_metadata:
                        all_metadata['merged_ids'] = []
                    all_metadata['merged_ids'].extend(merged_ids)
                    all_metadata['merged_ids'] = list(set(all_metadata['merged_ids']))
                    
                    # Update metadata
                    base_vuln.metadata = all_metadata
                    
                    # Save changes
                    base_vuln.save()
            
            end_count = Vulnerability.objects.filter(target=target).count()
            reduction = start_count - end_count
            
            self.logger.info(f"Deduplication completed: {reduction} duplicates merged")
            
            return {
                'status': 'success',
                'target': target,
                'original_count': start_count,
                'final_count': end_count,
                'duplicates_merged': reduction
            }
            
        except Exception as e:
            self.logger.error(f"Error during deduplication: {str(e)}")
            return {
                'status': 'error',
                'error': str(e)
            }
            
# Add this to your vulnerability/views.py file

@method_decorator(csrf_exempt, name='dispatch')
class NucleiDiagnosticView(View):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        try:
            from .nuclei_scanner import NucleiScanner
            self.scanner = NucleiScanner()
        except Exception as e:
            self.scanner = None
            self.error = str(e)

    def get(self, request):
        """Run diagnostics on the Nuclei scanner setup"""
        if not self.scanner:
            return JsonResponse({
                'status': 'error',
                'error': f"Could not initialize Nuclei scanner: {getattr(self, 'error', 'Unknown error')}"
            }, status=500)
            
        try:
            diagnostic_results = self.scanner.diagnose()
            return JsonResponse({
                'status': 'success',
                'diagnostic_results': diagnostic_results
            })
        except Exception as e:
            logger.error(f"Error running Nuclei diagnostics: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)from typing import Dict, List
import logging
from datetime import datetime
from .scanner import VulnerabilityScanner
from .zap_manager import ZAPManager
from .models import Vulnerability, NucleiFinding
from .nuclei_scanner import NucleiScanner
from .correlation import VulnerabilityCorrelator  # Import the enhanced correlator

class UnifiedVulnerabilityScanner:
    # Define valid scan types
    VALID_SCAN_TYPES = {
        'quick': 'Fast scan with basic checks',
        'standard': 'Standard comprehensive scan',
        'full': 'Full detailed scan with all checks'
    }

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.internal_scanner = VulnerabilityScanner()
        self.zap_manager = ZAPManager()
        self.correlator = VulnerabilityCorrelator()  # Initialize the correlator
        try:
            self.nuclei_scanner = NucleiScanner()
            self.logger.info("Successfully initialized Nuclei scanner")
        except Exception as e:
            self.logger.error(f"Failed to initialize Nuclei scanner: {str(e)}")
            self.nuclei_scanner = None
            # Don't raise the exception, just log it and continue with other scanners

    def validate_scan_type(self, scan_type: str) -> str:
        """Validate scan type and return normalized version"""
        if not scan_type:
            return 'standard'  # Default scan type
            
        normalized_type = scan_type.lower().strip()
        if normalized_type not in self.VALID_SCAN_TYPES:
            raise ValueError(
                f"Invalid scan type: '{scan_type}'. Valid types are: {list(self.VALID_SCAN_TYPES.keys())}"
            )
        return normalized_type

    def get_scan_types(self) -> Dict[str, str]:
        """Return available scan types and their descriptions"""
        return self.VALID_SCAN_TYPES

    def scan_target(self, target: str, scan_type: str = 'standard', 
                   include_zap: bool = True, include_nuclei: bool = True,
                   nuclei_scan_type: str = 'basic',
                   use_advanced_correlation: bool = True) -> Dict:
        """
        Perform a comprehensive scan using multiple scanners based on parameters
        """
        try:
            # Validate scan type first
            validated_scan_type = self.validate_scan_type(scan_type)
            
            results = {
                'target': target,
                'scan_start': datetime.now().isoformat(),
                'vulnerabilities': [],
                'scanners_used': ['internal'],
                'summary': {
                    'high': 0,
                    'medium': 0,
                    'low': 0,
                    'total': 0
                },
                'correlation': {}
            }

            # Track findings from each scanner for correlation
            internal_results = []
            zap_results = []
            nuclei_results = []

            # Run internal scanner with validated scan type
            self.logger.info(f"Starting internal scanner with scan type: {validated_scan_type}")
            internal_scan = self.internal_scanner.scan_target(target, validated_scan_type)
            
            if internal_scan.get('vulnerabilities'):
                internal_results = internal_scan['vulnerabilities']
                results['scanners_used'].append('internal')
                self.logger.info(f"Internal scanner found {len(internal_results)} vulnerabilities")

            # Run ZAP scan if requested
            if include_zap:
                self.logger.info("Starting ZAP scanner")
                if self.zap_manager.ensure_zap_running():
                    results['scanners_used'].append('zap')
                    zap_scan = self.zap_manager.run_scan(target)
                    if zap_scan.get('status') == 'success' and zap_scan.get('alerts'):
                        zap_results = zap_scan['alerts']
                        self.logger.info(f"ZAP scanner found {len(zap_results)} vulnerabilities")
                else:
                    self.logger.warning("ZAP scanner not available")

            # Run Nuclei scan if requested
            # In the scan_target method of UnifiedVulnerabilityScanner class:
            # Run Nuclei scan if requested
            if include_nuclei:
                if self.nuclei_scanner:
                    try:
                        self.logger.info(f"Starting Nuclei scan with type: {nuclei_scan_type}")
                        if nuclei_scan_type.lower() == 'advanced':
                            nuclei_scan = self.nuclei_scanner.run_advanced_scan(target)
                        else:
                            nuclei_scan = self.nuclei_scanner.run_basic_scan(target)
                        
                        if nuclei_scan.get('status') == 'success' and nuclei_scan.get('findings'):
                            results['scanners_used'].append('nuclei')
                            nuclei_results = nuclei_scan['findings']
                            self.logger.info(f"Nuclei scanner found {len(nuclei_results)} vulnerabilities")
                        else:
                            self.logger.warning(f"Nuclei scan completed but returned no findings or had an error: {nuclei_scan.get('error', 'No error specified')}")
                    except Exception as e:
                        self.logger.error(f"Nuclei scan failed: {str(e)}")
                        self.logger.error("Continuing with other scanners")
                else:
                    self.logger.warning("Nuclei scanner not available or failed to initialize")

            # Use enhanced correlation if enabled
            if use_advanced_correlation:
                self.logger.info("Using advanced correlation for findings")
                
                # Normalize target before passing to correlator
                normalized_target = self._normalize_target(target)
                
                correlation_result = self.correlator.correlate_findings(
                    internal_results=internal_results,
                    zap_results=zap_results,
                    nuclei_results=nuclei_results,
                    target=normalized_target  # Pass normalized target
                )
                              
                if correlation_result.get('status') == 'success':
                    results['vulnerabilities'] = correlation_result.get('findings', [])
                    results['correlation'] = {
                        'original_count': correlation_result.get('original_count', 0),
                        'correlated_count': correlation_result.get('correlated_count', 0),
                        'reduction_percentage': correlation_result.get('statistics', {}).get('reduction_percentage', 0),
                        'stats': correlation_result.get('statistics', {})
                    }
                    self.logger.info(f"Correlation reduced findings from {correlation_result.get('original_count', 0)} to {correlation_result.get('correlated_count', 0)}")
                else:
                    # Fallback to basic processing if correlation failed
                    self.logger.warning("Advanced correlation failed, falling back to basic processing")
                    self._process_scanner_results(results, internal_results, zap_results, nuclei_results)
            else:
                # Use basic processing
                self.logger.info("Using basic processing for findings")
                self._process_scanner_results(results, internal_results, zap_results, nuclei_results)
            
            # Update summary
            self._update_summary(results)
            
            # Add scan configuration to results
            results['scan_config'] = {
                'scan_type': validated_scan_type,
                'scanners': {
                    'internal': True,
                    'zap': include_zap,
                    'nuclei': {
                        'enabled': include_nuclei,
                        'type': nuclei_scan_type if include_nuclei else 'disabled'
                    }
                },
                'correlation': {
                    'advanced': use_advanced_correlation
                }
            }
            
            results['scan_end'] = datetime.now().isoformat()
            results['status'] = 'success'
            
            # Deduplicate vulnerabilities
            deduplication_stats = Vulnerability.deduplicate_vulnerabilities(target)
            
            # Add deduplication stats to results
            results['deduplication_stats'] = deduplication_stats
            
            return results

        except ValueError as e:
            # Handle validation errors
            error_msg = str(e)
            self.logger.error(f"Validation error: {error_msg}")
            return {
                'status': 'error',
                'error': error_msg,
                'valid_scan_types': list(self.VALID_SCAN_TYPES.keys())
            }
        except Exception as e:
            self.logger.error(f"Unified scan failed: {str(e)}")
            return {
                'status': 'error',
                'error': str(e)
            }
            
# Modify how vulnerabilities are processed:

    def _process_scanner_results(self, results: Dict, internal_results: List, 
                                zap_results: List, nuclei_results: List) -> None:
        """Process results from different scanners using basic approach"""
        
        # Normalize the target - extract just the domain from the URL
        target = results.get('target', '')
        if '://' in target:
            from urllib.parse import urlparse
            parsed = urlparse(target)
            normalized_target = parsed.netloc
        else:
            normalized_target = target
        
        # Process internal scanner results with normalized target
        for vuln in internal_results:
            results['vulnerabilities'].append({
                'source': 'internal',
                'name': vuln.get('name', ''),
                'description': vuln.get('description', ''),
                'severity': vuln.get('severity', 'LOW'),
                'type': vuln.get('type', ''),
                'evidence': vuln.get('evidence', ''),
                'confidence': vuln.get('confidence', 'medium'),
                'cvss_score': vuln.get('cvss', 0.0),
                'target': normalized_target  # Use normalized target
            })

        # Process ZAP scanner results with normalized target
        for alert in zap_results:
            results['vulnerabilities'].append({
                'source': 'zap',
                'name': alert.get('name', ''),
                'description': alert.get('description', ''),
                'severity': self._normalize_severity(alert.get('risk')),
                'type': 'web',
                'evidence': alert.get('evidence', ''),
                'confidence': alert.get('confidence', 'medium'),
                'solution': alert.get('solution', ''),
                'cwe': alert.get('cweid', ''),
                'target': normalized_target,  # Use normalized target
                'metadata': {
                    'url': alert.get('url', ''),
                    'parameter': alert.get('parameter', '')
                }
            })

        # Process Nuclei scanner results with normalized target
        for finding in nuclei_results:
            results['vulnerabilities'].append({
                'source': 'nuclei',
                'name': finding.get('name', ''),
                'description': finding.get('description', ''),
                'severity': finding.get('severity', 'LOW'),
                'type': finding.get('type', 'nuclei'),
                'evidence': finding.get('evidence', ''),
                'confidence': 'high',
                'cvss_score': finding.get('cvss_score', 0.0),
                'cwe': finding.get('cwe', ''),
                'references': finding.get('references', []),
                'target': normalized_target,  # Use normalized target
                'metadata': {
                    'template_id': finding.get('template_id', ''),
                    'tags': finding.get('tags', []),
                    'matched_at': finding.get('matched', ''),
                    'host': finding.get('host', '')
                }
            })
        
    def _basic_deduplicate(self, vulnerabilities: List[Dict]) -> List[Dict]:
        """Simple deduplication of vulnerabilities by name and severity"""
        unique_vulns = {}
        
        for vuln in vulnerabilities:
            # Create a key for deduplication
            key = f"{vuln['name']}_{vuln['severity']}"
            
            if key in unique_vulns:
                existing = unique_vulns[key]
                # Merge sources
                sources = set([existing['source']])
                sources.add(vuln['source'])
                existing['source'] = ','.join(sources)
                # Take highest confidence
                if vuln.get('confidence') == 'high':
                    existing['confidence'] = 'high'
            else:
                unique_vulns[key] = vuln
                
        return list(unique_vulns.values())

    def _normalize_severity(self, severity: str) -> str:
        """Normalize severity ratings across different scanners"""
        severity = str(severity).lower()
        
        if severity in ['critical', 'high', '3', '4']:
            return 'HIGH'
        elif severity in ['medium', 'warning', '2']:
            return 'MEDIUM'
        elif severity in ['low', 'info', '1']:
            return 'LOW'
        return 'INFO'

    def _update_summary(self, results: Dict) -> None:
        """Update the summary counts"""
        summary = {'high': 0, 'medium': 0, 'low': 0, 'total': 0}
        
        for vuln in results['vulnerabilities']:
            severity = vuln['severity'].lower()
            if severity in summary:
                summary[severity] += 1
            summary['total'] += 1

        results['summary'] = summary

    def get_scanner_status(self) -> Dict:
        """Get status of all scanners"""
        status = {
            'internal': {
                'status': 'available',
                'checks': list(self.internal_scanner.checks.keys())
            },
            'zap': self.zap_manager.get_status()
        }

        # Add Nuclei status if initialized
        if self.nuclei_scanner:
            try:
                nuclei_info = self.nuclei_scanner.get_template_info()
                status['nuclei'] = {
                    'status': 'available',
                    'templates': nuclei_info.get('total_templates', 0),
                    'template_types': nuclei_info.get('template_types', {})
                }
            except Exception as e:
                status['nuclei'] = {
                    'status': 'error',
                    'error': str(e)
                }

        return status

    def _save_findings(self, vulnerabilities: List[Dict], target: str) -> None:
        """Save findings to database"""
        for vuln in vulnerabilities:
            # Extract metadata
            metadata = {
                'url': vuln.get('url'),
                'parameter': vuln.get('parameter'),
                'extra_info': vuln.get('metadata', {})
            }
            
            try:
                Vulnerability.objects.create(
                    target=target,
                    name=vuln['name'],
                    description=vuln.get('description', ''),
                    severity=vuln['severity'],
                    vuln_type=vuln.get('type', 'unknown'),
                    evidence=vuln.get('evidence', ''),
                    source=vuln['source'],
                    confidence=vuln['confidence'],
                    solution=vuln.get('solution', ''),
                    cwe=vuln.get('cwe', ''),
                    cvss_score=vuln.get('cvss_score'),
                    references=vuln.get('references', []),
                    metadata=metadata
                )
            except Exception as e:
                self.logger.error(f"Error saving vulnerability: {str(e)}")
                self.logger.error(f"Vulnerability data: {vuln}")
                
    # File: vulnerability/unified_scanner.py
# Add a method to match vulnerabilities with exploits

    def match_vulnerabilities_with_exploits(self, vulnerabilities: list) -> dict:
        """
        Match vulnerabilities with potential exploits
        
        Args:
            vulnerabilities: List of vulnerability dictionaries
            
        Returns:
            dict: Statistics about matches
        """
        try:
            from exploit_manager.matcher import ExploitMatcher
            from vulnerability.models import Vulnerability
            
            matcher = ExploitMatcher()
            stats = {
                'total_vulnerabilities': len(vulnerabilities),
                'vulnerabilities_with_matches': 0,
                'total_matches': 0
            }
            
            # Process each vulnerability
            for vuln_data in vulnerabilities:
                try:
                    # Get vulnerability from database
                    vuln_id = vuln_data.get('id')
                    if not vuln_id:
                        continue
                        
                    vulnerability = Vulnerability.objects.get(id=vuln_id)
                    
                    # Find matches
                    matches = matcher.match_vulnerability(vulnerability)
                    
                    if matches:
                        stats['vulnerabilities_with_matches'] += 1
                        stats['total_matches'] += len(matches)
                        
                except Exception as e:
                    self.logger.error(f"Error matching vulnerability: {str(e)}")
                    
            return stats
            
        except Exception as e:
            self.logger.error(f"Error in match_vulnerabilities_with_exploits: {str(e)}")
            return {
                'total_vulnerabilities': len(vulnerabilities),
                'vulnerabilities_with_matches': 0,
                'total_matches': 0,
                'error': str(e)
            }
        


    def _normalize_target(self, target: str) -> str:
        """Normalize target URL to consistent format"""
        if not target:
            return ""
            
        # Remove protocol prefix
        if '://' in target:
            target = target.split('://', 1)[1]
        
        # Remove path, trailing slash, etc.
        if '/' in target:
            target = target.split('/', 1)[0]
            
        # Remove port if present
        if ':' in target:
            target = target.split(':', 1)[0]
            
        # Remove 'www.' prefix if present
        if target.startswith('www.'):
            target = target[4:]
            
        return target.lower()
            

                
    import subprocess
import json
import logging
import os
import re
import shutil
from typing import Dict, List, Optional, Tuple
from pathlib import Path
from datetime import datetime
from django.conf import settings

class NucleiScanner:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # Define Nuclei paths with better error handling
        self.go_path = str(Path.home() / "go")  # More reliable GOPATH
        
        # Try to find nuclei binary in multiple locations
        self.nuclei_path = self._find_nuclei_binary()
        self.logger.info(f"Using Nuclei at: {self.nuclei_path}")
        
        # Set up directories with explicit creation
        self.base_dir = Path(settings.BASE_DIR)
        self.results_dir = self.base_dir / "vulnerability" / "nuclei-results"
        self.debug_dir = self.base_dir / "vulnerability" / "debug_logs"
        
        # Create directories if they don't exist
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.debug_dir.mkdir(parents=True, exist_ok=True)
        
        # Set up environment with Go paths
        self.env = os.environ.copy()
        self.env["PATH"] = f"{self.go_path}/bin:{self.env.get('PATH', '')}"
        self.env["GOPATH"] = self.go_path
        
        # Verify installation
        if not self._verify_nuclei_installation():
            error_msg = (
                "Nuclei not found. Please install using:\n"
                "1. go install -v github.com/projectdiscovery/nuclei/v2/cmd/nuclei@latest\n"
                f"2. Verify installation at: {self.nuclei_path}"
            )
            raise Exception(error_msg)

    def _find_nuclei_binary(self) -> str:
        """Find the nuclei binary in various possible locations"""
        possible_locations = [
            str(Path.home() / "go" / "bin" / "nuclei"),  # Default Go installation
            "/usr/local/bin/nuclei",                     # Common system location
            "/usr/bin/nuclei",                          # Alternative system location
            str(Path(settings.BASE_DIR) / "bin" / "nuclei")  # Project-specific location
        ]
        
        # First check if nuclei is in PATH
        try:
            which_result = subprocess.run(["which", "nuclei"], capture_output=True, text=True)
            if which_result.returncode == 0 and which_result.stdout.strip():
                return which_result.stdout.strip()
        except Exception:
            pass  # Continue with other checks if 'which' fails
        
        # Check each possible location
        for location in possible_locations:
            if os.path.isfile(location) and os.access(location, os.X_OK):
                return location
        
        # Default to the standard location even if it doesn't exist yet
        return str(Path.home() / "go" / "bin" / "nuclei")

    def _verify_nuclei_installation(self) -> bool:
        """Verify Nuclei is installed and working"""
        try:
            result = subprocess.run(
                [self.nuclei_path, '-version'],
                capture_output=True,
                text=True,
                env=self.env,
                timeout=10
            )
            
            if result.stderr and 'INF' in result.stderr:
                version_line = result.stderr.splitlines()[0] if result.stderr.splitlines() else "Unknown version"
                self.logger.info(f"Nuclei version verified: {version_line}")
                if "outdated" in result.stderr:
                    self.logger.warning("Nuclei is outdated. Consider updating for better results.")
            
            # Update templates if needed
            try:
                self.update_templates()
            except Exception as e:
                self.logger.warning(f"Templates update failed but continuing: {str(e)}")
            
            return result.returncode == 0
        except Exception as e:
            self.logger.error(f"Error verifying nuclei: {str(e)}")
            return False

    def scan_target(self, target: str, scan_options: Optional[Dict] = None, 
                additional_flags: Optional[List[str]] = None, timeout: int = 300) -> Dict:
        """Core scanning method used by both basic and advanced scans"""
        try:
            # Clean target URL
            if not target.startswith(('http://', 'https://')):
                target = f"http://{target}"
                
            # Generate scan ID and output file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_target = re.sub(r'[^\w\-_]', '_', target)
            scan_id = f"scan_{timestamp}_{safe_target}"
            output_file = self.results_dir / f"{scan_id}.json"
            
            self.logger.info(f"Starting Nuclei scan {scan_id} for target: {target}")
            
            # Build base command
            cmd = [
                self.nuclei_path,
                "-target", target,
                "-j",               # JSON output
                "-o", str(output_file),
                "-stats",           # Show statistics
                "-duc",            # Disable update check
                "-no-color",        # No color output
                "-silent"           # Reduce output verbosity
            ]

            # Add templates path if auto-scan not supported
            if not self._check_as_flag_support():
                templates_path = self._get_templates_path()
                if templates_path:
                    cmd.extend(["-t", templates_path])
                else:
                    templates_path = str(Path.home() / "nuclei-templates" / "http" / "vulnerabilities")
                    cmd.extend(["-t", templates_path])
                    
                # Add severity levels
                cmd.extend(["-severity", "critical,high,medium,low"])

            # Add any additional flags
            if additional_flags:
                cmd.extend(additional_flags)

            # Log command to debug file instead of console
            debug_log_file = self.debug_dir / f"nuclei_cmd_{scan_id}.log"
            with open(debug_log_file, 'w') as f:
                f.write(f"Command: {' '.join(cmd)}\n")
            
            # Run the scan
            process = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                env=self.env,
                timeout=timeout
            )
            
            # Log stderr and stdout to debug file instead of console
            if process.stderr or process.stdout:
                with open(debug_log_file, 'a') as f:
                    if process.stderr:
                        f.write(f"\nSTDERR:\n{process.stderr}\n")
                    if process.stdout:
                        f.write(f"\nSTDOUT:\n{process.stdout}\n")
            
            # Process results
            findings = []
            if output_file.exists():
                try:
                    with open(output_file) as f:
                        content = f.read().strip()
                        if content:
                            for line in content.split('\n'):
                                try:
                                    if line.strip():
                                        finding = json.loads(line)
                                        findings.append(finding)
                                except json.JSONDecodeError:
                                    continue
                except Exception as e:
                    self.logger.error(f"Error reading output file: {str(e)}")

            processed_findings = self._process_findings(findings)
            
            # Log summary of findings instead of each individual finding
            summary = self._generate_summary(processed_findings)
            self.logger.info(f"Nuclei scan complete: {summary['total_findings']} findings ({', '.join([f'{sev}: {count}' for sev, count in summary['severity_distribution'].items() if count > 0])})")
            
            return {
                "status": "success",
                "target": target,
                "scan_id": scan_id,
                "timestamp": datetime.now().isoformat(),
                "findings": processed_findings,
                "summary": summary
            }

        except Exception as e:
            self.logger.error(f"Error during Nuclei scan: {str(e)}")
            return {
                "status": "error",
                "error": str(e)
            }

    def _check_as_flag_support(self) -> bool:
        """Check if -as flag (auto scan) is supported in this nuclei version"""
        try:
            help_output = subprocess.run(
                [self.nuclei_path, "-help"],
                capture_output=True,
                text=True,
                env=self.env,
                timeout=5
            )
            return "-as" in (help_output.stdout or "") or "-as" in (help_output.stderr or "")
        except Exception:
            return False

    def _get_templates_path(self) -> Optional[str]:
        """Find valid nuclei templates directory"""
        # Try standard location first
        home_templates = str(Path.home() / "nuclei-templates")
        if os.path.isdir(home_templates):
            return home_templates
            
        # Try specific vulnerability templates
        vuln_templates = str(Path.home() / "nuclei-templates" / "http" / "vulnerabilities")
        if os.path.isdir(vuln_templates):
            return vuln_templates
            
        # Check if templates are in the project directory
        project_templates = str(Path(settings.BASE_DIR) / "nuclei-templates")
        if os.path.isdir(project_templates):
            return project_templates
            
        return None

    def run_basic_scan(self, target: str) -> Dict:
        """Run a basic vulnerability scan using auto-scan mode with improved error handling"""
        try:
            # Clean target URL
            if not target.startswith(('http://', 'https://')):
                target = f"http://{target}"
                
            # Generate scan ID and output file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_target = re.sub(r'[^\w\-_]', '_', target)
            scan_id = f"nuclei_basic_scan_{timestamp}_{safe_target}"
            output_file = self.results_dir / f"{scan_id}.json"
            debug_log_file = self.debug_dir / f"{scan_id}.log"
            
            self.logger.info(f"Starting basic Nuclei scan for {target}")
            
            # Check if auto-scan is supported
            as_flag_supported = self._check_as_flag_support()
            
            # Build command
            cmd = [
                self.nuclei_path,
                "-target", target,
                "-j",               # JSON output
                "-o", str(output_file),
                "-stats",           # Show statistics
                "-silent"           # Reduce output verbosity
            ]
            
            # Add auto-scan mode if available, otherwise use templates
            if as_flag_supported:
                cmd.extend(["-as"])  # Automatic scan mode
            else:
                templates_path = self._get_templates_path()
                if templates_path:
                    cmd.extend(["-t", templates_path])
                    cmd.extend(["-severity", "critical,high,medium,low"])
                else:
                    self.logger.warning("No templates path found, scan may not work properly")
                    cmd.extend(["-t", "http"])  # Fallback to built-in http templates

            # Log command to debug file
            with open(debug_log_file, 'w') as f:
                f.write(f"Command: {' '.join(cmd)}\n")
                f.write(f"Auto-scan mode supported: {as_flag_supported}\n")
                f.write(f"Templates path: {self._get_templates_path() or 'Not found'}\n")
            
            # Run the scan with timeout
            process = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                env=self.env,
                timeout=300
            )
            
            # Log stderr and stdout to debug file
            with open(debug_log_file, 'a') as f:
                if process.stderr:
                    f.write(f"\nSTDERR:\n{process.stderr}\n")
                if process.stdout:
                    f.write(f"\nSTDOUT:\n{process.stdout}\n")
            
            # Process results
            findings = []
            if output_file.exists():
                try:
                    with open(output_file) as f:
                        content = f.read().strip()
                        if content:
                            for line in content.split('\n'):
                                try:
                                    if line.strip():
                                        finding = json.loads(line)
                                        findings.append(finding)
                                except json.JSONDecodeError:
                                    continue
                except Exception as e:
                    self.logger.error(f"Error reading output file: {str(e)}")

            processed_findings = self._process_findings(findings)
            
            # Log summary of findings
            summary = self._generate_summary(processed_findings)
            self.logger.info(f"Basic Nuclei scan complete: {summary['total_findings']} findings ({', '.join([f'{sev}: {count}' for sev, count in summary['severity_distribution'].items() if count > 0])})")
            
            return {
                "status": "success",
                "target": target,
                "scan_id": scan_id,
                "timestamp": datetime.now().isoformat(),
                "findings": processed_findings,
                "raw_output_file": str(debug_log_file),
                "summary": summary
            }

        except Exception as e:
            self.logger.error(f"Error during basic Nuclei scan: {str(e)}")
            return {
                "status": "error",
                "error": str(e)
            }

    def run_advanced_scan(self, target: str, options: Optional[Dict] = None) -> Dict:
        """Run a more comprehensive scan with additional options"""
        try:
            # Clean target URL
            if not target.startswith(('http://', 'https://')):
                target = f"http://{target}"
                
            # Generate scan ID and output file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_target = re.sub(r'[^\w\-_]', '_', target)
            scan_id = f"nuclei_advanced_scan_{timestamp}_{safe_target}"
            output_file = self.results_dir / f"{scan_id}.json"
            
            self.logger.info(f"Starting advanced Nuclei scan for {target}")
            
            # Extended command with additional options
            cmd = [
                self.nuclei_path,
                "-target", target,
                "-j",               # JSON output
                "-o", str(output_file),
                "-stats",           # Show statistics
                "-c", "50",         # Increased concurrency
                "-timeout", "15",   # Extended timeout
                "-retries", "2",    # Retry failed requests
                "-silent"           # Reduce output verbosity
            ]
            
            # Add auto-scan if supported, otherwise use templates
            as_flag_supported = self._check_as_flag_support()
            if as_flag_supported:
                cmd.extend(["-as"])  # Automatic scan mode
            else:
                templates_path = self._get_templates_path()
                if templates_path:
                    cmd.extend(["-t", templates_path])
                    cmd.extend(["-severity", "critical,high,medium,low"])
                else:
                    cmd.extend(["-t", "http"])  # Fallback to built-in http templates

            # Log command to debug file instead of console
            debug_log_file = self.debug_dir / f"{scan_id}.log"
            with open(debug_log_file, 'w') as f:
                f.write(f"Command: {' '.join(cmd)}\n")
                f.write(f"Auto-scan mode supported: {as_flag_supported}\n")
                f.write(f"Templates path: {self._get_templates_path() or 'Not found'}\n")
            
            # Run the scan with longer timeout
            process = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                env=self.env,
                timeout=600  # 10 minutes
            )
            
            # Log stderr and stdout to debug file
            with open(debug_log_file, 'a') as f:
                if process.stderr:
                    f.write(f"\nSTDERR:\n{process.stderr}\n")
                if process.stdout:
                    f.write(f"\nSTDOUT:\n{process.stdout}\n")
            
            # Process results
            findings = []
            if output_file.exists():
                try:
                    with open(output_file) as f:
                        content = f.read().strip()
                        if content:
                            for line in content.split('\n'):
                                try:
                                    if line.strip():
                                        finding = json.loads(line)
                                        findings.append(finding)
                                except json.JSONDecodeError:
                                    continue
                except Exception as e:
                    self.logger.error(f"Error reading output file: {str(e)}")

            processed_findings = self._process_findings(findings)
            
            # Log summary of findings
            summary = self._generate_summary(processed_findings)
            self.logger.info(f"Advanced Nuclei scan complete: {summary['total_findings']} findings ({', '.join([f'{sev}: {count}' for sev, count in summary['severity_distribution'].items() if count > 0])})")
            
            return {
                "status": "success",
                "target": target,
                "scan_id": scan_id,
                "timestamp": datetime.now().isoformat(),
                "findings": processed_findings,
                "raw_output_file": str(debug_log_file),
                "summary": summary
            }

        except Exception as e:
            self.logger.error(f"Error during advanced Nuclei scan: {str(e)}")
            return {
                "status": "error",
                "error": str(e)
            }

    def _process_findings(self, findings: List[Dict]) -> List[Dict]:
        """Process and normalize Nuclei findings with CVSS handling"""
        processed = []
        
        # Severity to CVSS base mapping
        severity_cvss = {
            'CRITICAL': 9.5,
            'HIGH': 7.5,
            'MEDIUM': 5.0,
            'LOW': 2.5,
            'INFO': 0.0,
            'unknown': 0.0
        }

        for finding in findings:
            try:
                # Get info block
                info = finding.get("info", {})
                if not info and "name" in finding:
                    info = finding

                # Extract matched data
                matched_data = ""
                if "matcher-name" in finding:
                    matched_data = f"Matcher: {finding['matcher-name']}"
                if "matched-at" in finding:
                    matched_data += f"\nMatched at: {finding['matched-at']}"
                if "extracted-results" in finding:
                    matched_data += f"\nExtracted: {finding['extracted-results']}"

                # Determine CVSS score
                cvss_score = None
                if 'classification' in info and 'cvss-score' in info['classification']:
                    try:
                        cvss_score = float(info['classification']['cvss-score'])
                    except (ValueError, TypeError):
                        # Use severity-based score if CVSS parse fails
                        severity = info.get('severity', 'unknown').upper()
                        cvss_score = severity_cvss.get(severity, 0.0)
                else:
                    # Use severity-based score if no CVSS provided
                    severity = info.get('severity', 'unknown').upper()
                    cvss_score = severity_cvss.get(severity, 0.0)

                processed.append({
                    "template_id": finding.get("template-id", ""),
                    "name": info.get("name", "Unknown Finding"),
                    "severity": info.get("severity", "unknown").upper(),
                    "type": finding.get("type", "unknown"),
                    "host": finding.get("host", ""),
                    "matched": finding.get("matched-at", ""),
                    "evidence": matched_data,
                    "description": info.get("description", ""),
                    "tags": info.get("tags", []),
                    "references": info.get("reference", []),
                    "cwe": info.get("classification", {}).get("cwe-id", ""),
                    "cvss_score": cvss_score,
                    "timestamp": finding.get("timestamp", datetime.now().isoformat())
                })
            except Exception as e:
                self.logger.error(f"Error processing finding: {str(e)}")

        return processed

    def _generate_summary(self, findings: List[Dict]) -> Dict:
        """Generate summary of findings"""
        severity_counts = {
            "critical": 0,
            "high": 0,
            "medium": 0,
            "low": 0,
            "info": 0
        }
        
        for finding in findings:
            severity = finding.get("severity", "").lower()
            if severity in severity_counts:
                severity_counts[severity] += 1
            else:
                severity_counts["info"] += 1

        return {
            "total_findings": len(findings),
            "severity_distribution": severity_counts,
            "unique_templates": len(set(f.get("template_id", "") for f in findings))
        }

    def update_templates(self) -> Dict:
        """Update Nuclei templates"""
        try:
            cmd = [self.nuclei_path, "-update-templates", "-silent"]
            process = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                env=self.env,
                timeout=180
            )
            
            if process.returncode != 0:
                return {
                    "status": "error",
                    "error": process.stderr or "Template update failed"
                }

            return {
                "status": "success",
                "message": "Templates updated successfully"
            }

        except Exception as e:
            self.logger.error(f"Error updating templates: {str(e)}")
            return {
                "status": "error",
                "error": str(e)
            }

    def get_template_info(self) -> Dict:
        """Get information about available templates"""
        try:
            cmd = [self.nuclei_path, "-tl", "-silent"]
            process = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                env=self.env,
                timeout=60
            )
            
            if process.returncode != 0:
                return {
                    "status": "error",
                    "error": process.stderr or "Failed to get template list"
                }

            templates = process.stdout.strip().split("\n")
            template_count = len(templates)
            
            template_types = {
                "cve": 0,
                "vulnerability": 0,
                "exposure": 0,
                "technology": 0,
                "misconfiguration": 0,
                "default-login": 0,
                "other": 0
            }
            
            for template in templates:
                template_lower = template.lower()
                if "cve" in template_lower:
                    template_types["cve"] += 1
                elif "vuln" in template_lower:
                    template_types["vulnerability"] += 1
                elif "exposure" in template_lower:
                    template_types["exposure"] += 1
                elif "tech" in template_lower:
                    template_types["technology"] += 1
                elif "misconfig" in template_lower:
                    template_types["misconfiguration"] += 1
                elif "default" in template_lower and "login" in template_lower:
                    template_types["default-login"] += 1
                else:
                    template_types["other"] += 1
            
            return {
                "status": "success",
                "total_templates": template_count,
                "template_types": template_types,
                "templates": templates[:50],
                "note": "Response limited to 50 templates" if template_count > 50 else ""
            }

        except Exception as e:
            self.logger.error(f"Error getting template info: {str(e)}")
            return {
                "status": "error",
                "error": str(e)
            }
            
    def diagnose(self) -> Dict:
        """Run diagnostics to troubleshoot Nuclei scanner issues"""
        try:
            # Check nuclei binary
            binary_exists = os.path.exists(self.nuclei_path)
            binary_executable = os.access(self.nuclei_path, os.X_OK) if binary_exists else False
            
            # Check directories
            results_dir_exists = self.results_dir.exists()
            results_dir_writable = os.access(str(self.results_dir), os.W_OK) if results_dir_exists else False
            debug_dir_exists = self.debug_dir.exists()
            debug_dir_writable = os.access(str(self.debug_dir), os.W_OK) if debug_dir_exists else False
            
            # Check templates
            templates_path = self._get_templates_path()
            templates_exist = templates_path is not None
            as_flag_supported = self._check_as_flag_support()
            
            # Try version check
            version_info = {}
            try:
                result = subprocess.run(
                    [self.nuclei_path, '-version'],
                    capture_output=True,
                    text=True,
                    env=self.env,
                    timeout=10
                )
                version_info = {
                    "success": result.returncode == 0,
                    "stdout": result.stdout,
                    "stderr": result.stderr,
                    "returncode": result.returncode
                }
            except Exception as e:
                version_info = {
                    "success": False,
                    "error": str(e)
                }
            
            return {
                "status": "success",
                "binary": {
                    "path": self.nuclei_path,
                    "exists": binary_exists,
                    "executable": binary_executable
                },
                "directories": {
                    "results_dir": {
                        "path": str(self.results_dir),
                        "exists": results_dir_exists,
                        "writable": results_dir_writable
                    },
                    "debug_dir": {
                        "path": str(self.debug_dir),
                        "exists": debug_dir_exists,
                        "writable": debug_dir_writable
                    }
                },
                "templates": {
                    "path": templates_path,
                    "exists": templates_exist,
                    "auto_scan_supported": as_flag_supported
                },
                "version_check": version_info,
                "environment": {
                    "gopath": self.go_path,
                    "path": self.env.get("PATH", "")
                }
            }
        except Exception as e:
            self.logger.error(f"Error running diagnostics: {str(e)}")
            return {
                "status": "error",
                "error": str(e)
            }# vulnerability/correlation.py

from typing import List, Dict, Tuple, Set, Optional
import logging
from .models import Vulnerability
from datetime import datetime
import hashlib
import re
from django.db import transaction

class VulnerabilityCorrelator:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # Correlation thresholds
        self.title_similarity_threshold = 0.8  # Title similarity threshold
        self.description_similarity_threshold = 0.6  # Description similarity threshold
        
        # Define correlation rules
        self.correlation_rules = [
            self._correlation_by_target_and_name,  # Match by normalized target and name
            self._correlation_by_name_and_type,    # Exact name + type match
            self._correlation_by_fingerprint,      # Using vulnerability fingerprint
            self._correlation_by_cve,              # By CVE/reference match
            self._correlation_by_similarity        # By content similarity
        ]

    def _normalize_target(self, target: str) -> str:
        """Normalize target URL to consistent format"""
        if not target:
            return ""
            
        # Remove protocol prefix
        if '://' in target:
            target = target.split('://', 1)[1]
        
        # Remove path, trailing slash, etc.
        if '/' in target:
            target = target.split('/', 1)[0]
            
        # Remove port if present
        if ':' in target:
            target = target.split(':', 1)[0]
            
        # Remove 'www.' prefix if present
        if target.startswith('www.'):
            target = target[4:]
            
        return target.lower()

    def correlate_findings(self, 
                           internal_results: List[Dict] = None, 
                           zap_results: List[Dict] = None, 
                           nuclei_results: List[Dict] = None,
                           openvas_results: List[Dict] = None, 
                           manual_results: List[Dict] = None,
                           target: str = None) -> Dict:
        """
        Correlate findings from multiple scanners
        Returns a structured result with correlated findings and statistics
        """
        try:
            # Initialize empty lists if not provided
            internal_results = internal_results or []
            zap_results = zap_results or []
            nuclei_results = nuclei_results or []
            openvas_results = openvas_results or []
            manual_results = manual_results or []
            
            # Normalize findings from each scanner
            normalized_findings = []
            
            # Process results from each scanner
            for finding in internal_results:
                normalized = self._normalize_internal_finding(finding)
                normalized_findings.append(normalized)

            for finding in zap_results:
                normalized = self._normalize_zap_finding(finding)
                normalized_findings.append(normalized)
                
            for finding in nuclei_results:
                normalized = self._normalize_nuclei_finding(finding)
                normalized_findings.append(normalized)

            for finding in openvas_results:
                normalized = self._normalize_openvas_finding(finding)
                normalized_findings.append(normalized)
                
            for finding in manual_results:
                normalized = self._normalize_manual_finding(finding)
                normalized_findings.append(normalized)

            # Apply correlation to identify related findings
            self.logger.info(f"Correlating {len(normalized_findings)} findings from multiple scanners")
            correlated_findings = self._correlate_findings(normalized_findings)
            
            # Save correlated findings if target is provided
            saved_findings = []
            if target:
                # Normalize target before saving
                normalized_target = self._normalize_target(target)
                saved_findings = self._save_findings(correlated_findings, normalized_target)
            
            # Generate correlation statistics
            stats = self._generate_correlation_stats(normalized_findings, correlated_findings)
            
            return {
                'status': 'success',
                'original_count': len(normalized_findings),
                'correlated_count': len(correlated_findings),
                'findings': saved_findings if target else correlated_findings,
                'statistics': stats,
                'correlation_timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            self.logger.error(f"Error correlating findings: {str(e)}")
            return {
                'status': 'error',
                'error': str(e)
            }

    def _normalize_internal_finding(self, finding: Dict) -> Dict:
        """Normalize internal scanner findings"""
        # Get and normalize target from finding
        target = finding.get('target', '')
        normalized_target = self._normalize_target(target)
        
        return {
            'name': finding.get('name', 'Unknown Finding'),
            'description': finding.get('description', ''),
            'severity': finding.get('severity', 'LOW'),
            'evidence': finding.get('evidence', ''),
            'source': 'internal',
            'confidence': finding.get('confidence', 'medium'),
            'type': finding.get('type', 'unknown'),
            'references': finding.get('references', []),
            'cve': self._extract_cve_references(finding.get('references', [])),
            'cvss_score': finding.get('cvss', 0.0),
            'cwe': finding.get('cwe', ''),
            'fingerprint': self._generate_finding_fingerprint(finding),
            'target': normalized_target,  # Add normalized target
            'metadata': {
                'original_source': 'internal_scanner',
                'original_id': finding.get('id'),
                'scan_date': datetime.now().isoformat(),
                'original_target': target  # Store original target
            }
        }

    def _normalize_zap_finding(self, finding: Dict) -> Dict:
        """Normalize ZAP findings"""
        name = finding.get('name', 'Unknown ZAP Finding')
        description = finding.get('description', '')
        
        # Extract any CVEs mentioned in the description or alerts
        cves = self._extract_cve_from_text(description)
        if 'alertItems' in finding:
            for item in finding['alertItems']:
                item_desc = item.get('description', '')
                cves.update(self._extract_cve_from_text(item_desc))
        
        refs = finding.get('references', [])
        if isinstance(refs, str):
            refs = [refs]
        
        # Extract and normalize target from URL
        url = finding.get('url', '')
        normalized_target = self._normalize_target(url)
        
        return {
            'name': name,
            'description': description,
            'severity': self._map_zap_severity(finding.get('risk')),
            'evidence': finding.get('evidence', ''),
            'source': 'zap',
            'confidence': self._map_zap_confidence(finding.get('confidence', '')),
            'type': 'web',
            'references': refs,
            'cve': list(cves),
            'cwe': str(finding.get('cweid', '')),
            'cvss_score': self._calculate_cvss_from_severity(self._map_zap_severity(finding.get('risk'))),
            'fingerprint': self._generate_finding_fingerprint(finding),
            'target': normalized_target,  # Add normalized target
            'metadata': {
                'original_source': 'zap',
                'original_id': finding.get('id'),
                'url': url,
                'parameter': finding.get('parameter', ''),
                'attack': finding.get('attack', ''),
                'wascid': finding.get('wascid', ''),
                'solution': finding.get('solution', ''),
                'scan_date': datetime.now().isoformat()
            }
        }

    def _normalize_nuclei_finding(self, finding: Dict) -> Dict:
        """Normalize Nuclei findings"""
        # Extract CVEs from template ID or name
        cves = set()
        template_id = finding.get('template_id', '')
        name = finding.get('name', '')
        
        # Check if template_id or name contains CVE reference
        cves.update(self._extract_cve_from_text(template_id))
        cves.update(self._extract_cve_from_text(name))
        
        # Add any CVEs from references
        if finding.get('references'):
            for ref in finding['references']:
                cves.update(self._extract_cve_from_text(ref))
        
        # Extract and normalize target from host field
        host = finding.get('host', '')
        normalized_target = self._normalize_target(host)
        
        return {
            'name': finding.get('name', 'Unknown Nuclei Finding'),
            'description': finding.get('description', ''),
            'severity': finding.get('severity', 'LOW'),
            'evidence': finding.get('evidence', finding.get('matched', '')),
            'source': 'nuclei',
            'confidence': 'high',  # Nuclei typically has high confidence due to specific template matching
            'type': finding.get('type', 'nuclei'),
            'references': finding.get('references', []),
            'cve': list(cves),
            'cwe': finding.get('cwe', ''),
            'cvss_score': finding.get('cvss_score', 0.0),
            'fingerprint': self._generate_finding_fingerprint(finding),
            'target': normalized_target,  # Add normalized target
            'metadata': {
                'original_source': 'nuclei',
                'template_id': finding.get('template_id', ''),
                'matched_at': finding.get('matched', ''),
                'host': host,
                'tags': finding.get('tags', []),
                'scan_date': datetime.now().isoformat()
            }
        }

    def _normalize_openvas_finding(self, finding: Dict) -> Dict:
        """Normalize OpenVAS findings"""
        cves = set()
        
        # Extract CVEs from direct field or references
        if 'cve' in finding:
            if isinstance(finding['cve'], list):
                for cve in finding['cve']:
                    cves.update(self._extract_cve_from_text(cve))
            else:
                cves.update(self._extract_cve_from_text(finding['cve']))
        
        # Extract and normalize target from host field
        host = finding.get('host', '')
        normalized_target = self._normalize_target(host)
                
        return {
            'name': finding.get('name', 'Unknown OpenVAS Finding'),
            'description': finding.get('description', ''),
            'severity': self._map_openvas_severity(finding.get('severity')),
            'evidence': f"Port: {finding.get('port')} - Host: {host}",
            'source': 'openvas',
            'confidence': 'high',
            'type': 'network',
            'references': finding.get('references', []),
            'cve': list(cves),
            'cwe': finding.get('cwe', ''),
            'cvss_score': float(finding.get('cvss', 0.0)) if finding.get('cvss') else 0.0,
            'fingerprint': self._generate_finding_fingerprint(finding),
            'target': normalized_target,  # Add normalized target
            'metadata': {
                'original_source': 'openvas',
                'solution': finding.get('solution'),
                'port': finding.get('port'),
                'scan_date': datetime.now().isoformat()
            }
        }
        
    def _normalize_manual_finding(self, finding: Dict) -> Dict:
        """Normalize manually entered findings"""
        # Extract and normalize target
        target = finding.get('target', '')
        normalized_target = self._normalize_target(target)
        
        return {
            'name': finding.get('name', 'Manual Finding'),
            'description': finding.get('description', ''),
            'severity': finding.get('severity', 'LOW'),
            'evidence': finding.get('evidence', ''),
            'source': 'manual',
            'confidence': finding.get('confidence', 'high'),
            'type': finding.get('type', 'manual'),
            'references': finding.get('references', []),
            'cve': self._extract_cve_from_text(finding.get('description', '')),
            'cwe': finding.get('cwe', ''),
            'cvss_score': finding.get('cvss_score', 0.0),
            'fingerprint': self._generate_finding_fingerprint(finding),
            'target': normalized_target,  # Add normalized target
            'metadata': {
                'original_source': 'manual',
                'author': finding.get('author', 'Unknown'),
                'notes': finding.get('notes', ''),
                'scan_date': datetime.now().isoformat(),
                'original_target': target  # Store original target
            }
        }

    #
    # Correlation methods
    #
    
    def _correlate_findings(self, findings: List[Dict]) -> List[Dict]:
        """
        Apply correlation rules to findings to identify related issues
        """
        if not findings:
            return []
            
        self.logger.info(f"Starting correlation of {len(findings)} findings")
        
        # Create groups of correlated findings
        groups = []
        processed = set()
        
        for i, finding in enumerate(findings):
            if i in processed:
                continue
                
            # Create a new group with this finding
            group = [finding]
            processed.add(i)
            
            # Check all other findings for correlation
            for j, other in enumerate(findings):
                if j in processed or i == j:
                    continue
                    
                # Apply correlation rules
                if self._are_findings_related(finding, other):
                    group.append(other)
                    processed.add(j)
            
            groups.append(group)
        
        # Merge findings in each group
        correlated_findings = []
        for group in groups:
            if len(group) == 1:
                correlated_findings.append(group[0])
            else:
                merged = self._merge_findings(group)
                correlated_findings.append(merged)
                
        self.logger.info(f"Correlation complete. Reduced {len(findings)} findings to {len(correlated_findings)}")
        return correlated_findings

    def _are_findings_related(self, finding1: Dict, finding2: Dict) -> bool:
        """
        Check if two findings are related using multiple correlation rules
        Returns True if any correlation rule matches
        """
        for rule in self.correlation_rules:
            if rule(finding1, finding2):
                return True
        return False

    def _correlation_by_target_and_name(self, finding1: Dict, finding2: Dict) -> bool:
        """Correlation rule: same normalized target and name"""
        if not (finding1.get('target') and finding2.get('target') and 
                finding1.get('name') and finding2.get('name')):
            return False
            
        # Check if targets match using normalized form
        # They should already be normalized during the normalization step
        norm_target1 = finding1['target']
        norm_target2 = finding2['target']
        
        # If same normalized target and similar name, they're likely duplicates
        if norm_target1 == norm_target2:
            name1 = self._normalize_text(finding1['name'])
            name2 = self._normalize_text(finding2['name'])
            
            # Check for exact name match or high similarity
            if name1 == name2:
                return True
            
            # Check similarity for close matches
            name_similarity = self._calculate_similarity(name1, name2)
            if name_similarity >= 0.9:  # Higher threshold for name-only matching
                return True
                
        return False

    def _correlation_by_name_and_type(self, finding1: Dict, finding2: Dict) -> bool:
        """Correlation rule: same name and vulnerability type"""
        if finding1.get('name') and finding2.get('name') and finding1.get('type') and finding2.get('type'):
            # Normalize names for comparison
            name1 = self._normalize_text(finding1['name'])
            name2 = self._normalize_text(finding2['name'])
            return name1 == name2 and finding1['type'] == finding2['type']
        return False

    def _correlation_by_fingerprint(self, finding1: Dict, finding2: Dict) -> bool:
        """Correlation rule: match by vulnerability fingerprint"""
        if finding1.get('fingerprint') and finding2.get('fingerprint'):
            return finding1['fingerprint'] == finding2['fingerprint']
        return False

    def _correlation_by_cve(self, finding1: Dict, finding2: Dict) -> bool:
        """Correlation rule: match by CVE references"""
        cves1 = set(finding1.get('cve', []))
        cves2 = set(finding2.get('cve', []))
        
        # If both have CVEs and there's an overlap, they're related
        return bool(cves1 and cves2 and cves1.intersection(cves2))

    def _correlation_by_similarity(self, finding1: Dict, finding2: Dict) -> bool:
        """Correlation rule: match by content similarity"""
        # Compare titles for similarity
        name1 = finding1.get('name', '')
        name2 = finding2.get('name', '')
        
        if name1 and name2:
            name_similarity = self._calculate_similarity(name1, name2)
            if name_similarity >= self.title_similarity_threshold:
                return True
                
        # Compare descriptions for similarity
        desc1 = finding1.get('description', '')
        desc2 = finding2.get('description', '')
        
        if desc1 and desc2:
            desc_similarity = self._calculate_similarity(desc1, desc2)
            if desc_similarity >= self.description_similarity_threshold:
                return True
                
        return False

    def _merge_findings(self, findings: List[Dict]) -> Dict:
        """
        Merge a group of related findings into a single comprehensive finding
        """
        if not findings:
            return {}
            
        if len(findings) == 1:
            return findings[0]
            
        # Start with the highest severity finding as the base
        findings.sort(key=lambda x: self._severity_to_score(x.get('severity', 'LOW')), reverse=True)
        base = findings[0].copy()
        
        # Track merged sources
        sources = set([base['source']])
        references = set(base.get('references', []))
        cves = set(base.get('cve', []))
        
        # Create a comprehensive description
        descriptions = [f"{base['source']}: {base['description']}"]
        evidences = [f"{base['source']}: {base['evidence']}"]
        
        # Merge additional findings
        for finding in findings[1:]:
            sources.add(finding['source'])
            
            # Add references
            for ref in finding.get('references', []):
                references.add(ref)
                
            # Add CVEs
            for cve in finding.get('cve', []):
                cves.add(cve)
                
            # Add description and evidence
            if finding.get('description'):
                descriptions.append(f"{finding['source']}: {finding['description']}")
            
            if finding.get('evidence'):
                evidences.append(f"{finding['source']}: {finding['evidence']}")
                
            # Merge metadata
            for key, value in finding.get('metadata', {}).items():
                if key not in base['metadata']:
                    base['metadata'][key] = value
                    
        # Update merged finding
        base['source'] = ','.join(sources)
        base['references'] = list(references)
        base['cve'] = list(cves)
        base['description'] = '\n\n'.join(descriptions)
        base['evidence'] = '\n\n'.join(evidences)
        
        # Set confidence based on number of sources
        base['confidence'] = 'high' if len(sources) > 1 else base['confidence']
        
        # Calculate highest CVSS score
        base['cvss_score'] = max([f.get('cvss_score', 0.0) for f in findings])
        
        # Add correlation metadata
        base['metadata']['correlated'] = True
        base['metadata']['correlated_count'] = len(findings)
        base['metadata']['correlated_sources'] = list(sources)
        
        return base

    #
    # Helper methods
    #
    
    def _generate_finding_fingerprint(self, finding: Dict) -> str:
        """
        Generate a unique fingerprint for a finding to aid in correlation
        """
        # Extract key attributes for fingerprinting
        name = finding.get('name', '')
        desc = finding.get('description', '')
        
        # Add target to fingerprint to avoid false matches across different targets
        target = finding.get('target', '')
        
        # Create a string representation
        fingerprint_str = f"{target}:{name}:{desc}"
        
        # For specific scanner types, add additional identifiers
        source = finding.get('source', '')
        if source == 'zap' and 'alertItems' in finding:
            fingerprint_str += f":{finding.get('cweid', '')}"
        elif source == 'nuclei':
            fingerprint_str += f":{finding.get('template_id', '')}"
        
        # Generate SHA256 hash
        return hashlib.sha256(fingerprint_str.encode()).hexdigest()

    def _extract_cve_from_text(self, text: str) -> Set[str]:
        """
        Extract CVE IDs from text using regex
        """
        if not text:
            return set()
            
        # CVE pattern: CVE-YYYY-NNNNN (YYYY is year, NNNNN is ID number)
        cve_pattern = r'CVE-\d{4}-\d{4,7}'
        
        # Find all matches
        cves = re.findall(cve_pattern, text, re.IGNORECASE)
        return set(cve.upper() for cve in cves)

    def _extract_cve_references(self, references: List[str]) -> List[str]:
        """
        Extract CVE IDs from reference URLs
        """
        cves = set()
        for ref in references:
            cves.update(self._extract_cve_from_text(ref))
        return list(cves)

    def _normalize_text(self, text: str) -> str:
        """
        Normalize text for more accurate comparison:
        - Convert to lowercase
        - Remove punctuation
        - Remove common words
        """
        if not text:
            return ""
            
        # Convert to lowercase
        text = text.lower()
        
        # Remove punctuation
        text = re.sub(r'[^\w\s]', '', text)
        
        # Remove common words
        stop_words = {'a', 'an', 'the', 'in', 'on', 'at', 'of', 'for', 'with', 'by'}
        words = text.split()
        filtered_words = [word for word in words if word not in stop_words]
        
        return ' '.join(filtered_words)

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """
        Calculate similarity between two strings using Jaccard similarity
        Returns value between 0.0 and 1.0
        """
        if not text1 or not text2:
            return 0.0
            
        # Normalize texts
        text1 = self._normalize_text(text1)
        text2 = self._normalize_text(text2)
        
        # Split into words
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        # Calculate Jaccard similarity: intersection / union
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        if union == 0:
            return 0.0
            
        return intersection / union

    def _severity_to_score(self, severity: str) -> int:
        """
        Convert severity string to numeric score for comparison
        """
        severity = severity.upper()
        scores = {
            'CRITICAL': 4,
            'HIGH': 3,
            'MEDIUM': 2,
            'LOW': 1,
            'INFO': 0
        }
        return scores.get(severity, 0)

    def _map_zap_severity(self, severity: str) -> str:
        """Map ZAP severity to standard severity"""
        if not severity:
            return 'LOW'
            
        mapping = {
            'Informational': 'INFO',
            'Low': 'LOW',
            'Medium': 'MEDIUM',
            'High': 'HIGH',
            'Critical': 'CRITICAL',
            '0': 'INFO',
            '1': 'LOW',
            '2': 'MEDIUM',
            '3': 'HIGH',
            '4': 'CRITICAL'
        }
        return mapping.get(severity, 'LOW')

    def _map_zap_confidence(self, confidence: str) -> str:
        """Map ZAP confidence to standard confidence"""
        if not confidence:
            return 'medium'
            
        mapping = {
            'False Positive': 'low',
            'Low': 'low',
            'Medium': 'medium',
            'High': 'high',
            'Confirmed': 'high',
            '0': 'low',
            '1': 'low',
            '2': 'medium',
            '3': 'high',
            '4': 'high'
        }
        return mapping.get(confidence, 'medium')

    def _map_openvas_severity(self, severity: str) -> str:
        """Map OpenVAS severity to standard severity"""
        try:
            severity_float = float(severity)
            if severity_float >= 9.0:
                return 'CRITICAL'
            elif severity_float >= 7.0:
                return 'HIGH'
            elif severity_float >= 4.0:
                return 'MEDIUM'
            elif severity_float > 0:
                return 'LOW'
            return 'INFO'
        except (ValueError, TypeError):
            return 'LOW'

    def _calculate_cvss_from_severity(self, severity: str) -> float:
        """Calculate estimated CVSS score from severity string"""
        severity = severity.upper()
        cvss_mapping = {
            'CRITICAL': 9.5,
            'HIGH': 7.5,
            'MEDIUM': 5.0,
            'LOW': 2.5,
            'INFO': 0.0
        }
        return cvss_mapping.get(severity, 0.0)

    def _generate_correlation_stats(self, original_findings: List[Dict], 
                                   correlated_findings: List[Dict]) -> Dict:
        """
        Generate statistics about the correlation process
        """
        # Count by source
        original_by_source = {}
        for finding in original_findings:
            source = finding.get('source', 'unknown')
            original_by_source[source] = original_by_source.get(source, 0) + 1
            
        # Count by severity
        severity_distribution = {
            'CRITICAL': 0,
            'HIGH': 0,
            'MEDIUM': 0,
            'LOW': 0,
            'INFO': 0
        }
        
        correlation_groups = []
        for finding in correlated_findings:
            # Update severity count
            severity = finding.get('severity', 'LOW').upper()
            if severity in severity_distribution:
                severity_distribution[severity] += 1
                
            # Track correlation groups
            if ',' in finding.get('source', ''):
                sources = finding.get('source', '').split(',')
                correlation_groups.append({
                    'name': finding.get('name', 'Unknown'),
                    'sources': sources,
                    'severity': severity
                })
        
        return {
            'original_count': len(original_findings),
            'correlated_count': len(correlated_findings),
            'reduction_percentage': round((1 - len(correlated_findings) / len(original_findings)) * 100, 2) if original_findings else 0,
            'original_by_source': original_by_source,
            'severity_distribution': severity_distribution,
            'correlation_groups': correlation_groups[:10]  # Limit to top 10 groups
        }

    @transaction.atomic
    def _save_findings(self, findings: List[Dict], target: str) -> List[Dict]:
        """
        Save correlated findings to database with transaction support
        """
        saved_findings = []
        
        # Ensure target is normalized
        normalized_target = self._normalize_target(target)
        
        for finding in findings:
            try:
                # Check if this vulnerability already exists
                existing_vulns = Vulnerability.objects.filter(
                    target=normalized_target,
                    name=finding['name'],
                    vuln_type=finding.get('type', 'unknown'),
                    severity=finding['severity']
                )
                
                if existing_vulns.exists():
                    # Update the existing vulnerability
                    vuln = existing_vulns.first()
                    
                    # Combine sources
                    sources = set(vuln.source.split(','))
                    sources.add(finding['source'])
                    vuln.source = ','.join(sorted(sources))
                    
                    # Update other fields if needed
                    if finding.get('cvss_score', 0) > (vuln.cvss_score or 0):
                        vuln.cvss_score = finding['cvss_score']
                    
                    # Update metadata
                    if not vuln.metadata:
                        vuln.metadata = {}
                    vuln.metadata.update({
                        **finding.get('metadata', {}),
                        'cve': finding.get('cve', []),
                        'updated_at': datetime.now().isoformat(),
                        'has_updates': True
                    })
                    
                    vuln.save()
                    saved_findings.append(self._serialize_vulnerability(vuln))
                else:
                    # Create a new vulnerability
                    vulnerability = Vulnerability.objects.create(
                        target=normalized_target,  # Use normalized target
                        name=finding['name'],
                        description=finding['description'],
                        severity=finding['severity'],
                        vuln_type=finding.get('type', 'unknown'),
                        evidence=finding['evidence'],
                        source=finding['source'],
                        confidence=finding['confidence'],
                        references=finding.get('references', []),
                        solution=finding.get('metadata', {}).get('solution', ''),
                        cwe=finding.get('cwe', ''),
                        cvss_score=finding.get('cvss_score', 0.0),
                        metadata={
                            **finding.get('metadata', {}),
                            'cve': finding.get('cve', []),
                            'original_target': finding.get('metadata', {}).get('original_target', target)
                        }
                    )
                    saved_findings.append(self._serialize_vulnerability(vulnerability))
            except Exception as e:
                self.logger.error(f"Error saving vulnerability: {str(e)}")
        
        return saved_findings

    def _serialize_vulnerability(self, vuln: Vulnerability) -> Dict:
        """Serialize vulnerability for response"""
        return {
            'id': vuln.id,
            'name': vuln.name,
            'description': vuln.description,
            'severity': vuln.severity,
            'type': vuln.vuln_type,
            'evidence': vuln.evidence,
            'sources': vuln.source.split(','),
            'confidence': vuln.confidence,
            'discovery_date': vuln.discovery_date.isoformat(),
            'references': vuln.references,
            'cvss_score': vuln.cvss_score,
            'cwe': vuln.cwe,
            'metadata': vuln.metadata
        }# File: exploit_manager/models.py
from django.db import models

class ExploitSource(models.Model):
    """Represents an exploit data source like ExploitDB"""
    name = models.CharField(max_length=100)
    description = models.TextField(blank=True)
    api_url = models.URLField(blank=True)
    last_update = models.DateTimeField(null=True, blank=True)
    is_active = models.BooleanField(default=True)
    
    def __str__(self):
        return self.name

class Exploit(models.Model):
    """Represents an individual exploit"""
    # Core fields
    exploit_id = models.CharField(max_length=50)  # Original ID from source
    title = models.CharField(max_length=255)
    description = models.TextField(blank=True)
    
    # Classification
    type = models.CharField(max_length=100, blank=True)  # webapps, remote, local, etc.
    platform = models.CharField(max_length=100, blank=True)  # Windows, Linux, etc.
    
    # Vulnerability information
    vulnerability_name = models.CharField(max_length=255, blank=True)
    cve_id = models.CharField(max_length=50, blank=True)
    
    # Dates
    date_published = models.DateField(null=True, blank=True)
    date_added = models.DateTimeField(auto_now_add=True)
    date_updated = models.DateTimeField(auto_now=True)
    
    # Source information
    source = models.ForeignKey(ExploitSource, on_delete=models.CASCADE, related_name='exploits')
    source_url = models.URLField(blank=True)
    
    # Code and technical details
    code = models.TextField(blank=True)
    file_path = models.CharField(max_length=255, blank=True)  # If stored as file
    author = models.CharField(max_length=255, blank=True)
    
    # Additional metadata
    verified = models.BooleanField(default=False)
    score = models.FloatField(null=True, blank=True)  # Relevance or quality score
    metadata = models.JSONField(default=dict, blank=True)
    
    class Meta:
        unique_together = ('exploit_id', 'source')
        indexes = [
            models.Index(fields=['cve_id']),
            models.Index(fields=['exploit_id']),
            models.Index(fields=['type']),
        ]
    
    def __str__(self):
        return f"{self.title} ({self.exploit_id})"
    
class ExploitMatch(models.Model):
    """Represents a match between a vulnerability and an exploit"""
    STATUS_CHOICES = [
        ('pending', 'Pending Review'),
        ('confirmed', 'Confirmed Match'),
        ('rejected', 'Rejected Match'),
        ('exploited', 'Successfully Exploited')
    ]
    
    vulnerability = models.ForeignKey('vulnerability.Vulnerability', on_delete=models.CASCADE, related_name='exploit_matches')
    exploit = models.ForeignKey(Exploit, on_delete=models.CASCADE, related_name='vulnerability_matches')
    
    confidence_score = models.FloatField(default=0.5)  # 0.0 to 1.0
    status = models.CharField(max_length=20, choices=STATUS_CHOICES, default='pending')
    
    # Match metadata
    match_reason = models.TextField(blank=True)
    matched_by = models.CharField(max_length=50, blank=True)  # user or system
    match_date = models.DateTimeField(auto_now_add=True)
    last_updated = models.DateTimeField(auto_now=True)
    
    # User feedback
    notes = models.TextField(blank=True)
    
    class Meta:
        unique_together = ('vulnerability', 'exploit')
        indexes = [
            models.Index(fields=['status']),
            models.Index(fields=['confidence_score']),
        ]
    
    def __str__(self):
        return f"Match: {self.vulnerability.name}  {self.exploit.title}"# File: exploit_manager/views.py
from django.shortcuts import render, get_object_or_404
from django.http import HttpResponse, JsonResponse
from django.views import View
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from django.core.paginator import Paginator
from django.db.models import Q
import json
import logging
from .models import Exploit, ExploitMatch
from .exploit_db import ExploitDBManager
from .matcher import ExploitMatcher
from vulnerability.models import Vulnerability

logger = logging.getLogger(__name__)

@method_decorator(csrf_exempt, name='dispatch')
class ExploitSearchView(View):
    """
    View for searching exploits
    """
    def get(self, request):
        """Handle GET requests"""
        try:
            # Parse search parameters
            query = request.GET.get('query', '')
            cve = request.GET.get('cve', '')
            platform = request.GET.get('platform', '')
            exploit_type = request.GET.get('type', '')
            page = int(request.GET.get('page', 1))
            limit = min(int(request.GET.get('limit', 20)), 100)
            
            # Determine search source
            source = request.GET.get('source', 'db')
            
            if source == 'exploitdb':
                # Search ExploitDB directly
                exploitdb = ExploitDBManager()
                results = exploitdb.search_exploits(
                    query=query,
                    cve=cve,
                    platform=platform,
                    exploit_type=exploit_type,
                    limit=limit
                )
                
                return JsonResponse({
                    'status': 'success',
                    'count': len(results),
                    'results': results
                })
            else:
                # Search local database
                query_obj = Q()
                
                if query:
                    query_obj |= Q(title__icontains=query)
                    query_obj |= Q(description__icontains=query)
                
                if cve:
                    query_obj |= Q(cve_id__icontains=cve)
                
                if platform:
                    query_obj |= Q(platform__icontains=platform)
                    
                if exploit_type:
                    query_obj |= Q(type__icontains=exploit_type)
                
                # If no filters, ensure we have some constraints
                if not (query or cve or platform or exploit_type):
                    return JsonResponse({
                        'status': 'error',
                        'message': 'At least one search parameter is required'
                    }, status=400)
                
                # Execute query
                exploits = Exploit.objects.filter(query_obj).order_by('-date_published')
                
                # Paginate results
                paginator = Paginator(exploits, limit)
                page_obj = paginator.get_page(page)
                
                # Format results
                results = []
                for exploit in page_obj:
                    results.append({
                        'id': exploit.id,
                        'exploit_id': exploit.exploit_id,
                        'title': exploit.title,
                        'description': exploit.description[:200] + '...' if len(exploit.description) > 200 else exploit.description,
                        'type': exploit.type,
                        'platform': exploit.platform,
                        'cve_id': exploit.cve_id,
                        'date_published': exploit.date_published.isoformat() if exploit.date_published else None,
                        'verified': exploit.verified,
                        'source': exploit.source.name,
                        'source_url': exploit.source_url
                    })
                
                return JsonResponse({
                    'status': 'success',
                    'count': paginator.count,
                    'page': page,
                    'pages': paginator.num_pages,
                    'results': results
                })
                
        except Exception as e:
            logger.error(f"Error in exploit search: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'message': str(e)
            }, status=500)
    
    def post(self, request):
        """Handle POST requests for more complex searches"""
        try:
            data = json.loads(request.body)
            
            # Extract parameters
            query = data.get('query', '')
            cve = data.get('cve', '')
            platform = data.get('platform', '')
            exploit_type = data.get('type', '')
            page = int(data.get('page', 1))
            limit = min(int(data.get('limit', 20)), 100)
            
            # Additional options
            import_results = data.get('import_results', False)
            
            # Search ExploitDB directly
            exploitdb = ExploitDBManager()
            results = exploitdb.search_exploits(
                query=query,
                cve=cve,
                platform=platform,
                exploit_type=exploit_type,
                limit=limit
            )
            
            # Import results if requested
            imported_ids = []
            if import_results and results:
                for result in results:
                    exploit = exploitdb.import_exploit(result)
                    if exploit:
                        imported_ids.append(exploit.id)
            
            return JsonResponse({
                'status': 'success',
                'count': len(results),
                'results': results,
                'imported': len(imported_ids),
                'imported_ids': imported_ids
            })
            
        except Exception as e:
            logger.error(f"Error in exploit search (POST): {str(e)}")
            return JsonResponse({
                'status': 'error',
                'message': str(e)
            }, status=500)

class ExploitDetailView(View):
    """
    View for exploit details with robust ID handling and enhanced error management
    """
    def get(self, request, exploit_id):
        """Handle GET requests for exploit details"""
        try:
            # Enhanced ID handling with multiple lookup strategies
            exploit = None
            lookup_attempts = []

            # Try parsing as integer database ID first
            if str(exploit_id).isdigit():
                try:
                    exploit = Exploit.objects.get(id=int(exploit_id))
                    lookup_attempts.append(f"Database ID {exploit_id}")
                except Exploit.DoesNotExist:
                    pass

            # If not found, try looking up by ExploitDB ID
            if not exploit:
                try:
                    exploit = Exploit.objects.get(exploit_id=str(exploit_id))
                    lookup_attempts.append(f"ExploitDB ID {exploit_id}")
                except Exploit.DoesNotExist:
                    pass

            # If still not found, return a detailed error
            if not exploit:
                logger.warning(f"Exploit not found for ID: {exploit_id}")
                return JsonResponse({
                    'status': 'error', 
                    'message': f'No exploit found matching ID: {exploit_id}',
                    'lookup_attempts': lookup_attempts
                }, status=404)

            # Optional code fetching with enhanced error handling
            fetch_code = request.GET.get('fetch_code') == 'true'
            if fetch_code and not exploit.code:
                try:
                    exploitdb = ExploitDBManager()
                    code = exploitdb.get_exploit_code(exploit.exploit_id)
                    if code:
                        exploit.code = code
                        exploit.save()
                except Exception as fetch_error:
                    logger.error(f"Failed to fetch exploit code for {exploit.exploit_id}: {str(fetch_error)}")

            # Prepare rich response with detailed exploit information
            data = {
                'database_id': exploit.id,                             # Unique database ID
                'exploit_id': exploit.exploit_id,                      # External ExploitDB ID
                'title': exploit.title,
                'description': exploit.description,
                'details': {
                    'type': exploit.type,
                    'platform': exploit.platform,
                    'cve_id': exploit.cve_id,
                    'vulnerability_name': exploit.vulnerability_name,
                    'author': exploit.author
                },
                'metadata': {
                    'date_published': exploit.date_published.isoformat() if exploit.date_published else None,
                    'date_added': exploit.date_added.isoformat(),
                    'verified': exploit.verified,
                    'source': {
                        'name': exploit.source.name,
                        'url': exploit.source_url
                    }
                },
                'code_availability': {
                    'has_code': bool(exploit.code),
                    'code_length': len(exploit.code) if exploit.code else 0
                }
            }

            # Conditionally include full code based on request
            include_full_code = request.GET.get('include_code') == 'true'
            if include_full_code and exploit.code:
                data['code'] = exploit.code

            # Log successful retrieval
            logger.info(f"Retrieved exploit details for ID: {exploit_id}")

            return JsonResponse({
                'status': 'success',
                'exploit': data,
                'lookup_method': lookup_attempts[-1] if lookup_attempts else 'Unknown'
            })

        except Exception as e:
            # Comprehensive error logging
            logger.error(f"Critical error retrieving exploit with ID {exploit_id}: {str(e)}", 
                         exc_info=True)
            
            return JsonResponse({
                'status': 'error',
                'message': 'An unexpected error occurred while retrieving exploit details',
                'error_details': str(e)
            }, status=500)

@method_decorator(csrf_exempt, name='dispatch')
class VulnerabilityMatchView(View):
    """View for matching vulnerabilities with exploits"""
    
    def get(self, request, vulnerability_id):
        """Get matches for a vulnerability"""
        try:
            vulnerability = get_object_or_404(Vulnerability, id=vulnerability_id)
            
            # Get existing matches
            matches = ExploitMatch.objects.filter(vulnerability=vulnerability)
            
            # Format response
            results = []
            for match in matches:
                exploit = match.exploit
                results.append({
                    'id': match.id,
                    'confidence_score': match.confidence_score,
                    'status': match.status,
                    'match_reason': match.match_reason,
                    'matched_by': match.matched_by,
                    'match_date': match.match_date.isoformat(),
                    'exploit': {
                        'id': exploit.id,
                        'exploit_id': exploit.exploit_id,
                        'title': exploit.title,
                        'description': exploit.description[:200] + '...' if len(exploit.description) > 200 else exploit.description,
                        'cve_id': exploit.cve_id,
                        'type': exploit.type,
                        'platform': exploit.platform,
                        'source': exploit.source.name,
                        'verified': exploit.verified,
                        'has_code': bool(exploit.code),
                        'source_url': exploit.source_url
                    }
                })
                
            return JsonResponse({
                'status': 'success',
                'vulnerability': {
                    'id': vulnerability.id,
                    'name': vulnerability.name,
                    'severity': vulnerability.severity,
                    'target': vulnerability.target
                },
                'matches': results,
                'count': len(results)
            })
            
        except Exception as e:
            logger.error(f"Error getting vulnerability matches: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'message': str(e)
            }, status=500)
    
    def post(self, request, vulnerability_id):
        """Find new matches for a vulnerability"""
        try:
            vulnerability = get_object_or_404(Vulnerability, id=vulnerability_id)
            
            # Create matcher
            matcher = ExploitMatcher()
            
            # Run matching algorithm
            matches = matcher.match_vulnerability(vulnerability)
            
            return JsonResponse({
                'status': 'success',
                'message': f"Found {len(matches)} potential matches",
                'matches_found': len(matches)
            })
            
        except Exception as e:
            logger.error(f"Error matching vulnerability: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'message': str(e)
            }, status=500)

@method_decorator(csrf_exempt, name='dispatch')
class ExploitSyncView(View):
    """View for synchronizing exploits from ExploitDB"""
    
    def get(self, request):
        """Get exploit sync status"""
        try:
            from django.db.models import Count
            
            # Get exploit stats
            total_exploits = Exploit.objects.count()
            exploits_by_source = Exploit.objects.values('source__name').annotate(count=Count('id'))
            
            # Get latest exploits
            latest_exploits = Exploit.objects.order_by('-date_added')[:5].values(
                'id', 'exploit_id', 'title', 'date_added'
            )
            
            # Format response
            return JsonResponse({
                'status': 'success',
                'total_exploits': total_exploits,
                'by_source': list(exploits_by_source),
                'latest_exploits': list(latest_exploits)
            })
            
        except Exception as e:
            logger.error(f"Error getting exploit sync status: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'message': str(e)
            }, status=500)
    
    def post(self, request):
        """Sync exploits from ExploitDB"""
        try:
            data = json.loads(request.body)
            
            # Get sync limit
            limit = int(data.get('limit', 100)) # Increase the sync limit to 2000
            
            # Create ExploitDB manager
            manager = ExploitDBManager()
            
            # Run sync
            results = manager.sync_recent_exploits(limit)
            
            return JsonResponse({
                'status': 'success',
                'message': f"Synced {results['new']} new exploits, updated {results['updated']} existing exploits",
                'stats': results
            })
            
        except Exception as e:
            logger.error(f"Error syncing exploits: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'message': str(e)
            }, status=500)

class ExploitCodeView(View):
    """View for exploit code"""
    
    def get(self, request, exploit_id):
        """Get exploit code"""
        try:
            # Get the exploit
            exploit = get_object_or_404(Exploit, id=exploit_id)
            
            # Check if we have code and need to fetch
            fetch = request.GET.get('fetch') == 'true'
            
            if not exploit.code and fetch:
                # Fetch code
                manager = ExploitDBManager()
                code = manager.get_exploit_code(exploit.exploit_id)
                
                if code:
                    exploit.code = code
                    exploit.save()
            
            # Check if we still don't have code
            if not exploit.code:
                return JsonResponse({
                    'status': 'error',
                    'message': 'Exploit code not available'
                }, status=404)
            
            # Return raw code if requested
            if request.GET.get('raw') == 'true':
                return HttpResponse(exploit.code, content_type='text/plain')
            
            # Check if this is actual executable code or just a description
            code_content = exploit.code
            is_executable = self._is_executable_code(code_content, exploit.type, exploit.platform)
            
            # Extract code snippets if this is just a description
            code_snippets = []
            if not is_executable:
                code_snippets = self._extract_code_snippets(code_content)
            
            # Return code in JSON response with improved structure
            return JsonResponse({
                'status': 'success',
                'exploit_id': exploit.exploit_id,
                'title': exploit.title,
                'content_type': 'executable_code' if is_executable else 'advisory',
                'code': code_content,
                'code_snippets': code_snippets
            })
            
        except Exception as e:
            logger.error(f"Error getting exploit code: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'message': str(e)
            }, status=500)
    
    def _is_executable_code(self, content, exploit_type, platform):
        """
        Determine if content is likely executable code rather than just a description
        """
        # Check if content contains common code indicators
        code_indicators = [
            '#!/', 'import ', 'function ', 'def ', 'class ', '<script',
            '#include', 'int main', 'public class', 'void '
        ]
        
        # Check if content has a high ratio of code indicators
        if any(indicator in content for indicator in code_indicators):
            return True
            
        # For specific platform/types, check patterns
        if platform == 'php' and ('<?php' in content or '<?=' in content):
            return True
        if exploit_type == 'shellcode' and ('\\x' in content and content.count('\\x') > 5):
            return True
            
        # Check if most of the content is code-like (fewer natural language sentences)
        sentences = content.split('.')
        if len(sentences) < 3 and len(content) > 50:
            return True
            
        return False
    
    def _extract_code_snippets(self, content):
        """
        Extract code snippets from a descriptive writeup with improved sanitization
        """
        snippets = []
        
        # Look for code blocks with common patterns
        # Pattern 1: Code between triple backticks (markdown style)
        import re
        code_blocks = re.findall(r'```(?:\w+)?\n(.*?)\n```', content, re.DOTALL)
        snippets.extend([block.strip() for block in code_blocks if block.strip()])
        
        # Pattern 2: Indented code blocks (4+ spaces)
        lines = content.split('\n')
        current_snippet = []
        in_snippet = False
        
        for line in lines:
            if line.startswith('    ') or line.startswith('\t'):
                current_snippet.append(line.lstrip())
                in_snippet = True
            elif in_snippet and line.strip() == '':
                # Empty line within snippet
                current_snippet.append('')
            elif in_snippet:
                # End of snippet
                if current_snippet:
                    snippets.append('\n'.join(current_snippet))
                current_snippet = []
                in_snippet = False
        
        # Add the last snippet if there is one
        if current_snippet:
            snippets.append('\n'.join(current_snippet))
        
        # Pattern 3: Look for command-line examples
        cmd_snippets = re.findall(r'((?:http|curl|wget|perl|python|bash|sh|nc|ncat|netcat|ssh|ftp)[^\n]+)', content)
        snippets.extend([s.strip() for s in cmd_snippets if len(s.strip()) > 10])
        
        # Pattern 4: Look for URLs that might be exploitation examples
        urls = re.findall(r'((?:http|https)://[^\s]+)', content)
        exploit_urls = [url for url in urls if 'cgi' in url or '?' in url or any(p in url for p in ['php', 'asp', 'jsp'])]
        snippets.extend(exploit_urls)
        
        # Deduplicate and filter out empty snippets
        unique_snippets = []
        seen = set()
        for snippet in snippets:
            snippet_clean = snippet.strip()
            if snippet_clean and snippet_clean not in seen:
                seen.add(snippet_clean)
                unique_snippets.append(snippet_clean)
        
        return unique_snippets

@method_decorator(csrf_exempt, name='dispatch')
class MatchUpdateView(View):
    """View for updating exploit matches"""
    
    def post(self, request, match_id):
        """Update match status"""
        try:
            # Ensure request.body is not empty
            if not request.body:
                return JsonResponse({
                    'status': 'error',
                    'message': 'Empty request body. Expected JSON data.'
                }, status=400)
                
            try:
                data = json.loads(request.body)
            except json.JSONDecodeError as json_error:
                return JsonResponse({
                    'status': 'error',
                    'message': f'Invalid JSON format: {str(json_error)}'
                }, status=400)
            
            # Get the match
            match = get_object_or_404(ExploitMatch, id=match_id)
            
            # Update fields
            if 'status' in data:
                # Validate status value
                valid_statuses = [choice[0] for choice in ExploitMatch.STATUS_CHOICES]
                if data['status'] not in valid_statuses:
                    return JsonResponse({
                        'status': 'error',
                        'message': f'Invalid status value. Must be one of: {", ".join(valid_statuses)}'
                    }, status=400)
                match.status = data['status']
                
            if 'notes' in data:
                match.notes = data['notes']
                
            if 'confidence_score' in data:
                try:
                    confidence_score = float(data['confidence_score'])
                    if not (0.0 <= confidence_score <= 1.0):
                        return JsonResponse({
                            'status': 'error',
                            'message': 'Confidence score must be between 0.0 and 1.0'
                        }, status=400)
                    match.confidence_score = confidence_score
                except ValueError:
                    return JsonResponse({
                        'status': 'error',
                        'message': 'Invalid confidence score format. Must be a number between 0.0 and 1.0.'
                    }, status=400)
            
            # Save the match
            match.save()
            
            return JsonResponse({
                'status': 'success',
                'message': 'Match updated successfully',
                'match': {
                    'id': match.id,
                    'status': match.status,
                    'notes': match.notes,
                    'confidence_score': match.confidence_score
                }
            })
            
        except Exception as e:
            logger.error(f"Error updating match: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'message': str(e)
            }, status=500)
            
@method_decorator(csrf_exempt, name='dispatch')
class MatchAllVulnerabilitiesView(View):
    """View for matching all vulnerabilities with exploits"""
    
    def post(self, request):
        """Find new matches for all vulnerabilities"""
        try:
            # Get all unmatched vulnerabilities
            vulnerabilities = Vulnerability.objects.filter(
                exploit_matches__isnull=True  # No existing matches
            ).distinct()
            
            # Create matcher
            matcher = ExploitMatcher()
            
            # Track results
            total_vulnerabilities = vulnerabilities.count()
            matched_vulnerabilities = 0
            total_matches = 0
            match_details = []
            
            # Process each vulnerability
            for vuln in vulnerabilities:
                try:
                    # Run matching algorithm
                    matches = matcher.match_vulnerability(vuln)
                    
                    if matches:
                        matched_vulnerabilities += 1
                        total_matches += len(matches)
                        match_details.append({
                            'vulnerability_id': vuln.id,
                            'name': vuln.name,
                            'target': vuln.target,
                            'matches': len(matches)
                        })
                except Exception as vuln_error:
                    logger.error(f"Error matching vulnerability ID {vuln.id}: {str(vuln_error)}")
            
            return JsonResponse({
                'status': 'success',
                'message': f"Found {total_matches} matches for {matched_vulnerabilities} vulnerabilities",
                'total_vulnerabilities': total_vulnerabilities,
                'matched_vulnerabilities': matched_vulnerabilities,
                'total_matches': total_matches,
                'details': match_details
            })
            
        except Exception as e:
            logger.error(f"Error matching all vulnerabilities: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'message': str(e)
            }, status=500)# File: exploit_manager/exploit_db.py
# Updated implementation to handle exploit synchronization correctly

import requests
import csv
import io
import json
import logging
import time
from datetime import datetime
from django.conf import settings
from django.utils import timezone  # Import timezone for timezone-aware datetimes
from django.db.models import Max
from .models import Exploit, ExploitSource

logger = logging.getLogger(__name__)

class ExploitDBManager:
    """Manages interaction with ExploitDB"""
    
    # ExploitDB CSV URL
# To:
    EXPLOITDB_CSV_URL = "https://gitlab.com/exploit-database/exploitdb/-/raw/main/files_exploits.csv"
    EXPLOITDB_RAW_URL = "https://gitlab.com/exploit-database/exploitdb/-/raw/main/exploits"
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.source, _ = ExploitSource.objects.get_or_create(
            name="ExploitDB",
            defaults={
                'description': "Offensive Security's Exploit Database Archive",
                'api_url': self.EXPLOITDB_CSV_URL,
                'is_active': True
            }
        )
    
    def search_exploits(self, query: str = None, cve: str = None, platform: str = None, 
                         exploit_type: str = None, limit: int = 100) -> list:
        """
        Search for exploits in ExploitDB based on various criteria
        
        Args:
            query: General search query
            cve: Specific CVE ID to search for
            platform: Platform filter (e.g., 'windows', 'linux')
            exploit_type: Type filter (e.g., 'webapps', 'remote', 'local')
            limit: Maximum number of results to return
            
        Returns:
            List of exploit dictionaries
        """
        try:
            # Download the CSV file
            response = requests.get(self.EXPLOITDB_CSV_URL, timeout=30)
            response.raise_for_status()  # Raise an exception for HTTP errors
            
            # Parse CSV
            csv_data = response.text
            reader = csv.DictReader(io.StringIO(csv_data))
            
            # Filter exploits based on criteria
            results = []
            
            for row in reader:
                # Apply filters
                matches = True
                
                if query and query.lower() not in row.get('description', '').lower():
                    matches = False
                    
                if cve:
                    # Clean CVE ID format
                    cve = cve.upper().replace('CVE-', '').strip()
                    if cve and ('CVE-' + cve) not in row.get('description', ''):
                        matches = False
                
                if platform and platform.lower() not in row.get('platform', '').lower():
                    matches = False
                    
                if exploit_type and exploit_type.lower() not in row.get('type', '').lower():
                    matches = False
                
                # Add to results if all filters match
                if matches:
                    # Extract CVE if available
                    cve_id = ''
                    if 'CVE-' in row.get('description', ''):
                        import re
                        cve_match = re.search(r'CVE-\d{4}-\d{4,}', row.get('description', ''))
                        if cve_match:
                            cve_id = cve_match.group(0)
                    
                    # Format result
                    result = {
                        'exploit_id': row.get('id', ''),
                        'title': row.get('description', '')[:255],  # Limit title length
                        'description': row.get('description', ''),
                        'date_published': row.get('date', ''),
                        'type': row.get('type', ''),
                        'platform': row.get('platform', ''),
                        'source_url': f"https://www.exploit-db.com/exploits/{row.get('id', '')}",
                        'file_path': row.get('file', ''),
                        'cve_id': cve_id,
                        'metadata': row
                    }
                    
                    results.append(result)
                    
                    # Check if we've reached the limit
                    if len(results) >= limit:
                        break
            
            return results
                
        except Exception as e:
            self.logger.error(f"Error searching ExploitDB: {str(e)}")
            return []
    
    def get_exploit_code(self, exploit_id: str) -> str:
        """
        Fetch the actual exploit code from ExploitDB
        
        Args:
            exploit_id: ExploitDB ID
            
        Returns:
            str: Exploit code or empty string on failure
        """
        try:
            # First try to find the file path in the database
            try:
                exploit = Exploit.objects.get(exploit_id=exploit_id, source=self.source)
                if exploit.file_path:
                    file_path = exploit.file_path
                else:
                    # Fetch from CSV to get file path
                    response = requests.get(self.EXPLOITDB_CSV_URL, timeout=30)
                    response.raise_for_status()
                    
                    csv_data = response.text
                    reader = csv.DictReader(io.StringIO(csv_data))
                    
                    file_path = None
                    for row in reader:
                        if row.get('id') == exploit_id:
                            file_path = row.get('file')
                            # Update exploit with file path
                            exploit.file_path = file_path
                            exploit.save()
                            break
                            
                    if not file_path:
                        raise Exception(f"Exploit ID {exploit_id} not found in CSV")
            except Exploit.DoesNotExist:
                # Fetch from CSV
                response = requests.get(self.EXPLOITDB_CSV_URL, timeout=30)
                response.raise_for_status()
                
                csv_data = response.text
                reader = csv.DictReader(io.StringIO(csv_data))
                
                file_path = None
                for row in reader:
                    if row.get('id') == exploit_id:
                        file_path = row.get('file')
                        break
                        
                if not file_path:
                    raise Exception(f"Exploit ID {exploit_id} not found in CSV")
            
            # Check if file_path already starts with 'exploits/' 
            if file_path.startswith('exploits/'):
                # Remove 'exploits/' from the beginning since EXPLOITDB_RAW_URL already has it
                clean_path = file_path[len('exploits/'):]
            else:
                clean_path = file_path
            
            # Now fetch the actual exploit file
            raw_url = f"{self.EXPLOITDB_RAW_URL}/{clean_path}"
            self.logger.info(f"Attempting to fetch exploit code from: {raw_url}")
            response = requests.get(raw_url, timeout=30)
            
            if response.status_code == 200:
                return response.text
                
            # If that fails, try direct ExploitDB download
            alt_url = f"https://www.exploit-db.com/download/{exploit_id}"
            self.logger.info(f"First attempt failed, trying alternate URL: {alt_url}")
            response = requests.get(alt_url, timeout=30)
            
            if response.status_code == 200:
                return response.text
            
            # Try one more method - raw.githubusercontent.com
            github_url = f"https://raw.githubusercontent.com/offensive-security/exploitdb/master/{clean_path}"
            self.logger.info(f"Second attempt failed, trying GitHub URL: {github_url}")
            response = requests.get(github_url, timeout=30)
            
            if response.status_code == 200:
                return response.text
                
            self.logger.error(f"Error fetching exploit code: HTTP {response.status_code} for {raw_url}")
            return ""
            
        except Exception as e:
            self.logger.error(f"Error fetching exploit code: {str(e)}")
            return ""
    
    def import_exploit(self, exploit_data: dict, fetch_code: bool = True) -> Exploit:
        """
        Import exploit data into the database
        
        Args:
            exploit_data: Dictionary containing exploit information
            fetch_code: Whether to fetch and include the exploit code
            
        Returns:
            Exploit object or None on failure
        """
        try:
            # Check if exploit already exists
            exploit_id = exploit_data.get('exploit_id')
            if not exploit_id:
                self.logger.error("Cannot import exploit without ID")
                return None
                
            # Parse date if available
            date_published = None
            if exploit_data.get('date_published'):
                try:
                    date_str = exploit_data['date_published']
                    # Handle different date formats
                    if 'T' in date_str:
                        # ISO format with time
                        date_published = datetime.fromisoformat(date_str.split('T')[0])
                    elif '-' in date_str:
                        # YYYY-MM-DD format
                        date_published = datetime.strptime(date_str, '%Y-%m-%d')
                except Exception as date_error:
                    self.logger.warning(f"Could not parse date: {date_str} - {str(date_error)}")
            
            # Check if exploit exists before creating/updating
            exploit_exists = Exploit.objects.filter(
                exploit_id=exploit_id,
                source=self.source
            ).exists()
            
            # Create or update exploit
            exploit, created = Exploit.objects.update_or_create(
                exploit_id=exploit_id,
                source=self.source,
                defaults={
                    'title': exploit_data.get('title', 'Unknown Exploit'),
                    'description': exploit_data.get('description', ''),
                    'type': exploit_data.get('type', ''),
                    'platform': exploit_data.get('platform', ''),
                    'vulnerability_name': exploit_data.get('vulnerability_name', ''),
                    'cve_id': exploit_data.get('cve_id', ''),
                    'date_published': date_published,
                    'source_url': exploit_data.get('source_url', ''),
                    'file_path': exploit_data.get('file_path', ''),
                    'author': exploit_data.get('author', ''),
                    'verified': exploit_data.get('verified', False),
                    'metadata': exploit_data.get('metadata', {})
                }
            )
            
            # Store the creation state for stats tracking
            exploit._created = created
            exploit._updated = not created
                    
            # Fetch code if requested and not already present
            if fetch_code and not exploit.code and exploit_id:
                code = self.get_exploit_code(exploit_id)
                if code:
                    exploit.code = code
                    exploit.save()
                    
            return exploit
                
        except Exception as e:
            self.logger.error(f"Error importing exploit: {str(e)}")
            return None
    
    def sync_recent_exploits(self, limit: int = 100) -> dict:
        """
        Sync recent exploits from ExploitDB
        
        Args:
            limit: Maximum number of exploits to sync
            
        Returns:
            dict: Statistics about the sync process
        """
        stats = {
            'total': 0,
            'new': 0,
            'updated': 0,
            'failed': 0
        }
        
        try:
            # Get the maximum exploit ID we've already processed
            max_exploit_id = Exploit.objects.filter(source=self.source).aggregate(Max('exploit_id'))['exploit_id__max']
            
            # Add debug logging for max_exploit_id
            self.logger.info(f"Max exploit ID from database: {max_exploit_id}")
            self.logger.info(f"Type of max_exploit_id: {type(max_exploit_id)}")
            
            if max_exploit_id:
                max_exploit_id = int(max_exploit_id)
                self.logger.info(f"Highest exploit ID in database: {max_exploit_id}")
            else:
                max_exploit_id = 0
                self.logger.info("No existing exploits in database, will import all new ones")
            
            # Download the CSV file
            response = requests.get(self.EXPLOITDB_CSV_URL, timeout=30)
            response.raise_for_status()
            
            # Parse CSV
            csv_data = response.text
            reader = csv.DictReader(io.StringIO(csv_data))
            
            # Separate new and existing exploits
            all_exploits = []
            new_exploits = []
            existing_exploits = []
            for row in reader:
                exploit_id_str = row.get('id', '0')
                if not exploit_id_str.isdigit():
                    continue
                    
                exploit_id = int(exploit_id_str)
                if exploit_id > max_exploit_id:
                    # This is a new exploit we don't have yet
                    new_exploits.append(row)
                else:
                    # This is an existing exploit
                    existing_exploits.append(row)

            # Sort by exploit ID (newest first)
            new_exploits.sort(key=lambda x: int(x.get('id', 0)), reverse=True)
            existing_exploits.sort(key=lambda x: int(x.get('id', 0)), reverse=True)

            # Process all new exploits first, regardless of limit
            self.logger.info(f"Found {len(new_exploits)} new exploits to process")
            all_exploits.extend(new_exploits)

            # Then add existing ones up to the limit (if needed)
            if limit > len(new_exploits):
                remaining_slots = limit - len(new_exploits)
                self.logger.info(f"Adding {min(remaining_slots, len(existing_exploits))} existing exploits to reach limit of {limit}")
                all_exploits.extend(existing_exploits[:remaining_slots])
            else:
                self.logger.info(f"Processing only {limit} of {len(new_exploits)} new exploits due to limit")
                all_exploits = all_exploits[:limit]
            
            stats['total'] = len(all_exploits)
            self.logger.info(f"Processing {len(all_exploits)} exploits (limit: {limit})")
            
            # Iterate through exploits with detailed logging
            for row in all_exploits:
                try:
                    exploit_id = row.get('id', '0')
                    
                    # Check if this exploit already exists
                    exists = Exploit.objects.filter(
                        exploit_id=exploit_id,
                        source=self.source
                    ).exists()
                    
                    # Add detailed logging
                    self.logger.info(f"Processing exploit ID: {exploit_id}, already exists: {exists}")
                    
                    # Convert exploit_id to integer for further processing
                    exploit_id = int(exploit_id)
                    is_new = exploit_id > max_exploit_id
                    
                    # Extract CVE if available
                    cve_id = ''
                    if 'CVE-' in row.get('description', ''):
                        import re
                        cve_match = re.search(r'CVE-\d{4}-\d{4,}', row.get('description', ''))
                        if cve_match:
                            cve_id = cve_match.group(0)
                    
                    # Format as exploit data
                    exploit_data = {
                        'exploit_id': row.get('id', ''),
                        'title': row.get('description', '')[:255],  # Limit title length
                        'description': row.get('description', ''),
                        'date_published': row.get('date', ''),
                        'type': row.get('type', ''),
                        'platform': row.get('platform', ''),
                        'source_url': f"https://www.exploit-db.com/exploits/{row.get('id', '')}",
                        'file_path': row.get('file', ''),
                        'cve_id': cve_id,
                        'metadata': row
                    }
                    
                    # Import exploit
                    exploit = self.import_exploit(exploit_data, fetch_code=False)
                    if exploit:
                        if getattr(exploit, '_created', False):
                            stats['new'] += 1
                        else:
                            stats['updated'] += 1
                    else:
                        stats['failed'] += 1
                        
                except Exception as e:
                    self.logger.error(f"Error processing exploit: {str(e)}")
                    stats['failed'] += 1
                    
            # Update source last_update with timezone-aware datetime
            self.source.last_update = timezone.now()
            self.source.save()
            
            return stats
                
        except Exception as e:
            self.logger.error(f"Error syncing exploits: {str(e)}")
            return stats# exploit_manager/matcher.py
import logging
import re
from typing import List, Tuple, Dict, Optional

from django.db.models import Q
from django.utils import timezone

from vulnerability.models import Vulnerability
from .models import Exploit, ExploitMatch

class ExploitMatcher:
    """
    Matches vulnerabilities with potential exploits from the database
    using a generalized approach that works with any vulnerability type
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def match_vulnerability(self, vulnerability: Vulnerability) -> List[ExploitMatch]:
        """
        Find potential exploits for a vulnerability using a flexible matching approach
        
        Args:
            vulnerability: The vulnerability to match
            
        Returns:
            List of ExploitMatch objects
        """
        try:
            self.logger.info(f"Matching vulnerability: {vulnerability.name} [{vulnerability.severity}]")
            
            # First attempt classification-based matching
            matches = self._classify_and_match(vulnerability)
            
            # If no matches found, fall back to generic content-based matching
            if not matches:
                self.logger.info(f"No matches found via classification, trying generic matching")
                matches = self._generic_match(vulnerability)
            
            # If still no matches, try a last-resort approach
            if not matches:
                self.logger.info(f"No matches found via generic matching, using fallback method")
                matches = self._fallback_match(vulnerability)
            
            # Save matches to database
            saved_matches = self.save_matches(vulnerability, matches)
            
            self.logger.info(f"Found {len(saved_matches)} potential exploits for vulnerability ID {vulnerability.id}")
            return saved_matches
            
        except Exception as e:
            self.logger.error(f"Error matching vulnerability: {str(e)}")
            return []
    
    def _classify_and_match(self, vulnerability: Vulnerability) -> List[Tuple[Exploit, float, str]]:
        """
        Classify vulnerability type and apply appropriate matching strategy
        """
        vuln_name = vulnerability.name.lower()
        vuln_type = vulnerability.vuln_type.lower() if vulnerability.vuln_type else ""
        vuln_desc = vulnerability.description.lower()
        
        # Match aggregation
        matches = []
        
        # Category 1: Port-related vulnerabilities
        if ('port' in vuln_name or 'port' in vuln_type) or re.search(r'port\s+\d+', vuln_name):
            port_matches = self._match_port_vulnerabilities(vulnerability)
            matches.extend(port_matches)
        
        # Category 2: Header-related vulnerabilities
        header_terms = ['header', 'csp', 'hsts', 'clickjacking', 'content-type', 'frame-options',
                        'x-frame', 'x-content', 'strict-transport', 'csrf', 'security policy']
        if any(term in vuln_name or term in vuln_desc for term in header_terms):
            header_matches = self._match_header_vulnerabilities(vulnerability)
            matches.extend(header_matches)
        
        # Category 3: Web-related vulnerabilities
        web_terms = ['web', 'http', 'https', 'webapp', 'website', 'javascript', 'dom', 'nginx', 'apache', 
                    'iis', 'cdn', 'waf', 'wappalyzer', 'redirect', 'aspnet']
        if any(term in vuln_name or term in vuln_desc or term in vuln_type for term in web_terms):
            web_matches = self._match_web_vulnerabilities(vulnerability)
            matches.extend(web_matches)
        
        # Category 4: Authentication and access-related vulnerabilities
        auth_terms = ['auth', 'access', 'login', 'password', 'credential', 'session', 'token', 'csrf', 'authentication']
        if any(term in vuln_name or term in vuln_desc for term in auth_terms):
            auth_matches = self._match_auth_vulnerabilities(vulnerability)
            matches.extend(auth_matches)
        
        # Category 5: Information disclosure vulnerabilities
        disclosure_terms = ['disclosure', 'leak', 'information', 'debug', 'error', 'version', 'sensitive']
        if any(term in vuln_name or term in vuln_desc for term in disclosure_terms):
            disclosure_matches = self._match_disclosure_vulnerabilities(vulnerability)
            matches.extend(disclosure_matches)
        
        # Category 6: XSS and other injection vulnerabilities
        injection_terms = ['xss', 'script', 'injection', 'cross site', 'cross-site', 'sql', 'command', 'xxe']
        if any(term in vuln_name or term in vuln_desc for term in injection_terms):
            injection_matches = self._match_injection_vulnerabilities(vulnerability)
            matches.extend(injection_matches)
        
        # Category 7: SSH-related vulnerabilities
        ssh_terms = ['ssh', 'secure shell', 'openssh']
        if any(term in vuln_name or term in vuln_desc or term in vuln_type for term in ssh_terms):
            ssh_matches = self._match_ssh_vulnerabilities(vulnerability)
            matches.extend(ssh_matches)
        
        return matches
    
    def _generic_match(self, vulnerability: Vulnerability) -> List[Tuple[Exploit, float, str]]:
        """
        Generic content-based matching using keywords from vulnerability name and description
        """
        # Extract key terms from vulnerability name and description
        vuln_name = vulnerability.name.lower() if vulnerability.name else ""
        vuln_desc = vulnerability.description.lower() if vulnerability.description else ""
        vuln_type = vulnerability.vuln_type.lower() if vulnerability.vuln_type else ""
        
        # Extract keywords from name and description
        name_keywords = self._extract_keywords(vuln_name)
        desc_keywords = self._extract_keywords(vuln_desc, max_words=5)
        type_keywords = self._extract_keywords(vuln_type, max_words=2)
        
        # Combine keywords, ensuring no duplicates
        all_keywords = list(set(name_keywords + desc_keywords + type_keywords))
        
        self.logger.info(f"Generic matching with keywords: {all_keywords}")
        
        if not all_keywords:
            return []
            
        # Build query using keywords
        query = Q()
        
        # Each keyword adds to the OR query
        for keyword in all_keywords:
            if len(keyword) > 3:  # Only use keywords with more than 3 chars
                query |= Q(title__icontains=keyword) | Q(description__icontains=keyword)
        
        # Filter by exploit type based on severity for better relevance
        if vulnerability.severity == 'CRITICAL' or vulnerability.severity == 'HIGH':
            # For high severity, prioritize remote exploits
            exploits = Exploit.objects.filter(query).filter(
                Q(type__icontains='remote') | Q(type__icontains='webapps')
            ).order_by('-date_published')[:20]
        else:
            # For medium/low severity, use broader criteria
            exploits = Exploit.objects.filter(query).order_by('-date_published')[:20]
        
        matches = []
        for exploit in exploits:
            # Calculate match score based on keyword presence
            score = self._calculate_keyword_match_score(exploit, all_keywords)
            matches.append((exploit, score, "Keyword match"))
        
        return matches
    
    def _fallback_match(self, vulnerability: Vulnerability) -> List[Tuple[Exploit, float, str]]:
        """
        Last-resort matching when other strategies fail to find matches
        """
        # For fallback, we'll use a very broad approach based on severity and type
        matches = []
        
        # Categorize by severity
        if vulnerability.severity in ['CRITICAL', 'HIGH']:
            # For high severity, prioritize exploits for critical vulnerabilities
            exploits = Exploit.objects.filter(
                Q(title__icontains='critical') | 
                Q(description__icontains='critical')
            ).order_by('-date_published')[:10]
            
            for exploit in exploits:
                matches.append((exploit, 0.3, f"Severity-based match ({vulnerability.severity})"))
                
        elif vulnerability.severity == 'MEDIUM':
            # For medium severity, look for common vulnerability types
            exploits = Exploit.objects.filter(
                Q(type__icontains='remote') | 
                Q(type__icontains='webapps')
            ).order_by('-date_published')[:10]
            
            for exploit in exploits:
                matches.append((exploit, 0.2, "General exploit match"))
                
        else:  # LOW or INFO severity
            # For low severity, just get some reasonable matches
            exploits = Exploit.objects.all().order_by('-date_published')[:5]
            
            for exploit in exploits:
                matches.append((exploit, 0.1, "General vulnerability match"))
        
        return matches
    
    def _match_port_vulnerabilities(self, vulnerability: Vulnerability) -> List[Tuple[Exploit, float, str]]:
        """Match port-related vulnerabilities"""
        matches = []
        vuln_name = vulnerability.name.lower()
        
        # Extract port number and service
        port_match = re.search(r'port\s+(\d+)\s*\(?(.*?)\)?', vuln_name, re.IGNORECASE)
        if port_match:
            port_number = port_match.group(1)
            service_name = port_match.group(2).strip() if port_match.group(2) else ""
            
            self.logger.info(f"Matching port vulnerability: Port {port_number} ({service_name})")
            
            # Special handling based on port/service combinations
            if port_number in ['80', '443', '8080', '8443'] or 'http' in service_name.lower() or 'web' in service_name.lower():
                # HTTP/Web ports
                http_query = (
                    Q(title__icontains='http') | 
                    Q(description__icontains='http') | 
                    Q(title__icontains='web') | 
                    Q(description__icontains='web') | 
                    Q(type__icontains='webapps') |
                    Q(platform__icontains='php')
                )
                
                exploits = Exploit.objects.filter(http_query).order_by('-date_published')[:20]
                
                for exploit in exploits:
                    score = 0.5 if exploit.type.lower() == 'webapps' else 0.4
                    matches.append((exploit, score, f"Web/HTTP exploit for port {port_number}"))
                
            elif port_number == '22' or 'ssh' in service_name.lower():
                # SSH port
                ssh_query = (
                    Q(title__icontains='ssh') | 
                    Q(description__icontains='ssh') | 
                    Q(title__icontains='remote') | 
                    Q(description__icontains='remote access')
                )
                
                exploits = Exploit.objects.filter(ssh_query).order_by('-date_published')[:15]
                
                for exploit in exploits:
                    matches.append((exploit, 0.4, f"SSH exploit for port {port_number}"))
                
            elif port_number == '21' or 'ftp' in service_name.lower():
                # FTP port
                ftp_query = (
                    Q(title__icontains='ftp') | 
                    Q(description__icontains='ftp') | 
                    Q(title__icontains='file transfer')
                )
                
                exploits = Exploit.objects.filter(ftp_query).order_by('-date_published')[:15]
                
                for exploit in exploits:
                    matches.append((exploit, 0.4, f"FTP exploit for port {port_number}"))
                
            else:
                # Other ports - generic approach
                general_query = Q(title__icontains='port') | Q(type__icontains='remote')
                
                # Try to include service name in query if available
                if service_name:
                    service_terms = service_name.lower().split()
                    for term in service_terms:
                        if len(term) > 3:
                            general_query |= Q(title__icontains=term) | Q(description__icontains=term)
                
                exploits = Exploit.objects.filter(general_query).order_by('-date_published')[:15]
                
                for exploit in exploits:
                    matches.append((exploit, 0.3, f"Service/port exploit for port {port_number}"))
                
        elif 'open port' in vuln_name:
            # Generic open port without specific port in title
            # Try to find port info in description
            desc = vulnerability.description.lower()
            port_in_desc = re.search(r'port\s+(\d+)', desc)
            
            if port_in_desc:
                port_number = port_in_desc.group(1)
                # Recursively call with the port number we found
                dummy_vuln = Vulnerability(
                    name=f"Port {port_number} (Extracted)",
                    description=vulnerability.description,
                    severity=vulnerability.severity,
                    vuln_type=vulnerability.vuln_type
                )
                return self._match_port_vulnerabilities(dummy_vuln)
            else:
                # No specific port found, use generic remote exploits
                exploits = Exploit.objects.filter(
                    Q(type__icontains='remote') |
                    Q(title__icontains='network')
                ).order_by('-date_published')[:15]
                
                for exploit in exploits:
                    matches.append((exploit, 0.3, "Generic port/network exploit"))
        
        return matches
    
    def _match_web_vulnerabilities(self, vulnerability: Vulnerability) -> List[Tuple[Exploit, float, str]]:
        """Match web-related vulnerabilities"""
        matches = []
        vuln_name = vulnerability.name.lower()
        vuln_desc = vulnerability.description.lower()
        
        # Map of web vulnerability types to search terms
        web_vuln_types = {
            'redirect': ['redirect', 'http to https', 'forwarding'],
            'webapp': ['web application', 'webapp', 'website', 'modern web'],
            'server': ['apache', 'nginx', 'iis', 'frontend', 'server'],
            'waf': ['waf', 'web application firewall', 'firewall detection'],
            'tech': ['wappalyzer', 'technology', 'detection', 'framework'],
            'cdn': ['cdn', 'content delivery', 'akamai']
        }
        
        # Determine web vulnerability type
        web_types = []
        for wtype, terms in web_vuln_types.items():
            if any(term in vuln_name or term in vuln_desc for term in terms):
                web_types.append(wtype)
        
        # If no specific type identified, use general web type
        if not web_types:
            web_types = ['webapp']
        
        self.logger.info(f"Web vulnerability types: {web_types}")
        
        # Build query based on identified types
        query = Q(type__icontains='webapps')
        
        for wtype in web_types:
            if wtype == 'redirect':
                query |= Q(title__icontains='redirect') | Q(description__icontains='http')
            elif wtype == 'server':
                for server in ['apache', 'nginx', 'iis']:
                    if server in vuln_name or server in vuln_desc:
                        query |= Q(title__icontains=server) | Q(description__icontains=server)
            elif wtype == 'waf':
                query |= Q(title__icontains='firewall') | Q(description__icontains='waf')
            elif wtype == 'tech':
                query |= Q(title__icontains='framework') | Q(description__icontains='technology')
            elif wtype == 'cdn':
                query |= Q(title__icontains='cdn') | Q(description__icontains='content delivery')
        
        # Get exploits matching the query
        exploits = Exploit.objects.filter(query).order_by('-date_published')[:20]
        
        for exploit in exploits:
            # Score based on type match
            if exploit.type.lower() == 'webapps':
                score = 0.4
            else:
                score = 0.3
                
            matches.append((exploit, score, "Web vulnerability match"))
        
        # If we didn't find enough matches, broaden the search
        if len(matches) < 5:
            general_web_query = Q(type__icontains='webapps') | Q(title__icontains='web')
            general_exploits = Exploit.objects.filter(general_web_query).order_by('-date_published')[:10]
            
            for exploit in general_exploits:
                if not any(exploit.id == match[0].id for match in matches):  # Avoid duplicates
                    matches.append((exploit, 0.3, "General web exploit"))
        
        return matches
    
    def _match_header_vulnerabilities(self, vulnerability: Vulnerability) -> List[Tuple[Exploit, float, str]]:
        """Match header-related vulnerabilities (HSTS, CSP, X-Frame-Options, etc.)"""
        matches = []
        vuln_name = vulnerability.name.lower()
        vuln_desc = vulnerability.description.lower()
        
        # Map header types to related search terms
        header_types = {
            'hsts': ['hsts', 'strict-transport-security', 'transport security'],
            'csp': ['csp', 'content security policy', 'content-security-policy'],
            'clickjacking': ['clickjacking', 'x-frame-options', 'frame-options', 'anti-clickjacking'],
            'content-type': ['content-type-options', 'x-content-type-options', 'mime', 'sniff'],
            'csrf': ['csrf', 'cross-site request forgery', 'anti-csrf'],
            'powered-by': ['x-powered-by', 'server leaks', 'information header'],
            'version': ['version information', 'server header', 'version detect']
        }
        
        # Determine header type(s)
        detected_types = []
        for htype, terms in header_types.items():
            if any(term in vuln_name or term in vuln_desc for term in terms):
                detected_types.append(htype)
        
        self.logger.info(f"Header vulnerability types: {detected_types}")
        
        # If we identified specific types, build a targeted query
        if detected_types:
            query = Q()
            
            for htype in detected_types:
                if htype == 'hsts':
                    query |= Q(title__icontains='transport') | Q(description__icontains='https')
                elif htype == 'csp':
                    query |= Q(title__icontains='security policy') | Q(description__icontains='csp')
                elif htype == 'clickjacking':
                    query |= Q(title__icontains='clickjacking') | Q(description__icontains='frame')
                elif htype == 'content-type':
                    query |= Q(title__icontains='mime') | Q(description__icontains='sniff')
                elif htype == 'csrf':
                    query |= Q(title__icontains='csrf') | Q(description__icontains='forgery')
                elif htype in ['powered-by', 'version']:
                    query |= Q(title__icontains='disclosure') | Q(description__icontains='information')
            
            # Get exploits for specific header types
            specific_exploits = Exploit.objects.filter(query & Q(type__icontains='webapps')).order_by('-date_published')[:15]
            
            for exploit in specific_exploits:
                matches.append((exploit, 0.4, f"Header security match: {', '.join(detected_types)}"))
        
        # Always add some generic web security exploits as these often address header issues
        general_query = (
            Q(title__icontains='web security') | 
            Q(description__icontains='header') |
            Q(title__icontains='misconfiguration') |
            Q(description__icontains='security headers')
        )
        
        general_exploits = Exploit.objects.filter(general_query).order_by('-date_published')[:10]
        
        for exploit in general_exploits:
            # Avoid duplicates
            if not any(exploit.id == match[0].id for match in matches):
                matches.append((exploit, 0.3, "Web security configuration exploit"))
        
        return matches
    
    def _match_auth_vulnerabilities(self, vulnerability: Vulnerability) -> List[Tuple[Exploit, float, str]]:
        """Match authentication and access control vulnerabilities"""
        matches = []
        
        # Build query for auth-related vulnerabilities
        auth_query = (
            Q(title__icontains='auth') | 
            Q(description__icontains='authentication') |
            Q(title__icontains='login') | 
            Q(description__icontains='access control') |
            Q(title__icontains='bypass') | 
            Q(description__icontains='credential')
        )
        
        # Get exploits
        auth_exploits = Exploit.objects.filter(auth_query).order_by('-date_published')[:15]
        
        for exploit in auth_exploits:
            matches.append((exploit, 0.4, "Authentication/access vulnerability match"))
        
        return matches
    
    def _match_disclosure_vulnerabilities(self, vulnerability: Vulnerability) -> List[Tuple[Exploit, float, str]]:
        """Match information disclosure vulnerabilities"""
        matches = []
        vuln_name = vulnerability.name.lower()
        
        # Build query for disclosure vulnerabilities
        disclosure_query = (
            Q(title__icontains='disclosure') | 
            Q(description__icontains='information leak') |
            Q(title__icontains='sensitive') | 
            Q(description__icontains='exposure')
        )
        
        # Refine if it's debug/error related
        if 'debug' in vuln_name or 'error' in vuln_name:
            disclosure_query &= (
                Q(title__icontains='debug') | 
                Q(description__icontains='error') |
                Q(title__icontains='exception') | 
                Q(description__icontains='message')
            )
        
        # Get exploits
        disclosure_exploits = Exploit.objects.filter(disclosure_query).order_by('-date_published')[:15]
        
        for exploit in disclosure_exploits:
            matches.append((exploit, 0.4, "Information disclosure vulnerability match"))
        
        return matches
    
    def _match_injection_vulnerabilities(self, vulnerability: Vulnerability) -> List[Tuple[Exploit, float, str]]:
        """Match XSS and other injection vulnerabilities"""
        matches = []
        vuln_name = vulnerability.name.lower()
        vuln_desc = vulnerability.description.lower()
        
        # Determine injection type
        injection_type = None
        if 'xss' in vuln_name or 'cross site scripting' in vuln_name or 'cross-site scripting' in vuln_name:
            injection_type = 'xss'
            
            # Check for XSS type
            if 'dom' in vuln_name or 'dom based' in vuln_name or 'dom' in vuln_desc:
                injection_type = 'dom-xss'
            elif 'stored' in vuln_name or 'persistent' in vuln_name:
                injection_type = 'stored-xss'
            elif 'reflected' in vuln_name:
                injection_type = 'reflected-xss'
                
        elif 'sql' in vuln_name or 'sql injection' in vuln_name:
            injection_type = 'sqli'
            
            # Check for SQL injection type
            if 'blind' in vuln_name or 'blind' in vuln_desc:
                injection_type = 'blind-sqli'
            elif 'time' in vuln_name or 'time based' in vuln_name:
                injection_type = 'time-sqli'
                
        elif 'command' in vuln_name and 'injection' in vuln_name:
            injection_type = 'cmdi'
        elif 'xxe' in vuln_name or 'xml' in vuln_name:
            injection_type = 'xxe'
        
        # Build query based on injection type
        query = Q()
        
        if injection_type in ['xss', 'dom-xss', 'stored-xss', 'reflected-xss']:
            query = (
                Q(title__icontains='xss') | 
                Q(description__icontains='cross site scripting') |
                Q(title__icontains='cross-site') | 
                Q(description__icontains='script injection')
            )
            
            # Refine for specific XSS types
            if injection_type == 'dom-xss':
                query &= Q(title__icontains='dom') | Q(description__icontains='dom')
            elif injection_type == 'stored-xss':
                query &= Q(title__icontains='stored') | Q(description__icontains='persistent')
            elif injection_type == 'reflected-xss':
                query &= Q(title__icontains='reflected') | Q(description__icontains='reflected')
                
        elif injection_type in ['sqli', 'blind-sqli', 'time-sqli']:
            query = (
                Q(title__icontains='sql') | 
                Q(description__icontains='sql injection') |
                Q(title__icontains='sqli')
            )
            
            # Refine for specific SQL injection types
            if injection_type == 'blind-sqli':
                query &= Q(title__icontains='blind') | Q(description__icontains='blind')
            elif injection_type == 'time-sqli':
                query &= Q(title__icontains='time') | Q(description__icontains='time based')
                
        elif injection_type == 'cmdi':
            query = (
                Q(title__icontains='command') | 
                Q(description__icontains='command injection') |
                Q(title__icontains='os command') | 
                Q(description__icontains='shell command')
            )
            
        elif injection_type == 'xxe':
            query = (
                Q(title__icontains='xxe') | 
                Q(description__icontains='xml external entity') |
                Q(title__icontains='xml injection')
            )
            
        else:
            # Generic injection query
            query = (
                Q(title__icontains='injection') | 
                Q(description__icontains='injection')
            )
        
        # Get matching exploits
        injection_exploits = Exploit.objects.filter(query).order_by('-date_published')[:15]
        
        for exploit in injection_exploits:
            if injection_type:
                match_reason = f"{injection_type.upper()} vulnerability match"
            else:
                match_reason = "Injection vulnerability match"
                
            matches.append((exploit, 0.5, match_reason))  # Higher score for injection vulnerabilities
        
        return matches
    
    def _match_ssh_vulnerabilities(self, vulnerability: Vulnerability) -> List[Tuple[Exploit, float, str]]:
        """Match SSH-related vulnerabilities"""
        matches = []
        vuln_name = vulnerability.name.lower()
        vuln_desc = vulnerability.description.lower()
        
        # Build query for SSH vulnerabilities
        ssh_query = (
            Q(title__icontains='ssh') | 
            Q(description__icontains='secure shell') |
            Q(title__icontains='openssh') | 
            Q(platform__icontains='ssh')
        )
        
        # Enhance query based on specifics
        if 'auth' in vuln_name or 'authentication' in vuln_name or 'auth' in vuln_desc:
            ssh_query &= (
                Q(title__icontains='auth') | 
                Q(description__icontains='authentication') |
                Q(title__icontains='login')
            )
            
            auth_exploits = Exploit.objects.filter(ssh_query).order_by('-date_published')[:15]
            
            for exploit in auth_exploits:
                matches.append((exploit, 0.5, "SSH authentication vulnerability match"))
        else:
            # General SSH exploits
            general_ssh_exploits = Exploit.objects.filter(ssh_query).order_by('-date_published')[:15]
            
            for exploit in general_ssh_exploits:
                matches.append((exploit, 0.4, "SSH service vulnerability match"))
        
        # If no SSH-specific exploits, try broader remote access exploits
        if not matches:
            remote_query = (
                Q(type__icontains='remote') & 
                (Q(title__icontains='authentication') | Q(description__icontains='login'))
            )
            
            remote_exploits = Exploit.objects.filter(remote_query).order_by('-date_published')[:10]
            
            for exploit in remote_exploits:
                matches.append((exploit, 0.3, "Remote access vulnerability match"))
        
        return matches
    
    def _extract_keywords(self, text: str, max_words: int = 6) -> List[str]:
        """Extract keywords from text, excluding common words"""
        if not text:
            return []
            
        # Common words to exclude
        common_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', 'be', 'been',
            'has', 'have', 'had', 'do', 'does', 'did', 'not', 'on', 'in', 'at', 'to', 'for',
            'with', 'by', 'about', 'against', 'between', 'into', 'through', 'during', 'before',
            'after', 'above', 'below', 'from', 'up', 'down', 'of', 'this', 'that', 'these', 
            'those', 'should', 'could', 'would', 'will', 'can', 'may', 'might', 'must', 'server',
            'vulnerability', 'issue', 'security', 'missing', 'detection', 'found', 'version'
        }
        
        # Split text into words, convert to lowercase, and filter
        words = re.findall(r'\b\w+\b', text.lower())
        # Split text into words, convert to lowercase, and filter
        words = re.findall(r'\b\w+\b', text.lower())
        keywords = [word for word in words if word not in common_words and len(word) > 2]
        
        # Limit number of keywords
        return keywords[:max_words]
    
    def _calculate_keyword_match_score(self, exploit: Exploit, keywords: List[str]) -> float:
        """Calculate match score based on keyword presence in exploit"""
        if not keywords:
            return 0.2  # Baseline score
            
        title = exploit.title.lower()
        description = exploit.description.lower() if exploit.description else ""
        
        # Count matches in title (weighted higher)
        title_matches = sum(1 for keyword in keywords if keyword in title)
        
        # Count matches in description
        desc_matches = sum(1 for keyword in keywords if keyword in description)
        
        # Calculate weighted score
        total_keywords = len(keywords)
        if total_keywords == 0:
            return 0.2
            
        score = (title_matches * 2 + desc_matches) / (total_keywords * 3)
        
        # Scale to appropriate range (0.2 - 0.8)
        return min(0.8, max(0.2, score * 0.8))
    
    def save_matches(self, vulnerability: Vulnerability, matches: List[Tuple[Exploit, float, str]]) -> List[ExploitMatch]:
        """Save matches to database and return saved objects"""
        saved_matches = []
        
        # Sort by confidence score (highest first)
        sorted_matches = sorted(matches, key=lambda x: x[1], reverse=True)
        
        # Limit number of matches to save
        max_matches = 10
        sorted_matches = sorted_matches[:max_matches]
        
        for exploit, confidence, reason in sorted_matches:
            try:
                # Skip exploits with very low confidence
                if confidence < 0.1:
                    continue
                    
                # Get or create match object
                match, created = ExploitMatch.objects.get_or_create(
                    vulnerability=vulnerability,
                    exploit=exploit,
                    defaults={
                        'confidence_score': confidence,
                        'match_reason': reason,
                        'status': 'pending',
                        # match_date is auto_now_add - no need to set it explicitly
                    }
                )
                
                # Update if it already existed
                if not created:
                    match.confidence_score = confidence
                    match.match_reason = reason
                    # last_updated has auto_now=True - no need to set it explicitly
                    match.save()
                
                saved_matches.append(match)
                
            except Exception as e:
                self.logger.error(f"Error saving match: {str(e)}")
        
        return saved_matches# automation/models.py

from django.db import models
from django.utils import timezone

class ScanWorkflow(models.Model):
    """
    Represents a complete security scanning workflow
    """
    STATUS_CHOICES = [
        ('pending', 'Pending'),
        ('scheduled', 'Scheduled'),
        ('in_progress', 'In Progress'),
        ('completed', 'Completed'),
        ('failed', 'Failed'),
        ('canceled', 'Canceled')
    ]
    
    PROFILE_CHOICES = [
        ('quick', 'Quick Scan'),
        ('standard', 'Standard Scan'),
        ('full', 'Full Scan')
    ]
    
    name = models.CharField(max_length=255)
    target = models.CharField(max_length=255)
    scan_profile = models.CharField(max_length=20, choices=PROFILE_CHOICES, default='standard')
    status = models.CharField(max_length=20, choices=STATUS_CHOICES, default='pending')
    
    # Timing information
    created_at = models.DateTimeField(auto_now_add=True)
    scheduled_time = models.DateTimeField(null=True, blank=True)
    start_time = models.DateTimeField(null=True, blank=True)
    end_time = models.DateTimeField(null=True, blank=True)
    
    # Notification settings
    notification_email = models.EmailField(null=True, blank=True)
    
    # Additional metadata
    metadata = models.JSONField(default=dict, blank=True)
    
    class Meta:
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['status']),
            models.Index(fields=['target']),
            models.Index(fields=['created_at']),
        ]
    
    def __str__(self):
        return f"{self.name} - {self.target} ({self.status})"
    
    @property
    def duration(self):
        """Calculate workflow duration in seconds"""
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return None
    
    @property
    def is_active(self):
        """Check if workflow is active (not in terminal state)"""
        return self.status in ['pending', 'scheduled', 'in_progress']
    
    @property
    def is_scheduled(self):
        """Check if workflow is scheduled for future execution"""
        return self.status == 'scheduled' and self.scheduled_time and self.scheduled_time > timezone.now()


class ScanTask(models.Model):
    """
    Represents an individual task within a scanning workflow
    """
    STATUS_CHOICES = [
        ('pending', 'Pending'),
        ('in_progress', 'In Progress'),
        ('completed', 'Completed'),
        ('failed', 'Failed'),
        ('skipped', 'Skipped'),
        ('canceled', 'Canceled')
    ]
    
    TASK_TYPE_CHOICES = [
        ('subdomain_enumeration', 'Subdomain Enumeration'),
        ('port_scanning', 'Port Scanning'),
        ('service_identification', 'Service Identification'),
        ('vulnerability_scanning', 'Vulnerability Scanning'),
        ('network_mapping', 'Network Mapping'),
        ('report_generation', 'Report Generation')
    ]
    
    workflow = models.ForeignKey(ScanWorkflow, on_delete=models.CASCADE, related_name='tasks')
    task_type = models.CharField(max_length=50, choices=TASK_TYPE_CHOICES)
    name = models.CharField(max_length=255)
    status = models.CharField(max_length=20, choices=STATUS_CHOICES, default='pending')
    
    # Task dependencies
    dependencies = models.ManyToManyField('self', symmetrical=False, related_name='dependents', blank=True)
    order = models.IntegerField(default=0)
    
    # Timing information
    created_at = models.DateTimeField(auto_now_add=True)
    start_time = models.DateTimeField(null=True, blank=True)
    end_time = models.DateTimeField(null=True, blank=True)
    
    # Results
    result = models.TextField(null=True, blank=True)
    
    class Meta:
        ordering = ['workflow', 'order']
        indexes = [
            models.Index(fields=['workflow', 'status']),
            models.Index(fields=['task_type']),
        ]
    
    def __str__(self):
        return f"{self.name} ({self.status})"
    
    @property
    def duration(self):
        """Calculate task duration in seconds"""
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return None
    
    @property
    def has_dependencies(self):
        """Check if task has dependencies"""
        return self.dependencies.exists()
    
    @property
    def is_blocked(self):
        """Check if any dependencies are not completed"""
        return self.dependencies.exclude(status='completed').exists()


class Notification(models.Model):
    """
    Represents a notification sent to a user
    """
    NOTIFICATION_TYPE_CHOICES = [
        ('workflow_scheduled', 'Workflow Scheduled'),
        ('workflow_started', 'Workflow Started'),
        ('workflow_completed', 'Workflow Completed'),
        ('workflow_failed', 'Workflow Failed'),
        ('workflow_canceled', 'Workflow Canceled'),
        ('task_failed', 'Task Failed'),
        ('critical_vulnerabilities', 'Critical Vulnerabilities'),
        ('report_ready', 'Report Ready')
    ]
    
    workflow = models.ForeignKey(ScanWorkflow, on_delete=models.CASCADE, related_name='notifications')
    notification_type = models.CharField(max_length=50, choices=NOTIFICATION_TYPE_CHOICES)
    recipient = models.EmailField()
    subject = models.CharField(max_length=255)
    message = models.TextField()
    
    created_at = models.DateTimeField(auto_now_add=True)
    sent = models.BooleanField(default=False)
    sent_time = models.DateTimeField(null=True, blank=True)
    
    # For tracking email opens, clicks, etc.
    metadata = models.JSONField(default=dict, blank=True)
    
    class Meta:
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['notification_type']),
            models.Index(fields=['created_at']),
            models.Index(fields=['sent']),
        ]
    
    def __str__(self):
        return f"{self.notification_type} for {self.workflow.name}"


class ScheduledTask(models.Model):
    """
    Represents a recurring scheduled task/scan
    """
    FREQUENCY_CHOICES = [
        ('daily', 'Daily'),
        ('weekly', 'Weekly'),
        ('monthly', 'Monthly'),
        ('custom', 'Custom')
    ]
    
    name = models.CharField(max_length=255)
    target = models.CharField(max_length=255)
    scan_profile = models.CharField(max_length=20, choices=ScanWorkflow.PROFILE_CHOICES, default='standard')
    
    # Schedule
    frequency = models.CharField(max_length=20, choices=FREQUENCY_CHOICES)
    cron_expression = models.CharField(max_length=100, null=True, blank=True)
    start_date = models.DateField()
    end_date = models.DateField(null=True, blank=True)
    
    # Active flag
    is_active = models.BooleanField(default=True)
    
    # Notification settings
    notification_email = models.EmailField(null=True, blank=True)
    
    # Created by
    created_at = models.DateTimeField(auto_now_add=True)
    created_by = models.CharField(max_length=100, null=True, blank=True)
    
    # Last execution
    last_execution = models.DateTimeField(null=True, blank=True)
    last_status = models.CharField(max_length=20, null=True, blank=True)
    last_workflow = models.ForeignKey(ScanWorkflow, on_delete=models.SET_NULL, null=True, blank=True, related_name='schedule')
    
    class Meta:
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['is_active']),
            models.Index(fields=['frequency']),
            models.Index(fields=['target']),
        ]
    
    def __str__(self):
        return f"{self.name} - {self.target} ({self.frequency})"# automation/views.py

from rest_framework import viewsets, status
from rest_framework.decorators import action
from rest_framework.response import Response
from django.shortcuts import get_object_or_404
from django.utils import timezone
import logging
import threading

from .models import ScanWorkflow, ScanTask, ScheduledTask, Notification
from .serializers import (
    WorkflowSerializer, WorkflowCreateSerializer,
    ScanTaskSerializer, ScheduledTaskSerializer,
    NotificationSerializer
)
from .workflow_orchestrator import WorkflowOrchestrator
from .scheduler import ScanScheduler

logger = logging.getLogger(__name__)

class WorkflowViewSet(viewsets.ModelViewSet):
    """
    API endpoint for scan workflows
    """
    queryset = ScanWorkflow.objects.all().order_by('-created_at')
    serializer_class = WorkflowSerializer
    # Removed authentication requirement
    
    def get_serializer_class(self):
        if self.action == 'create':
            return WorkflowCreateSerializer
        return WorkflowSerializer
    
    def perform_create(self, serializer):
        """Create a new workflow using the orchestrator"""
        # Save the model first
        workflow = serializer.save()
        
        # Set up the workflow with tasks and dependencies
        orchestrator = WorkflowOrchestrator()
        
        # Set target and other values for the existing workflow
        workflow.target = workflow.target
        workflow.name = workflow.name
        workflow.scan_profile = workflow.scan_profile
        workflow.scheduled_time = workflow.scheduled_time
        workflow.notification_email = workflow.notification_email
        workflow.save()
        
        # Setup tasks for the existing workflow instead of creating a new one
        orchestrator.setup_workflow(
            workflow=workflow,
            target=workflow.target,
            scan_profile=workflow.scan_profile
        )
        
        # If it's not scheduled, start it immediately
        if not workflow.scheduled_time:
            # Run in background thread to avoid blocking the API response
            threading.Thread(
                target=orchestrator.start_workflow,
                args=(workflow.id,),
                daemon=True
            ).start()
    
    @action(detail=True, methods=['post'])
    def start(self, request, pk=None):
        """Start a workflow"""
        workflow = self.get_object()
        
        # Check if workflow can be started
        if workflow.status not in ['pending', 'scheduled']:
            return Response(
                {'error': f"Cannot start workflow in '{workflow.status}' status"},
                status=status.HTTP_400_BAD_REQUEST
            )
        
        # Start the workflow
        orchestrator = WorkflowOrchestrator()
        
        # Run in background thread
        threading.Thread(
            target=orchestrator.start_workflow,
            args=(workflow.id,),
            daemon=True
        ).start()
        
        return Response({'status': 'starting workflow'})
    
    @action(detail=True, methods=['post'])
    def cancel(self, request, pk=None):
        """Cancel a workflow"""
        workflow = self.get_object()
        
        # Check if workflow can be cancelled
        if workflow.status in ['completed', 'failed', 'canceled']:
            return Response(
                {'error': f"Cannot cancel workflow in '{workflow.status}' status"},
                status=status.HTTP_400_BAD_REQUEST
            )
        
        # Cancel the workflow
        orchestrator = WorkflowOrchestrator()
        success = orchestrator.cancel_workflow(workflow.id)
        
        if success:
            return Response({'status': 'workflow canceled'})
        else:
            return Response(
                {'error': 'Failed to cancel workflow'},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
    
    @action(detail=True, methods=['get'])
    def status(self, request, pk=None):
        """Get detailed workflow status"""
        workflow = self.get_object()
        
        orchestrator = WorkflowOrchestrator()
        workflow_status = orchestrator.get_workflow_status(workflow.id)
        
        return Response(workflow_status)
    
    @action(detail=True, methods=['get'])
    def tasks(self, request, pk=None):
        """Get tasks for a workflow"""
        workflow = self.get_object()
        tasks = ScanTask.objects.filter(workflow=workflow).order_by('order')
        serializer = ScanTaskSerializer(tasks, many=True)
        return Response(serializer.data)
    
    @action(detail=True, methods=['get'])
    def notifications(self, request, pk=None):
        """Get notifications for a workflow"""
        workflow = self.get_object()
        notifications = Notification.objects.filter(workflow=workflow).order_by('-created_at')
        serializer = NotificationSerializer(notifications, many=True)
        return Response(serializer.data)

class ScheduledTaskViewSet(viewsets.ModelViewSet):
    """
    API endpoint for scheduled tasks
    """
    queryset = ScheduledTask.objects.all().order_by('-created_at')
    serializer_class = ScheduledTaskSerializer
    # Removed authentication requirement
    
    def perform_create(self, serializer):
        """Create a new scheduled task"""
        # Add the current user as creator if provided in request data
        if not serializer.validated_data.get('created_by') and hasattr(self.request, 'user') and self.request.user.is_authenticated:
            serializer.validated_data['created_by'] = self.request.user.username
            
        serializer.save()
    
    @action(detail=True, methods=['post'])
    def enable(self, request, pk=None):
        """Enable a scheduled task"""
        scheduler = ScanScheduler()
        success = scheduler.enable_scheduled_task(pk)
        
        if success:
            return Response({'status': 'scheduled task enabled'})
        else:
            return Response(
                {'error': 'Failed to enable scheduled task'},
                status=status.HTTP_404_NOT_FOUND
            )
    
    @action(detail=True, methods=['post'])
    def disable(self, request, pk=None):
        """Disable a scheduled task"""
        scheduler = ScanScheduler()
        success = scheduler.disable_scheduled_task(pk)
        
        if success:
            return Response({'status': 'scheduled task disabled'})
        else:
            return Response(
                {'error': 'Failed to disable scheduled task'},
                status=status.HTTP_404_NOT_FOUND
            )
    
    @action(detail=True, methods=['post'])
    def run_now(self, request, pk=None):
        """Run a scheduled task immediately"""
        scheduled_task = self.get_object()
        
        # Create and start a workflow
        orchestrator = WorkflowOrchestrator()
        scheduler = ScanScheduler()
        
        try:
            # Create workflow with name including "manual run"
            name = f"{scheduled_task.name} - Manual Run - {timezone.now().strftime('%Y-%m-%d %H:%M')}"
            
            workflow = orchestrator.create_workflow(
                target=scheduled_task.target,
                name=name,
                scan_profile=scheduled_task.scan_profile,
                notify_email=scheduled_task.notification_email
            )
            
            # Start workflow in background
            threading.Thread(
                target=orchestrator.start_workflow,
                args=(workflow.id,),
                daemon=True
            ).start()
            
            # Update scheduled task
            scheduled_task.last_execution = timezone.now()
            scheduled_task.last_status = 'started'
            scheduled_task.last_workflow = workflow
            scheduled_task.save()
            
            return Response({
                'status': 'scheduled task triggered',
                'workflow_id': workflow.id
            })
        
        except Exception as e:
            logger.error(f"Error running scheduled task: {str(e)}")
            return Response(
                {'error': f"Failed to run scheduled task: {str(e)}"},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )


class TaskViewSet(viewsets.ReadOnlyModelViewSet):
    """
    API endpoint for viewing scan tasks
    """
    queryset = ScanTask.objects.all().order_by('-created_at')
    serializer_class = ScanTaskSerializer
    # Removed authentication requirement
    
    @action(detail=True, methods=['get'])
    def result(self, request, pk=None):
        """Get task result data"""
        task = self.get_object()
        
        if not task.result:
            return Response({'result': None})
        
        import json
        try:
            result_data = json.loads(task.result)
            return Response(result_data)
        except json.JSONDecodeError:
            return Response(
                {'error': 'Invalid result data'},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )


class NotificationViewSet(viewsets.ReadOnlyModelViewSet):
    """
    API endpoint for viewing notifications
    """
    queryset = Notification.objects.all().order_by('-created_at')
    serializer_class = NotificationSerializer
    # Removed authentication requirement# automation/workflow_orchestrator.py

import logging
import json
import socket
import time
from datetime import datetime, timedelta
from typing import Dict, List
from django.utils import timezone
from django.db import transaction
from django.conf import settings
from urllib.parse import urlparse

from reconnaissance.subdomain_enumerator import SubdomainEnumerator
from reconnaissance.scanner import PortScanner
from reconnaissance.service_identifier import ServiceIdentifier
from vulnerability.unified_scanner import UnifiedVulnerabilityScanner
from network_visualization.topology_mapper import TopologyMapper
from reporting.report_generator import ReportGenerator
from .models import ScanWorkflow, ScanTask, Notification
from .notification_manager import NotificationManager

logger = logging.getLogger(__name__)

class WorkflowOrchestrator:
    """
    Orchestrates the complete scanning workflow including reconnaissance,
    vulnerability scanning, network visualization, and report generation.
    
    Supports automatic scheduling, task dependencies, and failure handling.
    """
    
    # Workflow task types
    TASK_TYPES = {
        'subdomain_enumeration': 'Subdomain Enumeration',
        'port_scanning': 'Port Scanning',
        'service_identification': 'Service Identification',
        'vulnerability_scanning': 'Vulnerability Scanning',
        'exploit_matching': 'Exploit Matching',  # New task type
        'network_mapping': 'Network Mapping',
        'report_generation': 'Report Generation'
    }
    
    # Task dependencies - keys depend on values
    TASK_DEPENDENCIES = {
        'port_scanning': ['subdomain_enumeration'],
        'service_identification': ['port_scanning'],
        'vulnerability_scanning': ['service_identification'],
        'exploit_matching': ['vulnerability_scanning'],  # New dependency
        'network_mapping': ['service_identification'],
        'report_generation': ['vulnerability_scanning', 'network_mapping', 'exploit_matching']  # Updated dependency
    }
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.subdomain_enumerator = SubdomainEnumerator()
        self.port_scanner = PortScanner()
        self.service_identifier = ServiceIdentifier()
        self.vulnerability_scanner = UnifiedVulnerabilityScanner()
        self.topology_mapper = TopologyMapper()
        self.report_generator = ReportGenerator()
        self.notification_manager = NotificationManager()
    
    def parse_target_url(self, url: str) -> str:
        """
        Parse a target URL and extract just the hostname.
        Handles various URL formats including those with protocols, paths, query strings, etc.
        
        Args:
            url: The target URL to parse
            
        Returns:
            str: The clean hostname
        """
        # Handle empty values
        if not url:
            return ""
        
        try:
            # Remove protocol if present by using urlparse
            parsed = urlparse(url)
            
            # If netloc is empty, the URL might not have a protocol
            if not parsed.netloc:
                # Try adding a protocol and parsing again
                parsed = urlparse(f"http://{url}")
            
            # Extract just the hostname (netloc without port)
            hostname = parsed.netloc
            if ':' in hostname:
                hostname = hostname.split(':', 1)[0]
                
            # If still empty, return the original url as a last resort
            if not hostname:
                return url
                
            return hostname
        except Exception as e:
            self.logger.error(f"Error parsing URL {url}: {str(e)}")
            # Return the original URL if parsing fails
            return url
    
    def setup_workflow(self, workflow, target: str, scan_profile: str = 'standard'):
        """
        Set up tasks for an existing workflow
        
        Args:
            workflow: The existing ScanWorkflow object
            target: The domain or IP to scan
            scan_profile: Scan intensity level (quick, standard, full)
            
        Returns:
            Updated workflow
        """
        with transaction.atomic():
            # Create tasks with proper dependencies
            task_ids = {}
            
            # Create all tasks first
            for task_type, task_name in self.TASK_TYPES.items():
                task = ScanTask.objects.create(
                    workflow=workflow,
                    task_type=task_type,
                    name=f"{task_name} - {target}",
                    status='pending',
                    order=list(self.TASK_TYPES.keys()).index(task_type)
                )
                task_ids[task_type] = task.id
            
            # Set dependencies
            for task_type, dependencies in self.TASK_DEPENDENCIES.items():
                task = ScanTask.objects.get(id=task_ids[task_type])
                for dependency in dependencies:
                    dependency_task = ScanTask.objects.get(id=task_ids[dependency])
                    task.dependencies.add(dependency_task)
                task.save()
                
            # If scheduled for the future, create a notification 
            if workflow.scheduled_time and workflow.notification_email:
                Notification.objects.create(
                    workflow=workflow,
                    notification_type='workflow_scheduled',
                    recipient=workflow.notification_email,
                    subject=f"Scan scheduled: {workflow.name}",
                    message=f"A security scan for {target} has been scheduled to start at {workflow.scheduled_time}."
                )
                
            return workflow
        
    def _complete_workflow(self, workflow: ScanWorkflow) -> None:
        """Mark workflow as completed and send notifications"""
        workflow.status = 'completed'
        workflow.end_time = timezone.now()
        workflow.save()
        
        self.logger.info(f"Workflow {workflow.id} for {workflow.target} completed successfully")
        
        # Send completion notification
        if workflow.notification_email:
            self.notification_manager.send_workflow_completion_notification(workflow)
                
    def create_workflow(self, target: str, name: str = None, scan_profile: str = 'standard',
                      scheduled_time: datetime = None, notify_email: str = None) -> ScanWorkflow:
        """
        Create a new scanning workflow for a target
        
        Args:
            target: The domain or IP to scan
            name: Optional name for this workflow
            scan_profile: Scan intensity level (quick, standard, full)
            scheduled_time: When to start the scan (None = immediate)
            notify_email: Email to notify when scan completes
            
        Returns:
            ScanWorkflow object
        """
        if not name:
            name = f"Scan {target} - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
            
        with transaction.atomic():
            # Create the workflow
            workflow = ScanWorkflow.objects.create(
                name=name,
                target=target,
                scan_profile=scan_profile,
                scheduled_time=scheduled_time,
                status='scheduled' if scheduled_time else 'pending',
                notification_email=notify_email
            )
            
            # Create tasks with proper dependencies
            task_ids = {}
            
            # Create all tasks first
            for task_type, task_name in self.TASK_TYPES.items():
                task = ScanTask.objects.create(
                    workflow=workflow,
                    task_type=task_type,
                    name=f"{task_name} - {target}",
                    status='pending',
                    order=list(self.TASK_TYPES.keys()).index(task_type)
                )
                task_ids[task_type] = task.id
            
            # Set dependencies
            for task_type, dependencies in self.TASK_DEPENDENCIES.items():
                task = ScanTask.objects.get(id=task_ids[task_type])
                for dependency in dependencies:
                    dependency_task = ScanTask.objects.get(id=task_ids[dependency])
                    task.dependencies.add(dependency_task)
                task.save()
                
            # If scheduled for the future, create a notification 
            if scheduled_time and notify_email:
                Notification.objects.create(
                    workflow=workflow,
                    notification_type='workflow_scheduled',
                    recipient=notify_email,
                    subject=f"Scan scheduled: {name}",
                    message=f"A security scan for {target} has been scheduled to start at {scheduled_time}."
                )
                
            return workflow
    
    def start_workflow(self, workflow_id: int) -> bool:
        """
        Start a workflow by ID
        
        Args:
            workflow_id: ID of the workflow to start
            
        Returns:
            bool: True if successfully started
        """
        try:
            workflow = ScanWorkflow.objects.get(id=workflow_id)
            
            # Check if it's time to start a scheduled workflow
            if workflow.status == 'scheduled' and workflow.scheduled_time:
                now = timezone.now()
                if now < workflow.scheduled_time:
                    self.logger.info(f"Workflow {workflow_id} is scheduled for {workflow.scheduled_time}, not starting yet")
                    return False
            
            # Update workflow status
            workflow.status = 'in_progress'
            workflow.start_time = timezone.now()
            workflow.save()
            
            self.logger.info(f"Starting workflow {workflow_id} for target {workflow.target}")
            
            # Get tasks with no dependencies (entry points)
            entry_tasks = ScanTask.objects.filter(
                workflow=workflow, 
                dependencies__isnull=True
            ).order_by('order')
            
            # Start entry tasks
            for task in entry_tasks:
                self._execute_task(task)
                
            return True
            
        except ScanWorkflow.DoesNotExist:
            self.logger.error(f"Workflow {workflow_id} not found")
            return False
        except Exception as e:
            self.logger.error(f"Error starting workflow {workflow_id}: {str(e)}")
            return False
    
    def check_pending_workflows(self) -> int:
        """
        Check for scheduled workflows that should be started
        
        Returns:
            int: Number of workflows started
        """
        now = timezone.now()
        
        # Find scheduled workflows that should start now
        scheduled_workflows = ScanWorkflow.objects.filter(
            status='scheduled',
            scheduled_time__lte=now
        )
        
        count = 0
        for workflow in scheduled_workflows:
            if self.start_workflow(workflow.id):
                count += 1
                
        return count
    
    def process_workflow_queue(self) -> int:
        """
        Process the workflow queue - check for tasks that can be started
        
        Returns:
            int: Number of tasks started
        """
        # Find in-progress workflows
        active_workflows = ScanWorkflow.objects.filter(
            status='in_progress'
        )
        
        tasks_started = 0
        
        for workflow in active_workflows:
            # Get pending tasks for this workflow
            pending_tasks = ScanTask.objects.filter(
                workflow=workflow,
                status='pending'
            )
            
            for task in pending_tasks:
                # Check if all dependencies are completed
                dependencies = task.dependencies.all()
                all_completed = all(dep.status == 'completed' for dep in dependencies)
                
                if all_completed:
                    self._execute_task(task)
                    tasks_started += 1
            
            # Check if workflow is complete
            if not ScanTask.objects.filter(workflow=workflow).exclude(status='completed').exists():
                self._complete_workflow(workflow)
                
        return tasks_started
    
    def _execute_task(self, task: ScanTask) -> None:
        """
        Execute a specific workflow task
        
        Args:
            task: The task to execute
        """
        try:
            # Update task status
            task.status = 'in_progress'
            task.start_time = timezone.now()
            task.save()
            
            self.logger.info(f"Executing task {task.id} ({task.task_type}) for workflow {task.workflow.id}")
            
            # Execute appropriate task type
            if task.task_type == 'subdomain_enumeration':
                result = self._run_subdomain_enumeration(task)
            elif task.task_type == 'port_scanning':
                result = self._run_port_scanning(task)
            elif task.task_type == 'service_identification':
                result = self._run_service_identification(task)
            elif task.task_type == 'vulnerability_scanning':
                result = self._run_vulnerability_scanning(task)
            elif task.task_type == 'exploit_matching':  # New task type
                result = self._run_exploit_matching(task)
            elif task.task_type == 'network_mapping':
                result = self._run_network_mapping(task)
            elif task.task_type == 'report_generation':
                result = self._run_report_generation(task)
            else:
                raise ValueError(f"Unknown task type: {task.task_type}")
            
            # Update task status based on result
            if result.get('status') == 'success':
                task.status = 'completed'
                task.result = json.dumps(result)
            else:
                task.status = 'failed'
                task.result = json.dumps({'error': result.get('error', 'Unknown error')})
                
                # Create error notification
                if task.workflow.notification_email:
                    self.notification_manager.send_task_failure_notification(
                        task, result.get('error', 'Unknown error')
                    )
            
            task.end_time = timezone.now()
            task.save()
            
            # Check for critical failures that should stop the workflow
            if task.status == 'failed' and task.task_type in ['subdomain_enumeration', 'port_scanning']:
                self._fail_workflow(task.workflow, f"Critical task {task.task_type} failed")
                
        except Exception as e:
            self.logger.error(f"Error executing task {task.id}: {str(e)}")
            task.status = 'failed'
            task.result = json.dumps({'error': str(e)})
            task.end_time = timezone.now()
            task.save()
            
            # Create error notification
            if task.workflow.notification_email:
                self.notification_manager.send_task_failure_notification(task, str(e))
    
# File: automation/workflow_orchestrator.py
# Update the _run_subdomain_enumeration method to handle subdomain enumeration issues

    def _run_subdomain_enumeration(self, task: ScanTask) -> dict:
        """Run subdomain enumeration task with improved URL handling and reliability"""
        # Get the original target from the task
        original_target = task.workflow.target
        
        # Clean the target URL to get just the domain
        target_url = self.parse_target_url(original_target)
        
        # Remove 'www.' prefix if present for better subdomain enumeration
        if target_url.startswith('www.'):
            search_domain = target_url[4:]  # Remove www. prefix
            self.logger.info(f"Removing www prefix for enumeration, using: {search_domain}")
        else:
            search_domain = target_url
            
        try:
            self.logger.info(f"Starting subdomain enumeration for {search_domain} (original: {original_target})")
            results = self.subdomain_enumerator.enumerate_subdomains(search_domain)
            
            # If no results returned, add the main domain as a fallback
            if not results:
                self.logger.warning(f"No subdomains found for {search_domain}, adding main domain as fallback")
                try:
                    main_ip = socket.gethostbyname(search_domain)
                    results = [{
                        'subdomain': search_domain,
                        'ip_address': main_ip,
                        'is_http': True,
                        'http_status': None,
                        'status': 'active'
                    }]
                except Exception as e:
                    self.logger.error(f"Failed to resolve main domain as fallback: {str(e)}")
                    results = [{
                        'subdomain': search_domain,
                        'ip_address': None,
                        'is_http': None,
                        'http_status': None,
                        'status': 'unknown'
                    }]
            
            # Save results to database to ensure they're available for later steps
            saved_count = 0
            from reconnaissance.models import Subdomain  # Import the model explicitly
            
            for subdomain_data in results:
                # Skip entries without subdomain
                if not subdomain_data.get('subdomain'):
                    continue
                    
                try:
                    sub_obj, created = Subdomain.objects.update_or_create(
                        domain=search_domain,
                        subdomain=subdomain_data['subdomain'],
                        defaults={
                            'ip_address': subdomain_data.get('ip_address'),
                            'is_active': True
                        }
                    )
                    saved_count += 1
                except Exception as save_error:
                    self.logger.error(f"Error saving subdomain: {str(save_error)}")
            
            self.logger.info(f"Saved {saved_count} subdomains to database")
            
            return {
                'status': 'success',
                'target': original_target,  # Return original target for consistency
                'target_domain': search_domain,  # Use the domain without www for lookup
                'subdomains_found': len(results),
                'subdomains': results
            }
        except Exception as e:
            self.logger.error(f"Subdomain enumeration failed: {str(e)}")
            # Create a fallback result with just the main domain
            try:
                # Add the main domain as a subdomain in the database
                from reconnaissance.models import Subdomain  # Import the model explicitly
                
                sub_obj, created = Subdomain.objects.update_or_create(
                    domain=search_domain,
                    subdomain=search_domain,
                    defaults={
                        'ip_address': socket.gethostbyname(search_domain),
                        'is_active': True
                    }
                )
                
                return {
                    'status': 'success',  # Return success to continue workflow
                    'target': original_target,
                    'target_domain': search_domain,
                    'subdomains_found': 1, 
                    'subdomains': [{
                        'subdomain': search_domain,
                        'ip_address': sub_obj.ip_address,
                        'is_http': None,
                        'http_status': None,
                        'status': 'active'
                    }],
                    'warning': f"Error during subdomain scan: {str(e)}. Using main domain only."
                }
            except Exception as fallback_error:
                # Absolute last resort - return minimal data to allow workflow to continue
                return {
                    'status': 'success',  # Return success to continue workflow
                    'target': original_target,
                    'target_domain': search_domain,
                    'subdomains_found': 1,
                    'subdomains': [{
                        'subdomain': search_domain, 
                        'ip_address': None,
                        'is_http': None,
                        'http_status': None,
                        'status': 'unknown'
                    }],
                    'warning': f"Subdomain enumeration failed: {str(e)}. Using main domain as fallback."
                }
    
    def _run_port_scanning(self, task: ScanTask) -> dict:
        """Run port scanning task with improved URL handling and database storage"""
        # Get the original target from the task
        original_target = task.workflow.target
        scan_profile = task.workflow.scan_profile
        
        # Clean the target URL to get just the hostname
        target_url = self.parse_target_url(original_target)
        
        # Map scan profile to scan type
        scan_type = {
            'quick': 'quick',
            'standard': 'partial',
            'full': 'full'
        }.get(scan_profile, 'partial')
        
        self.logger.info(f"Starting port scan for {target_url} (original: {original_target}) with profile: {scan_profile}, type: {scan_type}")
        
        try:
            # Validate target before scanning
            import socket
            try:
                # Try to resolve hostname to ensure it's valid
                socket.gethostbyname(target_url)
            except socket.gaierror:
                self.logger.error(f"Unable to resolve target: {target_url}")
                return {
                    'status': 'error',
                    'error': f"Unable to resolve target: {target_url}. Please check the domain name."
                }
            
            # First run a quick check to see if common ports are open
            # This is more reliable than waiting for nmap
            common_ports = [80, 443, 8080, 8443, 22, 21]
            manual_check_ports = []
            
            for port in common_ports:
                try:
                    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                        sock.settimeout(1)
                        result = sock.connect_ex((target_url, port))
                        if result == 0:
                            manual_check_ports.append(port)
                            self.logger.info(f"Manual port check found open port {port} on {target_url}")
                except Exception as e:
                    self.logger.debug(f"Socket error checking port {port}: {str(e)}")
            
            # Run the actual scan
            results = self.port_scanner.scan(target_url, scan_type)
            
            # Check for success
            if results.get('status') == 'success':
                # Check if the scan found any ports
                ports_found = False
                for host in results.get('results', []):
                    if host.get('ports') and len(host.get('ports', [])) > 0:
                        ports_found = True
                        break
                
                # If we have manual ports but no scan ports, add them to the results
                if not ports_found and manual_check_ports and 'manual_detected' not in results:
                    self.logger.info(f"Adding manually detected ports to scan results: {manual_check_ports}")
                    
                    # Create port data for each manual port
                    manual_ports = []
                    for port in manual_check_ports:
                        service_name = 'https' if port in [443, 8443] else 'http' if port in [80, 8080] else 'unknown'
                        manual_ports.append({
                            'port': port,
                            'state': 'open',
                            'service': service_name,
                            'reason': 'manual check'
                        })
                    
                    # Update the results with our manual ports
                    if len(results.get('results', [])) > 0:
                        results['results'][0]['ports'] = manual_ports
                    else:
                        results['results'] = [{
                            'host': target_url,
                            'state': 'up',
                            'ports': manual_ports
                        }]
                    
                    results['manual_added'] = True
                
                # Save scan results to the database
                from reconnaissance.models import PortScan
                
                # Track which ports we've saved to avoid duplicates
                saved_ports = set()
                saved_count = 0
                
                # Process and save all port results
                for host in results.get('results', []):
                    for port_data in host.get('ports', []):
                        port = port_data.get('port')
                        state = port_data.get('state')
                        service = port_data.get('service', '')
                        
                        # Skip if we already saved this port or if port is invalid
                        if not port or (target_url, port) in saved_ports:
                            continue
                        
                        try:
                            # Create or update port scan record
                            port_scan, created = PortScan.objects.update_or_create(
                                host=target_url,
                                port=port,
                                defaults={
                                    'service': service,
                                    'state': state,
                                    'protocol': 'tcp',
                                    'scan_status': 'completed',
                                    'scan_type': scan_type,
                                    'banner': port_data.get('extrainfo', ''),
                                    'notes': f"Version: {port_data.get('version', 'unknown')}"
                                }
                            )
                            
                            saved_ports.add((target_url, port))
                            saved_count += 1
                            
                            # Check if this port should be flagged as a vulnerability
                            if state == 'open' and service in ['ftp', 'telnet', 'rsh', 'rlogin']:
                                from vulnerability.models import Vulnerability
                                
                                # Create a vulnerability entry for high-risk open ports
                                Vulnerability.objects.get_or_create(
                                    target=target_url,
                                    name=f"Open {service.upper()} Port ({port})",
                                    defaults={
                                        'description': f"Port {port} is open and running {service}, which is potentially insecure.",
                                        'severity': 'HIGH',
                                        'vuln_type': 'open_port',
                                        'evidence': f"Port {port} is open and accessible.",
                                        'source': 'port_scan',
                                        'confidence': 'high',
                                        'cvss_score': 7.5,
                                        'is_fixed': False
                                    }
                                )
                            
                        except Exception as save_error:
                            self.logger.error(f"Error saving port scan result: {str(save_error)}")
                
                self.logger.info(f"Saved {saved_count} port scan results to database")
                
                # Add database save info to results
                results['database_saved'] = {
                    'saved_count': saved_count,
                    'target': target_url
                }
                
                return results
            else:
                error_msg = results.get('error', 'Port scanning failed without specific error')
                
                # If we have manually detected ports, return those instead
                if manual_check_ports:
                    self.logger.info(f"Using manually detected ports after scan error: {manual_check_ports}")
                    
                    # Create port data for each manual port
                    manual_ports = []
                    for port in manual_check_ports:
                        service_name = 'https' if port in [443, 8443] else 'http' if port in [80, 8080] else 'unknown'
                        manual_ports.append({
                            'port': port,
                            'state': 'open',
                            'service': service_name,
                            'reason': 'manual check'
                        })
                    
                    # Save manual results to database
                    from reconnaissance.models import PortScan
                    saved_count = 0
                    
                    for port_data in manual_ports:
                        try:
                            port_scan, created = PortScan.objects.update_or_create(
                                host=target_url,
                                port=port_data['port'],
                                defaults={
                                    'service': port_data['service'],
                                    'state': 'open',
                                    'protocol': 'tcp',
                                    'scan_status': 'completed',
                                    'scan_type': 'manual',
                                    'notes': 'Detected by manual scan'
                                }
                            )
                            saved_count += 1
                        except Exception as save_error:
                            self.logger.error(f"Error saving manual port result: {str(save_error)}")
                    
                    self.logger.info(f"Saved {saved_count} manual port results to database")
                    
                    return {
                        'status': 'success',
                        'scan_info': {
                            'scan_type': 'manual',
                            'command_line': 'Manual port check',
                        },
                        'results': [{
                            'host': target_url,
                            'state': 'up',
                            'ports': manual_ports
                        }],
                        'manual_only': True,
                        'database_saved': {
                            'saved_count': saved_count,
                            'target': target_url
                        }
                    }
                
                self.logger.error(f"Port scanning error: {error_msg}")
                return {
                    'status': 'error',
                    'error': error_msg
                }
        except Exception as e:
            self.logger.error(f"Port scanning failed: {str(e)}")
            return {
                'status': 'error',
                'error': f"Port scanning failed: {str(e)}"
            }
    
    def _run_service_identification(self, task: ScanTask) -> dict:
        """Run service identification task with improved URL handling and database storage"""
        # Get the original target from the task
        original_target = task.workflow.target
        scan_profile = task.workflow.scan_profile
        
        # Clean the target URL to get just the hostname
        target_url = self.parse_target_url(original_target)
        
        # Map scan profile to service ID scan type
        scan_type = {
            'quick': 'quick',
            'standard': 'standard',
            'full': 'standard'  # Use 'standard' for full profile for better reliability
        }.get(scan_profile, 'standard')
        
        # Set time limit based on profile
        time_limit = {
            'quick': 180,    # 3 minutes
            'standard': 300, # 5 minutes
            'full': 600      # 10 minutes 
        }.get(scan_profile, 300)
        
        self.logger.info(f"Starting service identification for {target_url} with type: {scan_type}, timeout: {time_limit}s")
        
        try:
            # First check if we have any port scan results to work with
            port_scan_task = ScanTask.objects.filter(
                workflow=task.workflow,
                task_type='port_scanning',
                status='completed'
            ).first()
            
            if not port_scan_task or not port_scan_task.result:
                self.logger.warning(f"No completed port scan found for service identification")
                
                # Do a quick manual check for common ports
                try:
                    common_ports = [80, 443, 8080, 8443, 22, 21]
                    found_ports = []
                    
                    for port in common_ports:
                        try:
                            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                                sock.settimeout(1)
                                result = sock.connect_ex((target_url, port))
                                if result == 0:
                                    found_ports.append(port)
                        except:
                            pass
                    
                    if found_ports:
                        self.logger.info(f"Manual check found {len(found_ports)} open ports for service identification")
                        # Create simple service details
                        services = []
                        for port in found_ports:
                            service_name = 'https' if port in [443, 8443] else 'http' if port in [80, 8080] else 'unknown'
                            category = 'web' if port in [80, 443, 8080, 8443] else 'other'
                            risk_level = 'LOW' if service_name == 'https' else 'MEDIUM'
                            
                            services.append({
                                'port': port,
                                'protocol': 'tcp',
                                'state': 'open',
                                'service': {
                                    'name': service_name,
                                    'product': '',
                                    'version': '',
                                    'extrainfo': 'Detected by manual scan',
                                },
                                'category': category,
                                'risk_level': risk_level
                            })
                        
                        # Save services to database
                        self._save_services_to_database(target_url, services)
                        
                        return {
                            'status': 'success',
                            'target': original_target,
                            'services': services,
                            'manual_detection': True,
                            'database_saved': True
                        }
                    else:
                        # Return empty results to continue workflow
                        return {
                            'status': 'success',
                            'target': original_target,
                            'services': [],
                            'warning': 'No port scan results available'
                        }
                except Exception as e:
                    self.logger.error(f"Manual port check failed: {str(e)}")
                    # Still return success with empty results to continue workflow
                    return {
                        'status': 'success',
                        'target': original_target,
                        'services': [],
                        'warning': 'No port scan results available'
                    }
            
            # Try to parse port scan results
            try:
                import json
                scan_results = json.loads(port_scan_task.result)
                
                # Check if manually detected ports exist
                if scan_results.get('manual_scan') or scan_results.get('manual_detected') or scan_results.get('manual_only') or scan_results.get('manual_added'):
                    self.logger.info("Using manually detected ports for service identification")
                    
                    services = []
                    for host in scan_results.get('results', []):
                        for port_data in host.get('ports', []):
                            port = port_data.get('port')
                            state = port_data.get('state')
                            service_name = port_data.get('service', 'unknown')
                            
                            if state == 'open':
                                category = 'web' if service_name in ['http', 'https'] else 'other'
                                risk_level = 'LOW' if service_name == 'https' else 'MEDIUM'
                                
                                services.append({
                                    'port': port,
                                    'protocol': 'tcp',
                                    'state': 'open',
                                    'service': {
                                        'name': service_name,
                                        'product': '',
                                        'version': '',
                                        'extrainfo': 'Detected by manual scan',
                                    },
                                    'category': category,
                                    'risk_level': risk_level
                                })
                    
                    # Save services to database
                    self._save_services_to_database(target_url, services)
                    
                    return {
                        'status': 'success',
                        'target': original_target,
                        'services': services,
                        'manual_identification': True,
                        'database_saved': True
                    }
                
                # Regular processing
                if not scan_results.get('results') or not any(host.get('ports') for host in scan_results.get('results', [])):
                    self.logger.warning(f"No open ports found in port scan results")
                    return {
                        'status': 'success',
                        'target': original_target,
                        'services': [],
                        'warning': 'No open ports found for service identification'
                    }
            except Exception as parse_error:
                self.logger.error(f"Error parsing port scan results: {str(parse_error)}")
            
            # Import needed for timeout handling
            import threading
            import time
            import queue
            
            # Create a queue to get results
            result_queue = queue.Queue()
            
            # Define a worker function
            def service_scan_worker():
                try:
                    scan_result = self.service_identifier.identify_services(target_url, scan_type)
                    result_queue.put(scan_result)
                except Exception as e:
                    self.logger.error(f"Service scan worker error: {str(e)}")
                    result_queue.put({
                        'status': 'error',
                        'error': str(e)
                    })
            
            # Start the worker thread
            worker_thread = threading.Thread(target=service_scan_worker)
            worker_thread.daemon = True
            worker_thread.start()
            
            # Wait for result with timeout
            start_time = time.time()
            try:
                result = result_queue.get(timeout=time_limit)
                self.logger.info(f"Service identification completed in {time.time() - start_time:.1f} seconds")
            except queue.Empty:
                self.logger.error(f"Service identification timed out after {time_limit} seconds")
                # Return success with limited info to continue workflow
                return {
                    'status': 'success',
                    'target': original_target,
                    'services': [],
                    'warning': f'Service identification timed out after {time_limit}s'
                }
            
            if result.get('status') == 'success':
                # If target in result doesn't match our original, update it
                if 'target' in result:
                    result['target'] = original_target
                
                # Save services to database
                services = result.get('services', [])
                if services:
                    self._save_services_to_database(target_url, services)
                    result['database_saved'] = True
                
                return result
            else:
                self.logger.error(f"Service identification failed: {result.get('error')}")
                # Even if the scan fails, return success with empty results to continue workflow
                return {
                    'status': 'success',
                    'target': original_target,
                    'services': [],
                    'warning': f"Service identification error: {result.get('error', 'Unknown error')}"
                }
        except Exception as e:
            self.logger.error(f"Service identification failed: {str(e)}")
            # Return success with empty results to continue workflow
            return {
                'status': 'success',
                'target': original_target,
                'services': [],
                'warning': f"Service identification error: {str(e)}"
            }

    def _save_services_to_database(self, target: str, services: List[Dict]) -> int:
        """Save service identification results to database
        
        Args:
            target: The target domain/IP
            services: List of service dictionaries
            
        Returns:
            int: Number of services saved
        """
        if not services:
            return 0
            
        from reconnaissance.models import Service
        from vulnerability.models import Vulnerability
        
        saved_count = 0
        
        # High risk services that should be flagged as vulnerabilities
        high_risk_services = {
            'ftp': 'File Transfer Protocol (FTP)',
            'telnet': 'Telnet Remote Access',
            'rsh': 'Remote Shell (RSH)',
            'rlogin': 'Remote Login (Rlogin)',
            'smb': 'Windows File Sharing (SMB)'
        }
        
        # Medium risk services
        medium_risk_services = {
            'smtp': 'Mail Server (SMTP)',
            'pop3': 'Mail Server (POP3)',
            'vnc': 'VNC Remote Desktop',
            'mysql': 'MySQL Database',
            'mssql': 'Microsoft SQL Server'
        }
        
        for service_data in services:
            try:
                port = service_data.get('port')
                if not port:
                    continue
                    
                # Extract service details
                protocol = service_data.get('protocol', 'tcp')
                state = service_data.get('state', 'open')
                service_info = service_data.get('service', {})
                
                if not service_info:
                    continue
                    
                service_name = service_info.get('name', 'unknown')
                product = service_info.get('product', '')
                version = service_info.get('version', '')
                extra_info = service_info.get('extrainfo', '')
                category = service_data.get('category', 'other')
                risk_level = service_data.get('risk_level', 'MEDIUM')
                
                # Create or update service record
                service_obj, created = Service.objects.update_or_create(
                    host=target,
                    port=port,
                    protocol=protocol,
                    defaults={
                        'name': service_name,
                        'product': product,
                        'version': version,
                        'extra_info': extra_info,
                        'category': category,
                        'risk_level': risk_level,
                        'is_active': True
                    }
                )
                
                saved_count += 1
                
                # Check if this service should be flagged as a vulnerability
                if state == 'open':
                    # For high risk services
                    if service_name.lower() in high_risk_services:
                        service_title = high_risk_services[service_name.lower()]
                        
                        # Create vulnerability entry
                        Vulnerability.objects.get_or_create(
                            target=target,
                            name=f"{service_title} on port {port}",
                            defaults={
                                'description': f"Port {port} is running {service_name}, which is potentially insecure. {product} {version}".strip(),
                                'severity': 'HIGH',
                                'vuln_type': 'insecure_service',
                                'evidence': f"Service detected on port {port}. {extra_info}".strip(),
                                'source': 'service_identification',
                                'confidence': 'high',
                                'cvss_score': 7.5,
                                'is_fixed': False
                            }
                        )
                    
                    # For medium risk services
                    elif service_name.lower() in medium_risk_services:
                        service_title = medium_risk_services[service_name.lower()]
                        
                        # Create vulnerability entry
                        Vulnerability.objects.get_or_create(
                            target=target,
                            name=f"{service_title} on port {port}",
                            defaults={
                                'description': f"Port {port} is running {service_name}, which might pose security risks if not properly configured. {product} {version}".strip(),
                                'severity': 'MEDIUM',
                                'vuln_type': 'potentially_risky_service',
                                'evidence': f"Service detected on port {port}. {extra_info}".strip(),
                                'source': 'service_identification',
                                'confidence': 'medium',
                                'cvss_score': 5.0,
                                'is_fixed': False
                            }
                        )
                        
                    # Flag uncommon open ports    
                    elif port not in [80, 443, 8080, 8443, 22] and port < 1024:
                        # Create vulnerability entry for uncommon open ports
                        Vulnerability.objects.get_or_create(
                            target=target,
                            name=f"Uncommon service on port {port} ({service_name})",
                            defaults={
                                'description': f"Port {port} is open and running {service_name}, which is uncommon and might indicate unnecessary services.",
                                'severity': 'LOW',
                                'vuln_type': 'uncommon_port',
                                'evidence': f"Service {service_name} detected on port {port}.",
                                'source': 'service_identification',
                                'confidence': 'medium',
                                'cvss_score': 3.0,
                                'is_fixed': False
                            }
                        )
                    
            except Exception as e:
                self.logger.error(f"Error saving service to database: {str(e)}")
        
        return saved_count
    
    def _run_vulnerability_scanning(self, task: ScanTask) -> dict:
        """Run vulnerability scanning task with improved URL handling"""
        # Get the original target from the task
        original_target = task.workflow.target
        scan_profile = task.workflow.scan_profile
        
        # Clean the target URL to get just the hostname
        target_url = self.parse_target_url(original_target)
        
        # Determine scanners to use based on scan profile
        include_zap = scan_profile in ['standard', 'full']
        include_nuclei = True  # Always use Nuclei
        nuclei_scan_type = 'advanced' if scan_profile == 'full' else 'basic'
        
        try:
            self.logger.info(f"Starting vulnerability scan for {target_url} (original: {original_target})")
            
            results = self.vulnerability_scanner.scan_target(
                target=target_url,
                scan_type=scan_profile,
                include_zap=include_zap,
                include_nuclei=include_nuclei,
                nuclei_scan_type=nuclei_scan_type,
                use_advanced_correlation=True
            )
            
            if results.get('status') == 'success':
                # Check for critical vulnerabilities
                high_vulns = 0
                critical_vulns = 0
                
                for vuln in results.get('vulnerabilities', []):
                    if vuln.get('severity') == 'CRITICAL':
                        critical_vulns += 1
                    elif vuln.get('severity') == 'HIGH':
                        high_vulns += 1
                
                # Add notification for critical vulnerabilities
                if (critical_vulns > 0 or high_vulns > 2) and task.workflow.notification_email:
                    self.notification_manager.send_critical_vulnerability_notification(
                        task.workflow, critical_vulns, high_vulns
                    )
                
                return results
            else:
                return {
                    'status': 'error',
                    'error': results.get('error', 'Vulnerability scanning failed without specific error')
                }
        except Exception as e:
            self.logger.error(f"Vulnerability scanning failed: {str(e)}")
            return {
                'status': 'error', 
                'error': f"Vulnerability scanning failed: {str(e)}"
            }
    
    def _run_network_mapping(self, task: ScanTask) -> dict:
        """Run network mapping task with improved visualization data"""
        # Get the original target from the task
        original_target = task.workflow.target
        
        # Clean the target URL to get just the hostname
        target_url = self.parse_target_url(original_target)
        
        try:
            self.logger.info(f"Starting network mapping for {target_url} (original: {original_target})")
            
            # Get services from previous tasks to include in the network map
            services_data = []
            try:
                service_task = ScanTask.objects.filter(
                    workflow=task.workflow,
                    task_type='service_identification',
                    status='completed'
                ).first()
                
                if service_task and service_task.result:
                    service_result = json.loads(service_task.result)
                    services_data = service_result.get('services', [])
            except Exception as e:
                self.logger.warning(f"Error fetching service data for network mapping: {str(e)}")
            
            # Get subdomains from previous tasks
            subdomains_data = []
            try:
                subdomain_task = ScanTask.objects.filter(
                    workflow=task.workflow,
                    task_type='subdomain_enumeration',
                    status='completed'
                ).first()
                
                if subdomain_task and subdomain_task.result:
                    subdomain_result = json.loads(subdomain_task.result)
                    subdomains_data = subdomain_result.get('subdomains', [])
            except Exception as e:
                self.logger.warning(f"Error fetching subdomain data for network mapping: {str(e)}")
            
            # Create network map
            results = self.topology_mapper.create_network_map(
                target_url, 
                services=services_data,
                subdomains=subdomains_data
            )
            
            # Get the number of nodes and connections for proper report display
            nodes_count = 0
            connections_count = 0
            
            if results.get('status') == 'success':
                # Try to get network node counts from the database
                from network_visualization.models import NetworkNode, NetworkConnection
                
                try:
                    nodes_count = NetworkNode.objects.filter(domain=target_url, is_active=True).count()
                    connections_count = NetworkConnection.objects.filter(
                        source__domain=target_url,
                        is_active=True
                    ).count()
                    
                    self.logger.info(f"Network map created with {nodes_count} nodes and {connections_count} connections")
                except Exception as db_error:
                    self.logger.error(f"Error counting network nodes from database: {str(db_error)}")
                
                # Add the node and connection counts to the result
                results['nodes'] = nodes_count
                results['connections'] = connections_count
                
                return results
            else:
                return {
                    'status': 'error',
                    'error': results.get('error', 'Network mapping failed without specific error'),
                    'nodes': 0,
                    'connections': 0
                }
        except Exception as e:
            self.logger.error(f"Network mapping failed: {str(e)}")
            return {
                'status': 'error',
                'error': f"Network mapping failed: {str(e)}",
                'nodes': 0,
                'connections': 0
            }
    
    def _run_exploit_matching(self, task: ScanTask) -> dict:
        """Run exploit matching for vulnerabilities identified in the target"""
        # Get the original target from the task
        original_target = task.workflow.target
        
        # Clean the target URL to get just the hostname
        target_url = self.parse_target_url(original_target)
        
        try:
            self.logger.info(f"Starting exploit matching for vulnerabilities in {target_url}")
            
            # Create a new matcher instance
            from exploit_manager.matcher import ExploitMatcher
            matcher = ExploitMatcher()
            
            # Get vulnerabilities for this target
            from vulnerability.models import Vulnerability
            vulnerabilities = Vulnerability.objects.filter(target=target_url, is_fixed=False)
            
            total_vulns = vulnerabilities.count()
            matched_vulns = 0
            total_matches = 0
            match_details = []
            
            # Match each vulnerability with potential exploits
            for vuln in vulnerabilities:
                matches = matcher.match_vulnerability(vuln)
                if matches:
                    matched_vulns += 1
                    total_matches += len(matches)
                    
                    # Add top match to details
                    if matches:
                        top_match = matches[0]  # Assuming matches are sorted by confidence
                        match_details.append({
                            'vulnerability_id': vuln.id,
                            'vulnerability_name': vuln.name,
                            'exploit_title': top_match.exploit.title,
                            'exploit_id': top_match.exploit.exploit_id,
                            'confidence': top_match.confidence_score
                        })
            
            # Create result data
            result = {
                'status': 'success',
                'target': original_target,
                'total_vulnerabilities': total_vulns,
                'vulnerabilities_with_matches': matched_vulns,
                'total_matches': total_matches,
                'match_details': match_details[:5]  # Include top 5 matches in the result
            }
            
            self.logger.info(f"Exploit matching completed: found {total_matches} exploits for {matched_vulns} vulnerabilities")
            return result
            
        except Exception as e:
            self.logger.error(f"Exploit matching failed: {str(e)}")
            return {
                'status': 'error',
                'error': f"Exploit matching failed: {str(e)}"
            }

    
# File: automation/workflow_orchestrator.py
    def _run_report_generation(self, task: ScanTask) -> dict:
        """Run report generation task with improved URL handling and exploit match data"""
        # Get the original target from the task
        original_target = task.workflow.target
        
        # Clean the target URL for DB lookups if needed
        target_url = self.parse_target_url(original_target)
        
        scan_profile = task.workflow.scan_profile
        
        # Map scan profile to report type
        report_type = {
            'quick': 'basic',
            'standard': 'detailed',
            'full': 'executive'
        }.get(scan_profile, 'detailed')
        
        try:
            # Get vulnerability scanning task result
            vuln_scan_task = ScanTask.objects.filter(
                workflow=task.workflow,
                task_type='vulnerability_scanning',
                status='completed'
            ).first()
            
            # Get exploit matching task result
            exploit_match_task = ScanTask.objects.filter(
                workflow=task.workflow,
                task_type='exploit_matching',
                status='completed'
            ).first()
            
            # Parse scan results if available
            scan_results = {'workflow_id': task.workflow.id}
            
            # Initialize variables to track findings
            vuln_count = 0
            processed_vulns = []
            
            if vuln_scan_task and vuln_scan_task.result:
                try:
                    vuln_results = json.loads(vuln_scan_task.result)
                    
                    # Extract the vulnerabilities directly
                    if 'vulnerabilities' in vuln_results:
                        vuln_list = vuln_results['vulnerabilities']
                        vuln_count = len(vuln_list)
                        self.logger.info(f"Found {vuln_count} vulnerabilities in scan results")
                        
                        # Explicitly process and save each vulnerability to ensure they're in the database
                        from vulnerability.models import Vulnerability
                        
                        for vuln_data in vuln_list:
                            try:
                                # Normalize the target
                                vuln_target = target_url
                                
                                # Extract and normalize the severity
                                severity = vuln_data.get('severity', 'MEDIUM')
                                if isinstance(severity, str):
                                    severity = severity.upper()
                                
                                # Create or update the vulnerability in the database
                                vuln, created = Vulnerability.objects.update_or_create(
                                    target=vuln_target,
                                    name=vuln_data.get('name', 'Unknown vulnerability'),
                                    defaults={
                                        'description': vuln_data.get('description', ''),
                                        'severity': severity,
                                        'vuln_type': vuln_data.get('type', vuln_data.get('vuln_type', 'unknown')),
                                        'evidence': vuln_data.get('evidence', ''),
                                        'source': vuln_data.get('source', 'scan'),
                                        'confidence': vuln_data.get('confidence', 'medium'),
                                        'cvss_score': vuln_data.get('cvss_score', 0.0),
                                        'is_fixed': False
                                    }
                                )
                                
                                status = 'created' if created else 'updated'
                                self.logger.info(f"Vulnerability {status} in database: {vuln.name} ({vuln.severity})")
                                processed_vulns.append(vuln)
                                
                            except Exception as ve:
                                self.logger.error(f"Error saving vulnerability: {str(ve)}")
                    
                    # Merge other relevant data into scan_results
                    for key, value in vuln_results.items():
                        if key not in ['status', 'vulnerabilities']:
                            scan_results[key] = value
                    
                    self.logger.info(f"Processed vulnerability scan results for report generation")
                except Exception as parse_error:
                    self.logger.error(f"Failed to parse vulnerability scan results: {str(parse_error)}")
            
            # Parse exploit matching results if available
            if exploit_match_task and exploit_match_task.result:
                try:
                    exploit_results = json.loads(exploit_match_task.result)
                    if exploit_results.get('status') == 'success':
                        # Add exploit matching data to scan results
                        scan_results['exploit_matching'] = {
                            'total_vulnerabilities': exploit_results.get('total_vulnerabilities', 0),
                            'vulnerabilities_with_matches': exploit_results.get('vulnerabilities_with_matches', 0),
                            'total_matches': exploit_results.get('total_matches', 0),
                            'match_details': exploit_results.get('match_details', [])
                        }
                        self.logger.info(f"Added exploit matching data to report: {exploit_results.get('total_matches', 0)} matches")
                    else:
                        self.logger.warning(f"Exploit matching task didn't complete successfully: {exploit_results.get('error', 'Unknown error')}")
                except Exception as parse_error:
                    self.logger.error(f"Failed to parse exploit matching results: {str(parse_error)}")
            else:
                self.logger.warning("No completed exploit matching task found for report generation")
                
                # If no exploit match task, try to get data from database directly
                try:
                    from exploit_manager.models import ExploitMatch
                    from vulnerability.models import Vulnerability
                    
                    vulnerabilities = Vulnerability.objects.filter(target=target_url, is_fixed=False)
                    
                    # Count total matches
                    total_matches = ExploitMatch.objects.filter(
                        vulnerability__target=target_url,
                        vulnerability__is_fixed=False
                    ).count()
                    
                    # Get vulnerabilities with matches
                    vulns_with_matches = vulnerabilities.filter(
                        exploit_matches__isnull=False
                    ).distinct().count()
                    
                    # Get top matches by confidence
                    top_matches = ExploitMatch.objects.filter(
                        vulnerability__target=target_url,
                        vulnerability__is_fixed=False
                    ).order_by('-confidence_score')[:5]
                    
                    match_details = []
                    for match in top_matches:
                        match_details.append({
                            'vulnerability_id': match.vulnerability.id,
                            'vulnerability_name': match.vulnerability.name,
                            'exploit_title': match.exploit.title,
                            'exploit_id': match.exploit.exploit_id,
                            'id': match.exploit.id,  # Add database ID for URL construction
                            'confidence': match.confidence_score,
                            'cve_id': match.exploit.cve_id or "None"
                        })
                    
                    # Add to scan results
                    scan_results['exploit_matching'] = {
                        'total_vulnerabilities': vulnerabilities.count(),
                        'vulnerabilities_with_matches': vulns_with_matches,
                        'total_matches': total_matches,
                        'match_details': match_details
                    }
                    
                    self.logger.info(f"Added exploit match data from database: {total_matches} matches")
                except Exception as db_error:
                    self.logger.error(f"Failed to get exploit matches from database: {str(db_error)}")
            
            # Double-check the vulnerability counts in the database
            from vulnerability.models import Vulnerability
            db_vulns = Vulnerability.objects.filter(target=target_url, is_fixed=False)
            db_vuln_count = db_vulns.count()
            
            self.logger.info(f"Vulnerabilities in database for {target_url}: {db_vuln_count}")
            
            # If we don't have vulnerability data in scan_results yet, add it from the database
            if 'vulnerabilities' not in scan_results or not scan_results['vulnerabilities']:
                scan_results['vulnerabilities'] = []
                for vuln in db_vulns:
                    scan_results['vulnerabilities'].append({
                        'id': vuln.id,
                        'name': vuln.name,
                        'description': vuln.description,
                        'severity': vuln.severity,
                        'type': vuln.vuln_type,
                        'evidence': vuln.evidence,
                        'source': vuln.source,
                        'confidence': vuln.confidence,
                        'cvss_score': vuln.cvss_score,
                    })
            
            # Generate different report formats
            self.logger.info(f"Generating {report_type} report for {original_target} with {db_vuln_count} vulnerabilities")
            
            # Generate HTML report
            report_html = self.report_generator.generate_report(report_type, original_target, 'html', scan_results)
            
            # Generate PDF report as well
            try:
                report_pdf = self.report_generator.generate_report(report_type, original_target, 'pdf', scan_results)
                pdf_id = report_pdf.id
            except Exception as pdf_error:
                self.logger.error(f"Error generating PDF report: {str(pdf_error)}")
                pdf_id = None
            
            # Only send a single notification email
            if task.workflow.notification_email:
                self.notification_manager.send_workflow_completion_notification(
                    task.workflow, 
                    report_id=report_html.id
                )
            
            return {
                'status': 'success',
                'target': original_target,
                'workflow_id': task.workflow.id,
                'report_types': [report_type],
                'report_formats': ['html', 'pdf'] if pdf_id else ['html'],
                'report_ids': {
                    'html': report_html.id,
                    'pdf': pdf_id
                },
                'vulnerability_count': db_vuln_count  # Include the count for verification
            }
        except Exception as e:
            self.logger.error(f"Report generation failed: {str(e)}")
            return {
                'status': 'error',
                'error': f"Report generation failed: {str(e)}"
            }
    
    def _fail_workflow(self, workflow: ScanWorkflow, reason: str) -> None:
        """Mark workflow as failed and send notifications"""
        workflow.status = 'failed'
        workflow.end_time = timezone.now()
        workflow.save()
        
        logger.error(f"Workflow {workflow.id} for {workflow.target} failed: {reason}")
        
        # Update pending tasks to skipped
        ScanTask.objects.filter(workflow=workflow, status='pending').update(
            status='skipped',
            result=json.dumps({'skipped_reason': reason})
        )
        
        # Send failure notification
        if workflow.notification_email:
            self.notification_manager.send_workflow_failure_notification(workflow, reason)
    
    def cancel_workflow(self, workflow_id: int) -> bool:
        """
        Cancel a running or scheduled workflow
        
        Args:
            workflow_id: ID of the workflow to cancel
            
        Returns:
            bool: True if successfully canceled
        """
        try:
            workflow = ScanWorkflow.objects.get(id=workflow_id)
            
            if workflow.status in ['completed', 'failed', 'canceled']:
                logger.warning(f"Workflow {workflow_id} already in terminal state: {workflow.status}")
                return False
            
            # Update workflow status
            original_status = workflow.status
            workflow.status = 'canceled'
            workflow.end_time = timezone.now()
            workflow.save()
            
            # Update in-progress tasks to canceled
            ScanTask.objects.filter(workflow=workflow, status='in_progress').update(
                status='canceled',
                end_time=timezone.now(),
                result=json.dumps({'canceled_reason': 'Workflow canceled by user'})
            )
            
            # Update pending tasks to skipped
            ScanTask.objects.filter(workflow=workflow, status='pending').update(
                status='skipped',
                result=json.dumps({'skipped_reason': 'Workflow canceled by user'})
            )
            
            logger.info(f"Workflow {workflow_id} canceled (was {original_status})")
            
            # Send cancellation notification
            if workflow.notification_email:
                self.notification_manager.send_workflow_cancellation_notification(workflow)
                
            return True
            
        except ScanWorkflow.DoesNotExist:
            logger.error(f"Workflow {workflow_id} not found")
            return False
        except Exception as e:
            logger.error(f"Error canceling workflow {workflow_id}: {str(e)}")
            return False
    
    def get_workflow_status(self, workflow_id: int) -> dict:
        """
        Get detailed status of a workflow
        
        Args:
            workflow_id: ID of the workflow
            
        Returns:
            dict: Workflow status details
        """
        try:
            workflow = ScanWorkflow.objects.get(id=workflow_id)
            tasks = ScanTask.objects.filter(workflow=workflow).order_by('order')
            
            # Calculate progress percentage
            total_tasks = tasks.count()
            completed_tasks = tasks.filter(status__in=['completed', 'skipped', 'canceled']).count()
            progress = int(completed_tasks / total_tasks * 100) if total_tasks > 0 else 0
            
            # Format task results
            task_results = []
            for task in tasks:
                result_data = {}
                if task.result:
                    try:
                        result_data = json.loads(task.result)
                    except:
                        result_data = {'error': 'Invalid JSON result'}
                
                task_results.append({
                    'id': task.id,
                    'name': task.name,
                    'type': task.task_type,
                    'status': task.status,
                    'start_time': task.start_time.isoformat() if task.start_time else None,
                    'end_time': task.end_time.isoformat() if task.end_time else None,
                    'duration': str(task.end_time - task.start_time) if task.start_time and task.end_time else None,
                    'result_summary': self._summarize_task_result(task.task_type, result_data)
                })
            
            return {
                'id': workflow.id,
                'name': workflow.name,
                'target': workflow.target,
                'status': workflow.status,
                'scan_profile': workflow.scan_profile,
                'scheduled_time': workflow.scheduled_time.isoformat() if workflow.scheduled_time else None,
                'start_time': workflow.start_time.isoformat() if workflow.start_time else None,
                'end_time': workflow.end_time.isoformat() if workflow.end_time else None,
                'duration': str(workflow.end_time - workflow.start_time) if workflow.start_time and workflow.end_time else None,
                'progress': progress,
                'tasks': task_results,
                'notification_email': workflow.notification_email
            }
            
        except ScanWorkflow.DoesNotExist:
            logger.error(f"Workflow {workflow_id} not found")
            return {'error': 'Workflow not found'}
        except Exception as e:
            logger.error(f"Error getting workflow status: {str(e)}")
            return {'error': str(e)}
    
    def _summarize_task_result(self, task_type: str, result: dict) -> dict:
        """Generate a summary of task results for display"""
        summary = {}
        
        if task_type == 'subdomain_enumeration':
            summary['subdomains_found'] = result.get('subdomains_found', 0)
        elif task_type == 'port_scanning':
            hosts = result.get('results', [])
            open_ports = 0
            for host in hosts:
                open_ports += len([p for p in host.get('ports', []) if p.get('state') == 'open'])
            summary['hosts_scanned'] = len(hosts)
            summary['open_ports'] = open_ports
        elif task_type == 'service_identification':
            summary['services_found'] = len(result.get('services', []))
        elif task_type == 'vulnerability_scanning':
            vulns = result.get('vulnerabilities', [])
            severity_counts = {
                'critical': len([v for v in vulns if v.get('severity') == 'CRITICAL']),
                'high': len([v for v in vulns if v.get('severity') == 'HIGH']),
                'medium': len([v for v in vulns if v.get('severity') == 'MEDIUM']),
                'low': len([v for v in vulns if v.get('severity') == 'LOW'])
            }
            summary['vulnerabilities_found'] = len(vulns)
            summary['severity_counts'] = severity_counts
        elif task_type == 'network_mapping':
            summary['nodes'] = result.get('nodes', 0)
            summary['connections'] = result.get('connections', 0)
        elif task_type == 'report_generation':
            summary['report_types'] = result.get('report_types', [])
            summary['report_formats'] = result.get('report_formats', [])
            summary['report_ids'] = result.get('report_ids', {})
            
        return summary# automation/processor.py

import threading
import time
import logging
from django.conf import settings
from django.db import connection

from .workflow_orchestrator import WorkflowOrchestrator
from .scheduler import ScanScheduler

logger = logging.getLogger(__name__)

class AutomationProcessor:
    """
    Process automation workflows and scheduled tasks in the background.
    Implemented as a singleton to ensure only one instance is running.
    """
    _instance = None
    _lock = threading.Lock()
    
    @classmethod
    def get_instance(cls):
        """Get or create the singleton instance"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = cls()
        return cls._instance
    
    def __init__(self):
        self.orchestrator = WorkflowOrchestrator()
        self.scheduler = ScanScheduler()
        self.stop_flag = threading.Event()
        self.processing_thread = None
        self.interval = getattr(settings, 'AUTOMATION_PROCESSING_INTERVAL', 60)  # seconds
    
    def start(self):
        """Start the background processing thread if not already running"""
        if self.processing_thread and self.processing_thread.is_alive():
            logger.warning("Automation processor already running")
            return False
            
        self.stop_flag.clear()
        self.processing_thread = threading.Thread(
            target=self._processing_loop,
            daemon=True
        )
        self.processing_thread.start()
        logger.info("Automation processor started")
        return True
    
    def stop(self):
        """Stop the background processing thread"""
        if not self.processing_thread or not self.processing_thread.is_alive():
            logger.warning("Automation processor not running")
            return False
            
        self.stop_flag.set()
        self.processing_thread.join(timeout=10)
        logger.info("Automation processor stopped")
        return True
    
    def is_running(self):
        """Check if the processor is running"""
        return self.processing_thread is not None and self.processing_thread.is_alive()
    
    def _processing_loop(self):
        """Main processing loop for automation tasks"""
        logger.info("Automation processing loop started")
        
        while not self.stop_flag.is_set():
            try:
                # Process scheduled tasks
                scheduled_count = self.scheduler.process_scheduled_tasks()
                if scheduled_count > 0:
                    logger.info(f"Processed {scheduled_count} scheduled tasks")
                
                # Start pending workflows
                started_count = self.orchestrator.check_pending_workflows()
                if started_count > 0:
                    logger.info(f"Started {started_count} pending workflows")
                
                # Process workflow queue
                processed_count = self.orchestrator.process_workflow_queue()
                if processed_count > 0:
                    logger.info(f"Processed {processed_count} workflow tasks")
                
            except Exception as e:
                logger.error(f"Error in automation processing: {str(e)}")
            finally:
                # Close database connections to prevent connection leaks
                connection.close()
            
            # Sleep until next interval
            self.stop_flag.wait(self.interval)
        
        logger.info("Automation processing loop stopped")
    
    @classmethod
    def run_once(cls):
        """
        Run a single cycle of the processor, useful for cron jobs or manual triggers
        """
        processor = cls()
        
        try:
            # Process scheduled tasks
            scheduled_count = processor.scheduler.process_scheduled_tasks()
            
            # Start pending workflows
            started_count = processor.orchestrator.check_pending_workflows()
            
            # Process workflow queue
            processed_count = processor.orchestrator.process_workflow_queue()
            
            return {
                'scheduled_tasks_processed': scheduled_count,
                'workflows_started': started_count,
                'tasks_processed': processed_count
            }
            
        except Exception as e:
            logger.error(f"Error in one-time automation processing: {str(e)}")
            return {
                'error': str(e)
            }
        finally:
            # Close database connections
            connection.close()# automation/scheduler.py

import logging
import json
from datetime import datetime, timedelta
from django.utils import timezone
from croniter import croniter

from .models import ScheduledTask, ScanWorkflow
from .workflow_orchestrator import WorkflowOrchestrator

logger = logging.getLogger(__name__)

class ScanScheduler:
    """
    Handles scheduling and execution of recurring security scans
    """
    
    def __init__(self):
        self.orchestrator = WorkflowOrchestrator()
    
    def process_scheduled_tasks(self) -> int:
        """
        Check for scheduled tasks that need to be triggered and create workflows
        
        Returns:
            int: Number of workflows created
        """
        now = timezone.now()
        active_schedules = ScheduledTask.objects.filter(is_active=True)
        
        workflows_created = 0
        
        for schedule in active_schedules:
            try:
                # Skip if end date is set and has passed
                if schedule.end_date and schedule.end_date < now.date():
                    continue
                
                # Determine next execution time
                next_run = self._calculate_next_run(schedule)
                
                # Check if it's time to run (or overdue)
                if next_run and next_run <= now:
                    # Create a new workflow
                    name = f"{schedule.name} - {now.strftime('%Y-%m-%d %H:%M')}"
                    
                    workflow = self.orchestrator.create_workflow(
                        target=schedule.target,
                        name=name,
                        scan_profile=schedule.scan_profile,
                        notify_email=schedule.notification_email
                    )
                    
                    # Start the workflow immediately
                    self.orchestrator.start_workflow(workflow.id)
                    
                    # Update the schedule's last execution
                    schedule.last_execution = now
                    schedule.last_status = 'started'
                    schedule.last_workflow = workflow
                    schedule.save()
                    
                    logger.info(f"Created scheduled workflow {workflow.id} for schedule {schedule.id}")
                    workflows_created += 1
            
            except Exception as e:
                logger.error(f"Error processing scheduled task {schedule.id}: {str(e)}")
        
        return workflows_created
    
    def _calculate_next_run(self, schedule: ScheduledTask) -> datetime:
        """
        Calculate the next run time for a scheduled task
        
        Args:
            schedule: The scheduled task
            
        Returns:
            datetime: Next execution time or None if cannot be determined
        """
        now = timezone.now()
        
        # If never run, use start_date as base
        if schedule.last_execution is None:
            base_time = datetime.combine(schedule.start_date, datetime.min.time())
            base_time = timezone.make_aware(base_time)
            
            # If start date is in future, return that
            if base_time > now:
                return base_time
        else:
            base_time = schedule.last_execution
        
        # Calculate next run based on frequency
        if schedule.frequency == 'daily':
            # Add 24 hours to last execution
            return base_time + timedelta(days=1)
            
        elif schedule.frequency == 'weekly':
            # Add 7 days to last execution
            return base_time + timedelta(days=7)
            
        elif schedule.frequency == 'monthly':
            # Add roughly a month (30 days) to last execution
            return base_time + timedelta(days=30)
            
        elif schedule.frequency == 'custom' and schedule.cron_expression:
            try:
                # Use croniter to calculate next run based on cron expression
                cron = croniter(schedule.cron_expression, base_time)
                return cron.get_next(datetime)
            except Exception as e:
                logger.error(f"Error parsing cron expression for schedule {schedule.id}: {str(e)}")
                return None
        
        return None
    
    def create_scheduled_task(self, name: str, target: str, frequency: str, 
                          start_date: datetime.date, end_date=None, 
                          scan_profile: str='standard', cron_expression=None, 
                          notification_email=None, created_by=None) -> ScheduledTask:
        """
        Create a new scheduled task
        
        Args:
            name: Name of the scheduled task
            target: Target domain/IP
            frequency: Frequency (daily, weekly, monthly, custom)
            start_date: Start date
            end_date: End date (optional)
            scan_profile: Scan profile (quick, standard, full)
            cron_expression: Cron expression for custom frequency
            notification_email: Email to notify
            created_by: User who created the schedule
            
        Returns:
            ScheduledTask: The created scheduled task
        """
        if frequency == 'custom' and not cron_expression:
            raise ValueError("Cron expression is required for custom frequency")
        
        scheduled_task = ScheduledTask.objects.create(
            name=name,
            target=target,
            frequency=frequency,
            start_date=start_date,
            end_date=end_date,
            scan_profile=scan_profile,
            cron_expression=cron_expression,
            notification_email=notification_email,
            created_by=created_by
        )
        
        logger.info(f"Created scheduled task {scheduled_task.id} for {target}")
        return scheduled_task
    
    def update_scheduled_task(self, task_id: int, **kwargs) -> ScheduledTask:
        """
        Update a scheduled task
        
        Args:
            task_id: ID of the task to update
            **kwargs: Fields to update
            
        Returns:
            ScheduledTask: The updated task
        """
        try:
            task = ScheduledTask.objects.get(id=task_id)
            
            # Update fields
            for field, value in kwargs.items():
                if hasattr(task, field):
                    setattr(task, field, value)
            
            task.save()
            logger.info(f"Updated scheduled task {task_id}")
            return task
            
        except ScheduledTask.DoesNotExist:
            logger.error(f"Scheduled task {task_id} not found")
            raise ValueError(f"Scheduled task {task_id} not found")
    
    def delete_scheduled_task(self, task_id: int) -> bool:
        """
        Delete a scheduled task
        
        Args:
            task_id: ID of the task to delete
            
        Returns:
            bool: True if successfully deleted
        """
        try:
            task = ScheduledTask.objects.get(id=task_id)
            task.delete()
            logger.info(f"Deleted scheduled task {task_id}")
            return True
            
        except ScheduledTask.DoesNotExist:
            logger.error(f"Scheduled task {task_id} not found")
            return False
    
    def disable_scheduled_task(self, task_id: int) -> bool:
        """
        Disable a scheduled task
        
        Args:
            task_id: ID of the task to disable
            
        Returns:
            bool: True if successfully disabled
        """
        try:
            task = ScheduledTask.objects.get(id=task_id)
            task.is_active = False
            task.save()
            logger.info(f"Disabled scheduled task {task_id}")
            return True
            
        except ScheduledTask.DoesNotExist:
            logger.error(f"Scheduled task {task_id} not found")
            return False
    
    def enable_scheduled_task(self, task_id: int) -> bool:
        """
        Enable a scheduled task
        
        Args:
            task_id: ID of the task to enable
            
        Returns:
            bool: True if successfully enabled
        """
        try:
            task = ScheduledTask.objects.get(id=task_id)
            task.is_active = True
            task.save()
            logger.info(f"Enabled scheduled task {task_id}")
            return True
            
        except ScheduledTask.DoesNotExist:
            logger.error(f"Scheduled task {task_id} not found")
            return Falsefrom django.db import models

# Create your models here.
class Report(models.Model):
    title = models.CharField(max_length=255)
    creation_date = models.DateTimeField(auto_now_add=True)
    content = models.TextField()
    report_type = models.CharField(max_length=50)from django.shortcuts import render, get_object_or_404
from django.http import JsonResponse, HttpResponse, FileResponse
from django.views import View
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from django.template.loader import render_to_string
from django.conf import settings
from .models import Report
from .report_generator import ReportGenerator
import json
import os
import logging
import tempfile
from datetime import datetime
from django.core.serializers.json import DjangoJSONEncoder
from weasyprint import HTML, CSS

logger = logging.getLogger(__name__)

@method_decorator(csrf_exempt, name='dispatch')
class GenerateReportView(View):
    def __init__(self):
        super().__init__()
        self.generator = ReportGenerator()

    def post(self, request):
        try:
            data = json.loads(request.body)
            report_type = data.get('report_type', 'basic')
            target = data.get('target')
            workflow_id = data.get('workflow_id')  # Capture workflow_id if provided

            if not target:
                return JsonResponse({
                    'error': 'Target is required'
                }, status=400)

            # Generate and save the report
            report = self.generator.generate_report(report_type, target)

            response_data = {
                'status': 'success',
                'message': 'Report generated successfully',
                'report_id': report.id,
                'report_type': report_type,
                'report_title': report.title
            }
            
            # Add workflow_id to the response if it was provided
            if workflow_id:
                response_data['workflow_id'] = workflow_id
                
            return JsonResponse(response_data)

        except Exception as e:
            return JsonResponse({
                'error': 'Report generation failed',
                'details': str(e)
            }, status=500)

class ReportListView(View):
    def get(self, request):
        reports = Report.objects.all().values(
            'id', 'title', 'creation_date', 'report_type'
        ).order_by('-creation_date')
        return JsonResponse(list(reports), safe=False)

@method_decorator(csrf_exempt, name='dispatch')
class DownloadReportView(View):
    def get(self, request, report_id):
        try:
            report = Report.objects.get(id=report_id)
            workflow_id = request.GET.get('workflow_id')  # Get workflow_id from request
            
            try:
                # Parse the stored JSON content
                report_content = json.loads(report.content)
            except json.JSONDecodeError as e:
                return JsonResponse({
                    'error': 'Invalid report format',
                    'details': str(e)
                }, status=500)
            
            # Format the complete report
            formatted_report = {
                'id': report.id,
                'title': report.title,
                'content': report_content,
                'creation_date': report.creation_date.isoformat(),
                'report_type': report.report_type,
            }
            
            # Add workflow_id if available
            if workflow_id:
                formatted_report['workflow_id'] = workflow_id
            
            # Handle download request
            if request.GET.get('download') == 'true':
                try:
                    # Create a temporary file
                    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as tmp_file:
                        # Write formatted JSON to temp file
                        json.dump(formatted_report, tmp_file, indent=2, cls=DjangoJSONEncoder)
                    
                    # Prepare file response
                    filename = f"security_report_{report.id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                    response = FileResponse(
                        open(tmp_file.name, 'rb'),
                        content_type='application/json',
                        as_attachment=True,
                        filename=filename
                    )
                    
                    # Clean up temp file after response is sent
                    os.unlink(tmp_file.name)
                    
                    return response
                    
                except Exception as e:
                    return JsonResponse({
                        'error': 'Error creating download file',
                        'details': str(e)
                    }, status=500)
            
            # Return regular JSON response
            return JsonResponse(formatted_report)
            
        except Report.DoesNotExist:
            return JsonResponse({
                'error': 'Report not found'
            }, status=404)
        except Exception as e:
            return JsonResponse({
                'error': 'Error retrieving report',
                'details': str(e)
            }, status=500)

# File: reporting/views.py
def download_pdf_view(request, report_id):
    """Generate and download a PDF version of the report"""
    report = get_object_or_404(Report, id=report_id)
    
    try:
        # Parse the JSON content
        import json
        report_data = json.loads(report.content)
        
        # Get workflow info if available
        workflow_id = request.GET.get('workflow_id') or report_data.get('workflow_id')
        workflow = None
        tasks = []
        task_results = []
        
        if workflow_id:
            from automation.models import ScanWorkflow, ScanTask
            try:
                workflow = ScanWorkflow.objects.get(id=workflow_id)
                tasks = ScanTask.objects.filter(workflow_id=workflow_id).order_by('order')
                
                # Collect task results
                for task in tasks:
                    result_data = {}
                    if task.result:
                        try:
                            result_data = json.loads(task.result)
                        except:
                            result_data = {'error': 'Invalid JSON result'}
                    
                    task_results.append({
                        'id': task.id,
                        'name': task.name,
                        'type': task.task_type,
                        'status': task.status,
                        'start_time': task.start_time,
                        'end_time': task.end_time,
                        'duration': task.duration,
                        'result_data': result_data
                    })
            except ScanWorkflow.DoesNotExist:
                pass
        
        # Create context dictionary
        context = {
            'report': report,
            'report_data': report_data,
            'workflow': workflow,
            'tasks': tasks,
            'task_results': task_results
        }
        
        # If flat summary is available, add it to the context
        if '_flat_summary' in report_data:
            context.update({'summary': report_data['_flat_summary']})
        
        # Render HTML content for PDF
        html_string = render_to_string('reporting/pdf_report.html', context)
        
        # Create PDF from HTML
        html = HTML(string=html_string, base_url=request.build_absolute_uri('/'))
        result = html.write_pdf()
        
        # Create response with PDF content
        response = HttpResponse(result, content_type='application/pdf')
        response['Content-Disposition'] = f'attachment; filename="security_report_{report_id}.pdf"'
        return response
        
    except Exception as e:
        logger.error(f"Error generating PDF for report {report_id}: {str(e)}")
        return HttpResponse(f"Error generating PDF: {str(e)}", status=500)

def view_html_report(request, report_id):
        """View an HTML report"""
        report = get_object_or_404(Report, id=report_id)
        workflow_id = request.GET.get('workflow_id')  # Get workflow_id from request
        
        try:
            # Parse the JSON content
            import json
            report_data = json.loads(report.content)
            
            # Render the report using a template
            return render(request, 'reporting/html_report.html', {
                'report': report,
                'report_data': report_data,
                'workflow_id': workflow_id  # Pass workflow_id to template
            })
        except:
            # If JSON parsing fails, just display the raw content
            return HttpResponse(f"<pre>{report.content}</pre>")
        
def comprehensive_html_report(request, report_id):
        """View a comprehensive HTML report that includes all phase results"""
        report = get_object_or_404(Report, id=report_id)
        
        try:
            # Parse the JSON content with improved error handling
            import json
            try:
                report_data = json.loads(report.content)
                logger.info(f"Successfully parsed report data for report ID {report_id}")
            except json.JSONDecodeError as e:
                logger.error(f"JSON parsing error for report ID {report_id}: {str(e)}")
                # Return raw content with error message for debugging
                error_message = f"Error parsing report JSON: {str(e)}"
                return HttpResponse(f"<h1>Error Rendering Report</h1><p>{error_message}</p><pre>{report.content}</pre>")
            
            # Get the workflow id from the URL query parameter
            workflow_id = request.GET.get('workflow_id')
            logger.info(f"Processing report with workflow_id={workflow_id}")
            
            # If workflow_id provided, fetch all task results
            workflow = None
            tasks = []
            task_results = []
            
            if workflow_id:
                from automation.models import ScanWorkflow, ScanTask
                
                try:
                    workflow = ScanWorkflow.objects.get(id=workflow_id)
                    tasks = ScanTask.objects.filter(workflow_id=workflow_id).order_by('order')
                    logger.info(f"Found workflow {workflow_id} with {tasks.count()} tasks")
                    
                    # Collect all task results
                    task_results = []
                    for i, task in enumerate(tasks):
                        result_data = {}
                        if task.result:
                            try:
                                result_data = json.loads(task.result)
                                logger.info(f"Parsed task result for task {task.id} ({task.task_type})")
                            except json.JSONDecodeError as e:
                                logger.error(f"Error parsing task result for task {task.id}: {str(e)}")
                                result_data = {'error': f'Invalid JSON result: {str(e)}'}
                        
                        task_results.append({
                            'id': task.id,
                            'name': task.name,
                            'type': task.task_type,
                            'status': task.status,
                            'start_time': task.start_time,
                            'end_time': task.end_time,
                            'duration': task.end_time - task.start_time if task.start_time and task.end_time else None,
                            'result_data': result_data
                        })
                    
                    # Check if comprehensive_report.html template exists in the correct location
                    from django.template.loader import get_template
                    try:
                        template = get_template('reporting/comprehensive_report.html')
                        logger.info("Successfully located comprehensive report template")
                    except Exception as e:
                        logger.error(f"Template error: {str(e)}")
                        # Check all template directories for debugging
                        from django.conf import settings
                        template_dirs = settings.TEMPLATES[0]['DIRS']
                        logger.error(f"Template directories: {template_dirs}")
                        return HttpResponse(f"<h1>Template Error</h1><p>Could not find template: {str(e)}</p>")
                    
                    # Render the comprehensive report
                    context = {
                        'report': report,
                        'report_data': report_data,
                        'workflow': workflow,
                        'tasks': tasks,
                        'task_results': task_results,
                        'workflow_id': workflow_id  # Make sure to pass the workflow_id to template
                    }
                    
                    # Log context summary for debugging
                    logger.info(f"Report context: report_id={report_id}, workflow={workflow_id}, tasks={len(tasks)}, task_results={len(task_results)}")
                    
                    return render(request, 'reporting/comprehensive_report.html', context)
                    
                except ScanWorkflow.DoesNotExist:
                    logger.error(f"Workflow {workflow_id} not found")
                    # Fall back to regular report if workflow not found
                    return render(request, 'reporting/html_report.html', {
                        'report': report,
                        'report_data': report_data,
                        'error_message': f"Workflow with ID {workflow_id} not found"
                    })
            
            # Default to standard report if no workflow_id or workflow not found
            logger.info(f"Rendering standard HTML report for report ID {report_id} (no workflow_id provided)")
            return render(request, 'reporting/html_report.html', {
                'report': report,
                'report_data': report_data
            })
        except Exception as e:
            logger.error(f"Error rendering comprehensive report: {str(e)}")
            # Return a more user-friendly error page with details
            return HttpResponse(
                f"<h1>Error Rendering Report</h1>"
                f"<p>An error occurred while rendering the report: {str(e)}</p>"
                f"<h2>Report Content</h2>"
                f"<pre>{report.content}</pre>"
            )import logging
from datetime import datetime
import json
from django.core.serializers.json import DjangoJSONEncoder
from django.forms.models import model_to_dict
from .models import Report
from reconnaissance.models import Subdomain, PortScan
from vulnerability.models import Vulnerability

class ReportGenerator:
    def __init__(self):
        # Add logger initialization
        self.logger = logging.getLogger(__name__)
        
# File: reporting/report_generator.py
    def generate_report(self, report_type: str, target: str, output_format: str = 'json', scan_results: dict = None) -> Report:
        """
        Generate a security report with improved vulnerability handling
        
        Args:
            report_type: Type of report ('basic', 'detailed', 'executive')
            target: Target hostname/domain
            output_format: Format to generate ('json', 'html', 'pdf')
            scan_results: Optional dictionary containing scan results to include
            
        Returns:
            Report: The generated report object
        """
        # Clean target string
        target = target.strip()
        
        if report_type not in ['basic', 'detailed', 'executive']:
            report_type = 'basic'
        
        # Generate report content based on type
        try:
            self.logger.info(f"Starting report generation for {target}, type: {report_type}, format: {output_format}")
            
            # First, ensure vulnerabilities from scan_results are saved to the database
            if scan_results and 'vulnerabilities' in scan_results:
                vuln_list = scan_results['vulnerabilities']
                vuln_count = len(vuln_list)
                self.logger.info(f"Processing {vuln_count} vulnerabilities from scan results")
                
                from vulnerability.models import Vulnerability
                saved_count = 0
                
                # Save each vulnerability to database
                for vuln_data in vuln_list:
                    try:
                        # Ensure we have basic required fields
                        name = vuln_data.get('name', 'Unknown Vulnerability')
                        
                        # Normalize severity
                        severity = vuln_data.get('severity', 'LOW')
                        if isinstance(severity, str):
                            severity = severity.upper()
                        
                        # Get vuln_type from either 'type' or 'vuln_type' field
                        vuln_type = vuln_data.get('type', vuln_data.get('vuln_type', 'unknown'))
                        
                        # Create or update vulnerability in database
                        vuln, created = Vulnerability.objects.update_or_create(
                            target=target,
                            name=name,
                            defaults={
                                'description': vuln_data.get('description', ''),
                                'severity': severity,
                                'vuln_type': vuln_type,
                                'evidence': vuln_data.get('evidence', ''),
                                'source': vuln_data.get('source', 'scan'),
                                'confidence': vuln_data.get('confidence', 'medium'),
                                'cvss_score': vuln_data.get('cvss_score', 0.0),
                                'is_fixed': False
                            }
                        )
                        
                        status = "Created" if created else "Updated"
                        self.logger.debug(f"{status} vulnerability in database: {name}")
                        saved_count += 1
                        
                    except Exception as e:
                        self.logger.error(f"Error saving vulnerability '{vuln_data.get('name', 'unknown')}': {str(e)}")
                
                self.logger.info(f"Saved {saved_count} vulnerabilities to database")
                
                # Get fresh data from database to ensure report accuracy
                from vulnerability.models import Vulnerability
                db_vulns = Vulnerability.objects.filter(target=target, is_fixed=False)
                db_vuln_count = db_vulns.count()
                self.logger.info(f"Found {db_vuln_count} vulnerabilities in database for report")
            
            # Generate the appropriate report type
            if report_type == 'detailed':
                content = self.generate_detailed_report(target, scan_results)
            elif report_type == 'executive':
                content = self.generate_executive_report(target, scan_results)
            else:
                content = self.generate_basic_report(target, scan_results)
            
            # Make sure severity counts are properly calculated
            self._update_severity_counts(content)
            
            # Add workflow_id to content if provided in scan_results
            if scan_results and 'workflow_id' in scan_results:
                content['workflow_id'] = scan_results['workflow_id']
            
            # If output format is PDF, ensure proper formatting
            if output_format == 'pdf':
                content = self._format_for_pdf(content)
            
            # Verify vulnerability data is present
            if 'vulnerabilities' in content:
                vuln_count = len(content['vulnerabilities'])
                self.logger.info(f"Report contains {vuln_count} vulnerabilities")
            else:
                self.logger.warning("No vulnerabilities included in the report")
            
            # Properly serialize content to JSON string
            json_content = json.dumps(content, cls=DjangoJSONEncoder)
            
            # Create and save the report
            report = Report.objects.create(
                title=f"{report_type.capitalize()} Security Report - {target}",
                content=json_content,
                report_type=f"{report_type}_{output_format}"
            )
            
            self.logger.info(f"Generated {report_type} report for {target} with ID {report.id}")
            return report
                
        except Exception as e:
            self.logger.error(f"Report generation failed: {str(e)}")
            raise
    
    
    def _update_severity_counts(self, content: dict) -> None:
        """
        Ensure vulnerability severity counts are accurate with detailed logging
        """
        if 'vulnerabilities' not in content:
            self.logger.warning("No 'vulnerabilities' key found in content - cannot update severity counts")
            return
        
        # Reset counters
        severity_counts = {
            'critical': 0,
            'high': 0,
            'medium': 0,
            'low': 0,
            'info': 0
        }
        
        # Count each vulnerability by severity
        self.logger.info(f"Counting {len(content['vulnerabilities'])} vulnerabilities by severity")
        
        for vuln in content['vulnerabilities']:
            severity = vuln.get('severity', '').lower()
            self.logger.debug(f"Vulnerability: {vuln.get('name')}, Severity: {severity}")
            
            if severity in severity_counts:
                severity_counts[severity] += 1
            else:
                self.logger.warning(f"Unknown severity level: {severity} for vulnerability {vuln.get('name')}")
        
        # Update summary with accurate counts
        if 'summary' in content:
            for severity, count in severity_counts.items():
                content['summary'][severity] = count
                
            self.logger.info(f"Updated severity counts: {severity_counts}")
        else:
            self.logger.warning("No 'summary' key found in content - cannot update severity counts")
        
        return

    def _format_for_pdf(self, content: dict) -> dict:
        """
        Format report content specifically for PDF output
        
        Args:
            content: The raw report content
            
        Returns:
            dict: Report content formatted for PDF output
        """
        # Create a copy to avoid modifying the original
        pdf_content = content.copy()
        
        # Ensure summary data is accessible at both top level and within report_data
        # This addresses the template variable lookup issue
        if 'summary' in pdf_content:
            # Add a copy at the top level for direct template access
            summary = pdf_content['summary'].copy()
            
            # Ensure exploit matching data is included in the report
            if 'executive_summary' in pdf_content and 'exploit_matches' in pdf_content['executive_summary']:
                summary['exploit_matches'] = pdf_content['executive_summary']['exploit_matches']
            
            # Make severity counts accessible at top level if available
            if 'detailed_info' in pdf_content and 'vulnerability_severity' in pdf_content['detailed_info']:
                for key, value in pdf_content['detailed_info']['vulnerability_severity'].items():
                    summary[key] = value
        
        return pdf_content

    def _calculate_risk_level_from_summary(self, summary):
        """Calculate risk level based on summary data"""
        if summary.get('critical', 0) > 0:
            return 'CRITICAL'
        elif summary.get('high', 0) > 2:
            return 'HIGH'
        elif summary.get('high', 0) > 0 or summary.get('open_ports_count', 0) > 5:
            return 'MEDIUM'
        else:
            return 'LOW'

    def _format_network_data_for_pdf(self, network_data: dict) -> dict:
        """Format network visualization data for PDF output"""
        # Simplify network data to avoid rendering issues
        if not network_data:
            return {}
            
        # Limit number of nodes and links to avoid overcrowding
        if 'nodes' in network_data and len(network_data['nodes']) > 20:
            # Keep only the most important 20 nodes
            network_data['nodes'] = sorted(
                network_data['nodes'], 
                key=lambda x: self._get_node_importance(x)
            )[:20]
            
            # Keep only links between these nodes
            node_ids = {node['id'] for node in network_data['nodes']}
            network_data['links'] = [
                link for link in network_data.get('links', [])
                if link['source'] in node_ids and link['target'] in node_ids
            ]
        
        return network_data

    def _get_node_importance(self, node: dict) -> int:
        """Calculate node importance for filtering"""
        # Host nodes are most important
        if node.get('type') == 'host':
            return 100
            
        # Next subdomains
        if node.get('type') == 'subdomain':
            return 90
            
        # Important service nodes
        if node.get('type') == 'service':
            # Web services are more important
            if 'http' in node.get('name', '').lower():
                return 80
            return 70
            
        # High and critical vulnerabilities
        if node.get('type') == 'vulnerability':
            if 'critical' in node.get('name', '').lower():
                return 85
            if 'high' in node.get('name', '').lower():
                return 75
                
        # Default importance
        return 0

    # Existing methods below
    
    def generate_basic_report(self, target: str, scan_results: dict = None) -> dict:
        """Generate a basic security report with more robust handling of scan results"""
        # Fetch existing data from the database
        subdomains = Subdomain.objects.filter(domain=target)
        port_scans = PortScan.objects.filter(host=target)
        vulnerabilities = Vulnerability.objects.filter(target=target, is_fixed=False)
        open_ports = self._get_open_ports(target)

        # Use provided scan_results if available
        processed_vulns = []
        if scan_results and 'vulnerabilities' in scan_results:
            # Process incoming vulnerabilities from scan results
            vuln_count = len(scan_results['vulnerabilities'])
            self.logger.info(f"Processing {vuln_count} vulnerabilities from scan results")
            
            # Save vulnerabilities to database for reporting
            for vuln_data in scan_results['vulnerabilities']:
                try:
                    # Extract core vulnerability data
                    vuln_name = vuln_data.get('name', 'Unknown Vulnerability')
                    vuln_severity = vuln_data.get('severity', 'LOW')
                    
                    # Normalize severity
                    if isinstance(vuln_severity, str):
                        vuln_severity = vuln_severity.upper()
                    
                    # Create/update vulnerability record
                    vuln, created = Vulnerability.objects.update_or_create(
                        target=target,
                        name=vuln_name,
                        defaults={
                            'description': vuln_data.get('description', ''),
                            'severity': vuln_severity,
                            'vuln_type': vuln_data.get('type', vuln_data.get('vuln_type', 'unknown')),
                            'evidence': vuln_data.get('evidence', ''),
                            'source': vuln_data.get('source', 'scan'),
                            'confidence': vuln_data.get('confidence', 'medium'),
                            'cvss_score': vuln_data.get('cvss_score', 0.0),
                            'is_fixed': False
                        }
                    )
                    
                    # Add to processed list
                    processed_vulns.append(vuln)
                    self.logger.info(f"Processed vulnerability: {vuln_name} ({vuln_severity})")
                    
                except Exception as e:
                    self.logger.error(f"Error saving vulnerability from scan results: {str(e)}")
            
            # Refresh vulnerabilities from database to include the ones we just added
            if processed_vulns:
                self.logger.info(f"Successfully processed {len(processed_vulns)} vulnerabilities")
                # Use a combination of existing and newly added vulnerabilities
                vulnerabilities = Vulnerability.objects.filter(target=target, is_fixed=False)
                self.logger.info(f"Total vulnerabilities in database: {vulnerabilities.count()}")

        # Create the report structure
        report = {
            'target': target,
            'scan_date': datetime.now().isoformat(),
            'summary': {
                'total_subdomains': subdomains.count(),
                'total_ports_scanned': port_scans.count(),
                'total_vulnerabilities': vulnerabilities.count(),
                'open_ports_count': len(open_ports)
            },
            'subdomains': [model_to_dict(sub, exclude=['id']) for sub in subdomains],
            'open_ports': open_ports,
            'port_scan_summary': {
                'total_scanned': port_scans.count(),
                'open': port_scans.filter(state='open').count(),
                'closed': port_scans.filter(state='closed').count(),
                'filtered': port_scans.filter(state='filtered').count()
            },
            'vulnerabilities': [self._serialize_vulnerability(vuln) for vuln in vulnerabilities]
        }
        
        # Log key metrics
        self.logger.info(f"Generated basic report for {target} with {vulnerabilities.count()} vulnerabilities")
        
        return report

    def generate_detailed_report(self, target: str, scan_results: dict = None) -> dict:
        """Generate a detailed security report with exploit matches"""
        basic_report = self.generate_basic_report(target, scan_results)
        vulnerabilities = Vulnerability.objects.filter(target=target, is_fixed=False)
        
        # Enhanced port analysis
        open_ports = self._get_open_ports(target)
        port_risks = self._analyze_port_risks(open_ports)
        
        # Calculate vulnerability severity counts
        vulnerability_severity = {
            'critical': vulnerabilities.filter(severity='CRITICAL').count(),
            'high': vulnerabilities.filter(severity='HIGH').count(),
            'medium': vulnerabilities.filter(severity='MEDIUM').count(),
            'low': vulnerabilities.filter(severity='LOW').count()
        }
        
        # Calculate source counts
        vulnerability_sources = {
            'internal': vulnerabilities.filter(source='internal').count(),
            'zap': vulnerabilities.filter(source='zap').count(),
            'nuclei': vulnerabilities.filter(source='nuclei').count(),
            'multiple': vulnerabilities.exclude(source__in=['internal', 'zap', 'nuclei']).count()
        }
        
        # Add exploit matching statistics
        exploit_matching = {}
        try:
            # Try to get exploit matching info from scan results first
            if scan_results and 'exploit_matching' in scan_results:
                exploit_matching = scan_results['exploit_matching']
            else:
                # Otherwise calculate it from the database
                from exploit_manager.models import ExploitMatch
                
                # Get total matches
                total_matches = ExploitMatch.objects.filter(
                    vulnerability__target=target,
                    vulnerability__is_fixed=False
                ).count()
                
                # Get vulnerabilities with matches
                vulns_with_matches = vulnerabilities.filter(
                    exploit_matches__isnull=False
                ).distinct().count()
                
                # Get top matches by confidence
                top_matches = ExploitMatch.objects.filter(
                    vulnerability__target=target,
                    vulnerability__is_fixed=False
                ).order_by('-confidence_score')[:5]
                
                match_details = []
                for match in top_matches:
                    match_details.append({
                        'vulnerability_id': match.vulnerability.id,
                        'vulnerability_name': match.vulnerability.name,
                        'exploit_title': match.exploit.title,
                        'exploit_id': match.exploit.exploit_id,
                        'confidence': match.confidence_score,
                        'source_url': match.exploit.source_url,
                        'cve_id': match.exploit.cve_id
                    })
                
                exploit_matching = {
                    'total_vulnerabilities': vulnerabilities.count(),
                    'vulnerabilities_with_matches': vulns_with_matches,
                    'total_matches': total_matches,
                    'match_details': match_details
                }
                
        except Exception as e:
            self.logger.error(f"Error getting exploit match data: {str(e)}")
        
        detailed_info = {
            'port_analysis': {
                'high_risk_ports': port_risks['high_risk'],
                'medium_risk_ports': port_risks['medium_risk'],
                'low_risk_ports': port_risks['low_risk']
            },
            'vulnerability_severity': vulnerability_severity,
            'vulnerability_sources': vulnerability_sources,
            'exploit_matching': exploit_matching
        }
        
        # Log key metrics
        self.logger.info(f"Vulnerability severity counts: {vulnerability_severity}")
        if exploit_matching:
            self.logger.info(f"Exploit matching: {exploit_matching.get('total_matches', 0)} matches for {exploit_matching.get('vulnerabilities_with_matches', 0)} vulnerabilities")
        
        return {**basic_report, 'detailed_info': detailed_info}

    def generate_executive_report(self, target: str, scan_results: dict = None) -> dict:
        """Generate an executive summary report with exploit matches"""
        basic_report = self.generate_basic_report(target, scan_results)
        vulnerabilities = Vulnerability.objects.filter(target=target, is_fixed=False)
        high_vulns = vulnerabilities.filter(severity='HIGH')
        critical_vulns = vulnerabilities.filter(severity='CRITICAL')
        
        # Enhanced metrics
        risk_metrics = {
            'critical_severity_vulns': critical_vulns.count(),
            'high_severity_vulns': high_vulns.count(),
            'open_ports': len(basic_report['open_ports']),
            'total_vulnerabilities': vulnerabilities.count(),
            'high_risk_ports': len(self._analyze_port_risks(basic_report['open_ports'])['high_risk'])
        }
        
        # Get exploit matching information
        exploit_data = {}
        try:
            # Try to get exploit matching info from scan results first
            if scan_results and 'exploit_matching' in scan_results:
                exploit_data = scan_results['exploit_matching']
            else:
                # Otherwise calculate it from the database
                from exploit_manager.models import ExploitMatch
                
                # Count matches
                total_matches = ExploitMatch.objects.filter(
                    vulnerability__target=target,
                    vulnerability__is_fixed=False
                ).count()
                
                # Count vulnerabilities with matches
                vulns_with_matches = vulnerabilities.filter(
                    exploit_matches__isnull=False
                ).distinct().count()
                
                exploit_data = {
                    'total_matches': total_matches,
                    'vulnerabilities_with_matches': vulns_with_matches
                }
            
            # Add to risk metrics
            if exploit_data:
                risk_metrics['vulnerabilities_with_exploits'] = exploit_data.get('vulnerabilities_with_matches', 0)
                risk_metrics['total_exploit_matches'] = exploit_data.get('total_matches', 0)
        except Exception as e:
            self.logger.error(f"Error getting exploit match data for executive report: {str(e)}")
        
        # Combine critical and high vulnerabilities in findings
        top_findings = []
        for vuln in critical_vulns:
            top_findings.append(self._serialize_vulnerability(vuln))
        if len(top_findings) < 5:  # Limit to 5 findings total
            for vuln in high_vulns[:5-len(top_findings)]:
                top_findings.append(self._serialize_vulnerability(vuln))
        
        # Get top exploit matches
        top_exploit_matches = []
        try:
            if 'match_details' in exploit_data:
                top_exploit_matches = exploit_data['match_details']
            else:
                # Get from database if not in scan results
                from exploit_manager.models import ExploitMatch
                matches = ExploitMatch.objects.filter(
                    vulnerability__target=target,
                    vulnerability__is_fixed=False,
                    confidence_score__gte=0.4  # Only high confidence matches for executive report
                ).order_by('-confidence_score')[:3]
                
                for match in matches:
                    top_exploit_matches.append({
                        'vulnerability_name': match.vulnerability.name,
                        'exploit_title': match.exploit.title,
                        'confidence': match.confidence_score,
                        'cve_id': match.exploit.cve_id or "None",
                        'source_url': match.exploit.source_url
                    })
        except Exception as e:
            self.logger.error(f"Error getting top exploit matches: {str(e)}")
        
        executive_summary = {
            'risk_level': self._calculate_risk_level(risk_metrics),
            'critical_findings': top_findings,
            'risk_metrics': risk_metrics,
            'recommendations': self._generate_recommendations(risk_metrics, basic_report),
            'exploit_matches': top_exploit_matches
        }
        
        # Add extra recommendations for exploits if needed
        if risk_metrics.get('vulnerabilities_with_exploits', 0) > 0:
            executive_summary['recommendations'].insert(0, {
                'title': 'Address Vulnerabilities with Known Exploits',
                'description': f"Fix the {risk_metrics.get('vulnerabilities_with_exploits', 0)} vulnerabilities that have known public exploits as highest priority."
            })
        
        # Log key metrics
        self.logger.info(f"Executive report metrics: Critical={critical_vulns.count()}, High={high_vulns.count()}, Total={vulnerabilities.count()}")
        self.logger.info(f"Executive report exploit matches: {risk_metrics.get('total_exploit_matches', 0)}")
        
        return {**basic_report, 'executive_summary': executive_summary}

    def _serialize_port_scan(self, port_scan):
        """Serialize port scan data"""
        return {
            'port': port_scan.port,
            'service': port_scan.service,
            'state': port_scan.state,
            'protocol': port_scan.protocol,
            'banner': port_scan.banner if port_scan.banner else '',
            'scan_date': port_scan.scan_date.isoformat()
        }

    def _get_open_ports(self, target: str) -> list:
        """Get all open ports with details"""
        open_ports = PortScan.objects.filter(
            host=target,
            state='open'
        ).order_by('port')
        return [self._serialize_port_scan(port) for port in open_ports]

    def _analyze_port_risks(self, open_ports: list) -> dict:
        """Analyze risks associated with open ports"""
        high_risk_ports = [21, 23, 445, 3389]  # FTP, Telnet, SMB, RDP
        medium_risk_ports = [22, 25, 110, 143]  # SSH, SMTP, POP3, IMAP
        
        port_risks = {
            'high_risk': [],
            'medium_risk': [],
            'low_risk': []
        }
        
        for port_data in open_ports:
            port = port_data['port']
            if port in high_risk_ports:
                port_risks['high_risk'].append(port_data)
            elif port in medium_risk_ports:
                port_risks['medium_risk'].append(port_data)
            else:
                port_risks['low_risk'].append(port_data)
                
        return port_risks

    def _calculate_risk_level(self, metrics):
        """Calculate overall risk level based on metrics"""
        if metrics['critical_severity_vulns'] > 0:
            return 'CRITICAL'
        elif metrics['high_severity_vulns'] > 2:
            return 'HIGH'
        elif metrics['high_severity_vulns'] > 0 or metrics['high_risk_ports'] > 0:
            return 'MEDIUM'
        else:
            return 'LOW'
    
    def _generate_recommendations(self, metrics, basic_report):
        """Generate security recommendations based on findings"""
        recommendations = []
        
        # Add recommendations based on findings
        if metrics['critical_severity_vulns'] > 0 or metrics['high_severity_vulns'] > 0:
            recommendations.append({
                'title': 'Address High and Critical Vulnerabilities',
                'description': 'Fix identified critical and high vulnerabilities as a priority.'
            })
            
        if metrics['high_risk_ports'] > 0:
            recommendations.append({
                'title': 'Secure High Risk Ports',
                'description': 'Restrict access to high risk services or replace with more secure alternatives.'
            })
            
        # Always add general recommendations
        recommendations.append({
            'title': 'Regular Security Testing',
            'description': 'Perform regular security assessments to identify new vulnerabilities.'
        })
        
        return recommendations
    
# File: reporting/report_generator.py
# In the _serialize_vulnerability method, add exploit match information

# File: reporting/report_generator.py
# In the _serialize_vulnerability method, add exploit match information

    def _serialize_vulnerability(self, vuln):
        """
        Properly serialize a vulnerability instance with correct ID handling for exploits
        
        Args:
            vuln: Vulnerability model instance
            
        Returns:
            dict: Serialized vulnerability data with exploit details
        """
        # Clean up description and evidence using helper methods
        description = self._clean_description(vuln.description)
        evidence = self._clean_evidence(vuln.evidence)
        
        # Get exploit matches with proper ID handling for links
        exploit_matches = []
        try:
            from exploit_manager.models import ExploitMatch
            matches = ExploitMatch.objects.filter(vulnerability=vuln)
            
            # Log the number of matches found for debugging
            match_count = matches.count()
            self.logger.info(f"Found {match_count} exploit matches for vulnerability {vuln.id}")
            
            if match_count > 0:
                for match in matches:
                    exploit = match.exploit
                    # Include both database ID and exploit_id to support proper linking
                    exploit_data = {
                        'id': exploit.id,  # Database ID for URL construction
                        'exploit_id': exploit.exploit_id,  # ExploitDB ID for reference
                        'title': exploit.title,
                        'description': exploit.description[:100] + '...' if len(exploit.description) > 100 else exploit.description,
                        'confidence': match.confidence_score,
                        'status': match.status,
                        'source_url': exploit.source_url,
                        'cve_id': exploit.cve_id or "None"
                    }
                    exploit_matches.append(exploit_data)
                    self.logger.debug(f"Added exploit match: Database ID={exploit.id}, ExploitDB ID={exploit.exploit_id}")
        except Exception as e:
            self.logger.error(f"Error retrieving exploit matches for vulnerability {vuln.id}: {str(e)}")
        
        # Build the complete vulnerability data object
        vuln_data = {
            'id': vuln.id,
            'target': vuln.target,
            'name': vuln.name,
            'description': description,
            'severity': vuln.severity,
            'vuln_type': vuln.vuln_type,
            'type': vuln.vuln_type,  # Duplicate field for template compatibility
            'evidence': evidence,
            'discovery_date': vuln.discovery_date.isoformat(),
            'is_fixed': vuln.is_fixed,
            'fix_date': vuln.fix_date.isoformat() if vuln.fix_date else None,
            'source': vuln.source,
            'confidence': vuln.confidence,
            'cvss_score': vuln.cvss_score,
            'solution': vuln.solution if vuln.solution else '',
            'references': list(vuln.references) if vuln.references else [],
            'cwe': vuln.cwe if vuln.cwe else '',
            'metadata': dict(vuln.metadata) if vuln.metadata else {},
            'exploit_matches': exploit_matches  # Include properly formatted exploit matches
        }
        
        return vuln_data
# File: reporting/report_generator.py

    def _clean_description(self, description):
        """Clean up repetitive content in descriptions"""
        if not description:
            return ""
        
        # Remove prefixes like "zap:" that appear at the beginning
        if description.startswith("zap:"):
            description = description[4:].strip()
        
        # Split by paragraphs first (handles cases like the CSP text)
        paragraphs = description.split('\n\n')
        
        # Store unique paragraphs preserving order
        unique_paragraphs = []
        seen_paragraphs = set()
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            # Use the first 100 chars as a fingerprint to identify similar paragraphs
            fingerprint = paragraph[:100].lower()
            
            if fingerprint not in seen_paragraphs:
                unique_paragraphs.append(paragraph)
                seen_paragraphs.add(fingerprint)
        
        # For each paragraph, clean duplicate sentences
        cleaned_paragraphs = []
        for paragraph in unique_paragraphs:
            # Split by sentences
            sentences = paragraph.split('. ')
            
            # Remove duplicate sentences
            unique_sentences = []
            seen_sentences = set()
            
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                
                # Create a fingerprint for the sentence
                fingerprint = sentence.lower()
                
                # Only add if not already seen
                if fingerprint not in seen_sentences:
                    unique_sentences.append(sentence)
                    seen_sentences.add(fingerprint)
            
            # Rejoin sentences
            cleaned_paragraph = '. '.join(unique_sentences)
            if not cleaned_paragraph.endswith('.'):
                cleaned_paragraph += '.'
            
            cleaned_paragraphs.append(cleaned_paragraph)
        
        # Join the cleaned paragraphs, limit to a reasonable length
        result = '\n\n'.join(cleaned_paragraphs)
        
        # If still too long, truncate with an indicator
        if len(result) > 2000:
            result = result[:1997] + '...'
            
        return result

    def _clean_evidence(self, evidence):
        """Clean up repetitive content in evidence"""
        if not evidence:
            return ""
        
        # Split by newlines
        lines = evidence.split('\n')
        
        # Store unique evidence items with counts
        unique_lines = []
        seen_patterns = {}
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Extract the pattern (e.g., "nginx/1.19.0" from "zap: nginx/1.19.0")
            if ': ' in line:
                prefix, pattern = line.split(': ', 1)
                processed_line = f"{prefix}: {pattern}"
            else:
                prefix, pattern = "", line
                processed_line = line
            
            # Use lowercase for matching but preserve original case for display
            pattern_key = pattern.lower()
            
            # Track this pattern
            if pattern_key in seen_patterns:
                seen_patterns[pattern_key]['count'] += 1
                
                # Only keep a maximum of 2 examples per pattern
                if seen_patterns[pattern_key]['count'] <= 2:
                    unique_lines.append(processed_line)
            else:
                seen_patterns[pattern_key] = {
                    'count': 1,
                    'line': processed_line
                }
                unique_lines.append(processed_line)
        
        # Add counts for patterns with more than 2 occurrences
        result_lines = []
        
        # First add all the unique lines we want to keep
        for line in unique_lines:
            result_lines.append(line)
        
        # Then add summary counts for patterns with more occurrences
        for pattern, info in seen_patterns.items():
            if info['count'] > 2:
                extra_count = info['count'] - 2
                if extra_count > 0:
                    # Extract prefix from the line to maintain consistency
                    if ': ' in info['line']:
                        prefix = info['line'].split(': ', 1)[0]
                        result_lines.append(f"{prefix}: ... and {extra_count} more similar items")
                    else:
                        result_lines.append(f"... and {extra_count} more similar items")
        
        # Rejoin lines, limit overall size
        result = '\n'.join(result_lines)
        
        # If still too long, truncate
        if len(result) > 500:
            result = result[:497] + '...'
            
        return result
        
    def _similarity(self, str1, str2):
        """Calculate similarity between two strings"""
        # Simple similarity check based on word overlap
        words1 = set(str1.lower().split())
        words2 = set(str2.lower().split())
        
        if not words1 or not words2:
            return 0.0
            
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)from django.db import models

class NetworkNode(models.Model):
    """Represents a node in the network topology"""
    
    NODE_TYPES = [
        ('host', 'Host'),
        ('subdomain', 'Subdomain'),
        ('service', 'Service'),
        ('gateway', 'Gateway')
    ]
    
    domain = models.CharField(max_length=255, db_index=True)
    name = models.CharField(max_length=255)
    node_type = models.CharField(max_length=50, choices=NODE_TYPES)
    ip_address = models.GenericIPAddressField(null=True, blank=True)
    metadata = models.JSONField(default=dict, blank=True)
    
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    last_seen = models.DateTimeField(auto_now=True)
    is_active = models.BooleanField(default=True)
    
    class Meta:
        unique_together = ['domain', 'name', 'node_type']
        indexes = [
            models.Index(fields=['domain', 'is_active']),
            models.Index(fields=['node_type']),
        ]
    
    def __str__(self):
        return f"{self.name} ({self.node_type})"

class NetworkConnection(models.Model):
    """Represents a connection between two nodes in the network topology"""
    
    CONNECTION_TYPES = [
        ('domain', 'Domain Link'),
        ('service', 'Service Connection'),
        ('subdomain', 'Subdomain Link'),
        ('external', 'External Connection')
    ]
    
    source = models.ForeignKey(NetworkNode, on_delete=models.CASCADE, related_name='outgoing_connections')
    target = models.ForeignKey(NetworkNode, on_delete=models.CASCADE, related_name='incoming_connections')
    connection_type = models.CharField(max_length=50, choices=CONNECTION_TYPES)
    metadata = models.JSONField(default=dict, blank=True)
    
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    is_active = models.BooleanField(default=True)
    
    class Meta:
        unique_together = ['source', 'target', 'connection_type']
        indexes = [
            models.Index(fields=['source', 'is_active']),
            models.Index(fields=['target', 'is_active']),
        ]
    
    def __str__(self):
        return f"{self.source.name}  {self.target.name} ({self.connection_type})"# File: network_visualization/views.py
from django.shortcuts import render
from django.http import JsonResponse
from django.views import View
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from django.views.decorators.http import require_http_methods
import json
import logging
from urllib.parse import urlparse

from .models import NetworkNode, NetworkConnection
from .topology_mapper import TopologyMapper, generate_test_network_data

logger = logging.getLogger(__name__)

@method_decorator(csrf_exempt, name='dispatch')
class TopologyView(View):
    """View to get the network topology data for a target"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.mapper = TopologyMapper()
    
    def get(self, request, target=None):
        """Get the network topology for a target"""
        try:
            # Get target from URL parameter if not provided in the route
            if not target:
                target = request.GET.get('target')
            
            if not target:
                return JsonResponse({
                    'status': 'error',
                    'error': 'Target parameter is required'
                }, status=400)
            
            # Clean the target URL to extract just the domain
            parsed_url = urlparse(target)
            
            # If the target includes a protocol, extract just the domain
            if parsed_url.netloc:
                target_domain = parsed_url.netloc
            else:
                # If no protocol, the domain might be in the path
                target_domain = parsed_url.path
            
            # Remove port information if present
            if ':' in target_domain:
                target_domain = target_domain.split(':', 1)[0]
                
            # Remove trailing slashes
            target_domain = target_domain.rstrip('/')
            
            logger.info(f"Processing topology request for cleaned domain: {target_domain}")
            
            # Generate network map
            result = self.mapper.create_network_map(target_domain)
            
            # If no nodes found or error, return test data for development
            if result.get('status') == 'error' or not result.get('network_data', {}).get('nodes'):
                logger.warning(f"No network data found for {target_domain}, using test data")
                test_data = generate_test_network_data(target_domain)
                return JsonResponse({
                    'status': 'success',
                    'nodes': test_data['nodes'],
                    'links': test_data['links'],
                    'target': target_domain,
                    'test_data': True  # Flag that this is test data
                })
            
            # Return real data if available
            return JsonResponse({
                'status': 'success',
                'nodes': result['network_data']['nodes'],
                'links': result['network_data']['links'],
                'target': target_domain,
                'node_count': result['nodes'],
                'connection_count': result['connections']
            })
        
        except Exception as e:
            logger.error(f"Error generating topology: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'error': str(e)
            }, status=500)


class NetworkVisualizationView(View):
    """View to render the network visualization page"""
    
    def get(self, request, target=None):
        """Render the network visualization page"""
        if not target:
            target = request.GET.get('target', 'example.com')
        
        # Clean the target URL
        parsed_url = urlparse(target)
        
        # If the target includes a protocol, extract just the domain
        if parsed_url.netloc:
            target_domain = parsed_url.netloc
        else:
            # If no protocol, the domain might be in the path
            target_domain = parsed_url.path
        
        # Remove port information if present
        if ':' in target_domain:
            target_domain = target_domain.split(':', 1)[0]
            
        # Remove trailing slashes
        target_domain = target_domain.rstrip('/')
        
        # Get node count for the target
        node_count = NetworkNode.objects.filter(domain=target_domain, is_active=True).count()
        connection_count = NetworkConnection.objects.filter(source__domain=target_domain, is_active=True).count()
        
        return render(request, 'network_visualization/visualization.html', {
            'target': target_domain,
            'node_count': node_count,
            'connection_count': connection_count
        })

# Keep the rest of the file unchanged
@require_http_methods(["GET"])
def get_node_details(request, node_id):
    """Get detailed information about a specific node"""
    try:
        node = NetworkNode.objects.get(id=node_id, is_active=True)
        
        # Get connected nodes
        connected_nodes = NetworkNode.objects.filter(
            outgoing_connections__target=node,
            is_active=True
        ) | NetworkNode.objects.filter(
            incoming_connections__source=node,
            is_active=True
        ).distinct()
        
        return JsonResponse({
            'status': 'success',
            'node': {
                'id': node.id,
                'name': node.name,
                'domain': node.domain,
                'type': node.node_type,
                'ip': node.ip_address,
                'metadata': node.metadata,
                'last_seen': node.last_seen.isoformat(),
                'connected_nodes': [
                    {
                        'id': n.id,
                        'name': n.name,
                        'type': n.node_type,
                        'connection_type': n.outgoing_connections.filter(target=node).first().connection_type
                        if n.outgoing_connections.filter(target=node).exists()
                        else n.incoming_connections.filter(source=node).first().connection_type
                    }
                    for n in connected_nodes
                ]
            }
        })
        
    except NetworkNode.DoesNotExist:
        return JsonResponse({
            'status': 'error',
            'error': 'Node not found'
        }, status=404)
    except Exception as e:
        logger.error(f"Error getting node details: {str(e)}")
        return JsonResponse({
            'status': 'error',
            'error': str(e)
        }, status=500)

@require_http_methods(["GET"])
def get_network_stats(request, target_domain):
    """Get statistics about the network topology"""
    try:
        nodes = NetworkNode.objects.filter(
            domain=target_domain,
            is_active=True
        )
        node_ids = nodes.values_list('id', flat=True)
        connections = NetworkConnection.objects.filter(
            source_id__in=node_ids,
            is_active=True
        )
        
        stats = {
            'total_nodes': nodes.count(),
            'nodes_by_type': {},
            'total_connections': connections.count(),
            'connections_by_type': {},
            'domain_info': {
                'host_nodes': nodes.filter(node_type='host').count(),
                'subdomain_nodes': nodes.filter(node_type='subdomain').count(),
                'service_nodes': nodes.filter(node_type='service').count(),
                'gateway_nodes': nodes.filter(node_type='gateway').count(),
            }
        }
        
        # Count nodes by type
        for node_type, _ in NetworkNode.NODE_TYPES:
            stats['nodes_by_type'][node_type] = nodes.filter(node_type=node_type).count()
            
        # Count connections by type
        for conn_type, _ in NetworkConnection.CONNECTION_TYPES:
            stats['connections_by_type'][conn_type] = connections.filter(connection_type=conn_type).count()
            
        return JsonResponse({
            'status': 'success',
            'stats': stats
        })
        
    except Exception as e:
        logger.error(f"Error getting network stats: {str(e)}")
        return JsonResponse({
            'status': 'error',
            'error': str(e)
        }, status=500)
    
@require_http_methods(["GET"])
def get_network_topology(request, target_domain):
    """Get network topology for visualization"""
    try:
        # Clean the target URL
        from urllib.parse import urlparse
        parsed_url = urlparse(target_domain)
        
        # If the target includes a protocol, extract just the domain
        if parsed_url.netloc:
            clean_domain = parsed_url.netloc
        else:
            # If no protocol, the domain might be in the path
            clean_domain = parsed_url.path
        
        # Remove port information if present
        if ':' in clean_domain:
            clean_domain = clean_domain.split(':', 1)[0]
            
        # Remove trailing slashes
        clean_domain = clean_domain.rstrip('/')
        
        logger.info(f"Getting network topology for cleaned domain: {clean_domain}")
        
        # Create/update network map
        mapper = TopologyMapper()
        result = mapper.create_network_map(clean_domain)
        
        if result['status'] == 'error':
            return JsonResponse(result, status=500)
            
        # Return the network data directly from the result
        if 'network_data' in result and result['network_data']:
            return JsonResponse({
                'status': 'success',
                'nodes': result['network_data']['nodes'],
                'links': result['network_data']['links'],
                'stats': {
                    'total_nodes': len(result['network_data']['nodes']),
                    'total_links': len(result['network_data']['links'])
                }
            })
        
        # Fallback to test data if no real data available
        test_data = generate_test_network_data(clean_domain)
        return JsonResponse({
            'status': 'success',
            'nodes': test_data['nodes'],
            'links': test_data['links'],
            'stats': {
                'total_nodes': len(test_data['nodes']),
                'total_links': len(test_data['links'])
            },
            'test_data': True
        })
        
    except Exception as e:
        logger.error(f"Error getting network topology: {str(e)}")
        return JsonResponse({
            'status': 'error',
            'error': str(e)
        }, status=500)import math
from typing import Dict, List, Optional
import logging
from django.db import transaction
from reconnaissance.models import PortScan, Subdomain, Service
from .models import NetworkNode, NetworkConnection
import socket
from subprocess import Popen, PIPE
from datetime import datetime
import json
from django.core.serializers.json import DjangoJSONEncoder
from vulnerability.models import Vulnerability

class TopologyMapper:
    """
    Handles network topology mapping and visualization
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    @transaction.atomic
    def create_network_map(self, target_domain: str, services: list = None, subdomains: list = None) -> dict:
        """
        Create a network map for a target domain
        
        Args:
            target_domain: The domain to map
            services: Optional list of service data from service identification
            subdomains: Optional list of subdomain data from subdomain enumeration
            
        Returns:
            dict: Status and count information about created nodes/connections
        """
        try:
            self.logger.info(f"Creating network map for {target_domain}")
            
            # Create domain node if it doesn't exist
            domain_node, created = NetworkNode.objects.get_or_create(
                domain=target_domain,
                name=target_domain,
                node_type='host',
                defaults={
                    'ip_address': None,
                    'metadata': {'main_domain': True},
                    'is_active': True
                }
            )
            
            if created:
                self.logger.info(f"Created domain node for {target_domain}")
            
            # Process subdomain data if provided
            if subdomains:
                self.logger.info(f"Processing {len(subdomains)} subdomains for network mapping")
                self._process_subdomains(target_domain, domain_node, subdomains)
            else:
                # Otherwise use database subdomain records
                self._process_database_subdomains(target_domain, domain_node)
                
            # Process service data if provided
            if services:
                self.logger.info(f"Processing {len(services)} services for network mapping")
                self._process_services(target_domain, domain_node, services)
            else:
                # Otherwise use database service records
                self._process_database_services(target_domain, domain_node)
            
            # Add gateway nodes for key external connections
            self._add_gateway_nodes(target_domain, domain_node)
            
            # Add vulnerability nodes
            self._add_vulnerability_nodes(target_domain, domain_node)
            
            # Count nodes and connections for the target domain
            node_count = NetworkNode.objects.filter(
                domain=target_domain,
                is_active=True
            ).count()
            
            connection_count = NetworkConnection.objects.filter(
                source__domain=target_domain,
                is_active=True
            ).count()
            
            self.logger.info(f"Network map created with {node_count} nodes and {connection_count} connections")
            
            # Get the network data for visualization
            network_data = self.get_network_data(target_domain)
            
            return {
                'status': 'success',
                'target': target_domain,
                'nodes': node_count,
                'connections': connection_count,
                'network_data': network_data  # Include the network data in the result
            }
            
        except Exception as e:
            self.logger.error(f"Error creating network map: {str(e)}")
            return {
                'status': 'error',
                'error': str(e)
            }

    def _process_subdomains(self, target_domain, domain_node, subdomains):
        """Process subdomain data from scan results"""
        for subdomain_data in subdomains:
            subdomain = subdomain_data.get('subdomain')
            if not subdomain:
                continue
                
            try:
                # Create subdomain node
                subdomain_node, created = NetworkNode.objects.get_or_create(
                    domain=target_domain,
                    name=subdomain,
                    node_type='subdomain',
                    defaults={
                        'ip_address': subdomain_data.get('ip_address'),
                        'metadata': {
                            'is_http': subdomain_data.get('is_http'),
                            'http_status': subdomain_data.get('http_status'),
                            'status': subdomain_data.get('status')
                        },
                        'is_active': True
                    }
                )
                
                # Create connection to main domain
                NetworkConnection.objects.get_or_create(
                    source=domain_node,
                    target=subdomain_node,
                    connection_type='domain',
                    defaults={
                        'metadata': {},
                        'is_active': True
                    }
                )
            except Exception as e:
                self.logger.error(f"Error processing subdomain {subdomain}: {str(e)}")

    def _process_services(self, target_domain, domain_node, services):
        """Process service data from scan results"""
        for service_data in services:
            port = service_data.get('port')
            if not port:
                continue
                
            service_info = service_data.get('service', {})
            if not service_info:
                continue
                
            service_name = service_info.get('name', 'unknown')
            node_name = f"{service_name}:{port}"
            
            try:
                # Create service node
                service_node, created = NetworkNode.objects.get_or_create(
                    domain=target_domain,
                    name=node_name,
                    node_type='service',
                    defaults={
                        'ip_address': None,
                        'metadata': {
                            'port': port,
                            'protocol': service_data.get('protocol', 'tcp'),
                            'product': service_info.get('product', ''),
                            'version': service_info.get('version', ''),
                            'category': service_data.get('category', 'other'),
                            'risk_level': service_data.get('risk_level', 'MEDIUM')
                        },
                        'is_active': True
                    }
                )
                
                # Create connection to main domain
                NetworkConnection.objects.get_or_create(
                    source=domain_node,
                    target=service_node,
                    connection_type='service',
                    defaults={
                        'metadata': {'port': port},
                        'is_active': True
                    }
                )
            except Exception as e:
                self.logger.error(f"Error processing service {node_name}: {str(e)}")

    def _process_database_subdomains(self, target_domain, domain_node):
        """Use subdomain information from the database"""
        from reconnaissance.models import Subdomain
        
        subdomains = Subdomain.objects.filter(domain=target_domain, is_active=True)
        self.logger.info(f"Processing {subdomains.count()} subdomains from database")
        
        for subdomain in subdomains:
            try:
                # Create subdomain node
                subdomain_node, created = NetworkNode.objects.get_or_create(
                    domain=target_domain,
                    name=subdomain.subdomain,
                    node_type='subdomain',
                    defaults={
                        'ip_address': subdomain.ip_address,
                        'metadata': {},
                        'is_active': True
                    }
                )
                
                # Create connection to main domain
                NetworkConnection.objects.get_or_create(
                    source=domain_node,
                    target=subdomain_node,
                    connection_type='subdomain',
                    defaults={
                        'metadata': {},
                        'is_active': True
                    }
                )
            except Exception as e:
                self.logger.error(f"Error processing DB subdomain {subdomain.subdomain}: {str(e)}")

    def _process_database_services(self, target_domain, domain_node):
        """Use service information from the database"""
        from reconnaissance.models import Service
        
        services = Service.objects.filter(host=target_domain, is_active=True)
        self.logger.info(f"Processing {services.count()} services from database")
        
        for service in services:
            try:
                node_name = f"{service.name}:{service.port}"
                
                # Create service node
                service_node, created = NetworkNode.objects.get_or_create(
                    domain=target_domain,
                    name=node_name,
                    node_type='service',
                    defaults={
                        'ip_address': None,
                        'metadata': {
                            'port': service.port,
                            'protocol': service.protocol,
                            'product': service.product,
                            'version': service.version,
                            'category': service.category,
                            'risk_level': service.risk_level
                        },
                        'is_active': True
                    }
                )
                
                # Create connection to main domain
                NetworkConnection.objects.get_or_create(
                    source=domain_node,
                    target=service_node,
                    connection_type='service',
                    defaults={
                        'metadata': {'port': service.port},
                        'is_active': True
                    }
                )
            except Exception as e:
                self.logger.error(f"Error processing DB service {service.name}:{service.port}: {str(e)}")

    def _add_gateway_nodes(self, target_domain, domain_node):
        """Add gateway nodes for external connections"""
        from vulnerability.models import Vulnerability
        
        # Check if target has vulnerabilities related to external services
        external_vulns = Vulnerability.objects.filter(
            target=target_domain,
            vuln_type__in=['ssrf', 'open_redirect', 'external_service']
        )
        
        for vuln in external_vulns:
            try:
                # Create a gateway node for the vulnerability
                gateway_name = f"External: {vuln.name[:30]}"
                
                gateway_node, created = NetworkNode.objects.get_or_create(
                    domain=target_domain,
                    name=gateway_name,
                    node_type='gateway',
                    defaults={
                        'ip_address': None,
                        'metadata': {
                            'vulnerability_id': vuln.id,
                            'severity': vuln.severity
                        },
                        'is_active': True
                    }
                )
                
                # Create connection to main domain
                NetworkConnection.objects.get_or_create(
                    source=domain_node,
                    target=gateway_node,
                    connection_type='external',
                    defaults={
                        'metadata': {'severity': vuln.severity},
                        'is_active': True
                    }
                )
            except Exception as e:
                self.logger.error(f"Error adding gateway node for {vuln.name}: {str(e)}")

    def _add_vulnerability_nodes(self, target_domain, domain_node):
        """Add vulnerability nodes to network map"""
        from vulnerability.models import Vulnerability
        
        vulnerabilities = Vulnerability.objects.filter(
            target=target_domain,
            is_fixed=False
        )
        for vuln in vulnerabilities:
            try:
                # Skip some vulnerability types to avoid cluttering
                if vuln.vuln_type in ['info_disclosure', 'outdated_component']:
                    continue
                    
                # Create node name based on severity
                severity_prefix = {
                    'CRITICAL': 'Critical:',
                    'HIGH': 'High:',
                    'MEDIUM': 'Medium:',
                    'LOW': 'Low:'
                }.get(vuln.severity, '')
                
                node_name = f"{severity_prefix} {vuln.name[:40]}"
                
                # Check if vulnerability node type exists in model choices
                # If not, handle potential database integrity errors
                try:
                    vuln_node, created = NetworkNode.objects.get_or_create(
                        domain=target_domain,
                        name=node_name,
                        node_type='vulnerability',
                        defaults={
                            'ip_address': None,
                            'metadata': {
                                'vulnerability_id': vuln.id,
                                'severity': vuln.severity,
                                'type': vuln.vuln_type,
                                'cvss': vuln.cvss_score
                            },
                            'is_active': True
                        }
                    )
                except Exception as type_error:
                    # Fall back to a more generic node type if 'vulnerability' not in choices
                    self.logger.warning(f"Could not create vulnerability node with type 'vulnerability', using 'gateway' instead: {str(type_error)}")
                    vuln_node, created = NetworkNode.objects.get_or_create(
                        domain=target_domain,
                        name=node_name,
                        node_type='gateway',  # Fallback to gateway which should exist
                        defaults={
                            'ip_address': None,
                            'metadata': {
                                'vulnerability_id': vuln.id,
                                'severity': vuln.severity,
                                'type': vuln.vuln_type,
                                'cvss': vuln.cvss_score,
                                'is_vulnerability': True  # Mark it as actually a vulnerability
                            },
                            'is_active': True
                        }
                    )
                
                # Connect to domain node
                NetworkConnection.objects.get_or_create(
                    source=domain_node,
                    target=vuln_node,
                    connection_type='external',  # Using 'external' for vulnerabilities too
                    defaults={
                        'metadata': {'severity': vuln.severity},
                        'is_active': True
                    }
                )
            except Exception as e:
                self.logger.error(f"Error adding vulnerability node for {vuln.name}: {str(e)}")

    def _cleanup_old_data(self, target_domain: str):
        """Clean up old nodes and connections for the target domain"""
        # Deactivate old nodes
        NetworkNode.objects.filter(domain=target_domain).update(is_active=False)
        
        # Deactivate old connections
        NetworkConnection.objects.filter(
            source__domain=target_domain
        ).update(is_active=False)

    def _get_or_create_node(self, name: str, domain: str, node_type: str, 
                           ip_address: Optional[str] = None, 
                           metadata: Optional[Dict] = None) -> NetworkNode:
        """Gets existing node or creates a new one"""
        node, created = NetworkNode.objects.get_or_create(
            name=name,
            domain=domain,
            node_type=node_type,
            defaults={
                'ip_address': ip_address,
                'metadata': metadata or {},
                'is_active': True
            }
        )
        if not created:
            node.ip_address = ip_address
            node.is_active = True
            if metadata:
                node.metadata.update(metadata)
            node.save()
        return node

    def _create_connection(self, source: NetworkNode, target: NetworkNode, 
                          connection_type: str, metadata: Optional[Dict] = None) -> NetworkConnection:
        """Creates or updates a connection between nodes"""
        connection, created = NetworkConnection.objects.get_or_create(
            source=source,
            target=target,
            connection_type=connection_type,
            defaults={
                'metadata': metadata or {},
                'is_active': True
            }
        )
        if not created:
            connection.is_active = True
            if metadata:
                connection.metadata.update(metadata)
            connection.save()
        return connection
            
    def get_network_data(self, target_domain: str) -> Dict:
        """
        Get formatted network data suitable for D3.js visualization
        
        Args:
            target_domain: The domain to get network data for
            
        Returns:
            dict: Network data with nodes and links arrays
        """
        try:
            nodes = []
            links = []
            
            # Get all active nodes for the domain
            domain_nodes = NetworkNode.objects.filter(
                domain=target_domain,
                is_active=True
            )
            
            if not domain_nodes.exists():
                self.logger.warning(f"No active nodes found for domain {target_domain}")
                return {
                    'nodes': [],
                    'links': []
                }
            
            # Create node data for visualization
            for node in domain_nodes:
                node_data = {
                    'id': str(node.id),  # Convert to string to ensure compatibility with D3
                    'name': node.name,
                    'type': node.node_type,
                    'info': str(node.ip_address) if node.ip_address else ''
                }
                
                # Add some metadata if available
                if isinstance(node.metadata, dict):
                    for key in ['port', 'product', 'version', 'category', 'risk_level', 'severity']:
                        if key in node.metadata:
                            node_data[key] = node.metadata[key]
                
                nodes.append(node_data)
            
            # Get all active connections for the domain
            node_ids = domain_nodes.values_list('id', flat=True)
            connections = NetworkConnection.objects.filter(
                source_id__in=node_ids,
                is_active=True
            )
            
            # Create link data for visualization, ensuring source/target are strings
            for conn in connections:
                links.append({
                    'source': str(conn.source_id),  # Must match the node id format (string)
                    'target': str(conn.target_id),  # Must match the node id format (string)
                    'type': conn.connection_type
                })
            
            # Log the data being returned
            self.logger.info(f"Returning network data with {len(nodes)} nodes and {len(links)} links for {target_domain}")
            
            if nodes:
                # Position main domain node at center
                center_node = next((node for node in nodes if node.get('type') == 'host'), nodes[0])
                center_node['x'] = 0
                center_node['y'] = 0
                
                # Position other nodes in concentric circles
                types_order = ['subdomain', 'service', 'gateway', 'vulnerability']
                
                for i, node_type in enumerate(types_order):
                    type_nodes = [node for node in nodes if node.get('type') == node_type]
                    radius = 150 * (i + 1)  # Increasing radius for each type
                    angle_step = (2 * math.pi) / max(len(type_nodes), 1)
                    
                    for j, node in enumerate(type_nodes):
                        angle = angle_step * j
                        node['x'] = radius * math.cos(angle)
                        node['y'] = radius * math.sin(angle)
            
            return {
                'nodes': nodes,
                'links': links
            }


            
        except Exception as e:
            self.logger.error(f"Error getting network data: {str(e)}")
            # Return minimal valid data structure instead of raising error
            return {
                'nodes': [],
                'links': [],
                'error': str(e)
            }

# Example dummy data for testing
def generate_test_network_data(target: str = "example.com") -> Dict:
    """Generate test network data for visualization testing"""
    return {
        "nodes": [
            {"id": "host_1", "name": target, "type": "host", "info": "Main target"},
            {"id": "subdomain_1", "name": f"www.{target}", "type": "subdomain", "info": "Web server"},
            {"id": "subdomain_2", "name": f"api.{target}", "type": "subdomain", "info": "API server"},
            {"id": "port_1", "name": "Port 80", "type": "port", "info": "HTTP port"},
            {"id": "port_2", "name": "Port 443", "type": "port", "info": "HTTPS port"},
            {"id": "port_3", "name": "Port 22", "type": "port", "info": "SSH port"},
            {"id": "service_1", "name": "HTTP", "type": "service", "info": "Web service"},
            {"id": "service_2", "name": "HTTPS", "type": "service", "info": "Secure web service"},
            {"id": "service_3", "name": "SSH", "type": "service", "info": "Secure shell"},
            {"id": "vuln_1", "name": "SQL Injection", "type": "vulnerability", "info": "Severity: HIGH"}
        ],
        "links": [
            {"source": "host_1", "target": "subdomain_1", "type": "contains"},
            {"source": "host_1", "target": "subdomain_2", "type": "contains"},
            {"source": "subdomain_1", "target": "port_1", "type": "contains"},
            {"source": "subdomain_1", "target": "port_2", "type": "contains"},
            {"source": "subdomain_2", "target": "port_3", "type": "contains"},
            {"source": "port_1", "target": "service_1", "type": "communicates"},
            {"source": "port_2", "target": "service_2", "type": "communicates"},
            {"source": "port_3", "target": "service_3", "type": "communicates"},
            {"source": "service_1", "target": "vuln_1", "type": "has_vulnerability"}
        ]
    }# manual_exploitation/models.py
from django.db import models
from django.utils import timezone

class ExploitationSession(models.Model):
    """
    Represents a controlled manual exploitation session against a target
    """
    STATUS_CHOICES = [
        ('created', 'Created'),
        ('authorized', 'Authorized'),
        ('in_progress', 'In Progress'),
        ('completed', 'Completed'),
        ('failed', 'Failed'),
        ('aborted', 'Aborted')
    ]
    
    # Session identification
    name = models.CharField(max_length=255)
    target = models.CharField(max_length=255)
    description = models.TextField(blank=True)
    
    # Exploitation details
    vulnerability = models.ForeignKey('vulnerability.Vulnerability', on_delete=models.CASCADE, related_name='exploitation_sessions')
    exploit = models.ForeignKey('exploit_manager.Exploit', on_delete=models.CASCADE, related_name='exploitation_sessions')
    
    # Status tracking
    status = models.CharField(max_length=20, choices=STATUS_CHOICES, default='created')
    created_at = models.DateTimeField(auto_now_add=True)
    started_at = models.DateTimeField(null=True, blank=True)
    completed_at = models.DateTimeField(null=True, blank=True)
    
    # Parameters and customization
    parameters = models.JSONField(default=dict, blank=True)
    customized_code = models.TextField(blank=True)
    
    # Safety controls
    authorized_by = models.CharField(max_length=100, blank=True)
    safety_checks_performed = models.BooleanField(default=False)
    scope_limitations = models.TextField(blank=True)
    max_execution_time = models.IntegerField(default=300)  # in seconds
    
    # Results
    result_summary = models.TextField(blank=True)
    success = models.BooleanField(null=True)
    
    # Metadata
    notes = models.TextField(blank=True)
    tags = models.JSONField(default=list, blank=True)
    
    class Meta:
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['status']),
            models.Index(fields=['target']),
            models.Index(fields=['created_at']),
        ]
    
    def __str__(self):
        return f"{self.name} - {self.target} ({self.status})"
    
    def start_session(self):
        """Start the exploitation session"""
        self.status = 'in_progress'
        self.started_at = timezone.now()
        self.save()
    
    def complete_session(self, success, summary):
        """Complete the exploitation session"""
        self.status = 'completed'
        self.completed_at = timezone.now()
        self.success = success
        self.result_summary = summary
        self.save()
    
    def abort_session(self, reason):
        """Abort the exploitation session"""
        self.status = 'aborted'
        self.completed_at = timezone.now()
        self.success = False
        self.result_summary = f"Aborted: {reason}"
        self.save()


class ExploitationLog(models.Model):
    """
    Log entry for an exploitation session
    """
    LOG_LEVELS = [
        ('info', 'Info'),
        ('warning', 'Warning'),
        ('error', 'Error'),
        ('critical', 'Critical'),
        ('success', 'Success'),
        ('debug', 'Debug')
    ]
    
    session = models.ForeignKey(ExploitationSession, on_delete=models.CASCADE, related_name='logs')
    timestamp = models.DateTimeField(auto_now_add=True)
    level = models.CharField(max_length=10, choices=LOG_LEVELS, default='info')
    message = models.TextField()
    
    class Meta:
        ordering = ['timestamp']
    
    def __str__(self):
        return f"[{self.level.upper()}] {self.timestamp.strftime('%Y-%m-%d %H:%M:%S')} - {self.message[:50]}"


class ExploitationEvidence(models.Model):
    """
    Evidence collected during an exploitation session
    """
    EVIDENCE_TYPES = [
        ('screenshot', 'Screenshot'),
        ('log', 'Log File'),
        ('data', 'Extracted Data'),
        ('traffic', 'Network Traffic'),
        ('response', 'Server Response'),
        ('output', 'Command Output'),
        ('other', 'Other')
    ]
    
    session = models.ForeignKey(ExploitationSession, on_delete=models.CASCADE, related_name='evidence')
    name = models.CharField(max_length=255)
    description = models.TextField(blank=True)
    evidence_type = models.CharField(max_length=20, choices=EVIDENCE_TYPES)
    content = models.TextField()
    metadata = models.JSONField(default=dict, blank=True)
    timestamp = models.DateTimeField(auto_now_add=True)
    
    class Meta:
        ordering = ['-timestamp']
    
    def __str__(self):
        return f"{self.name} ({self.evidence_type})"


class SafetyControl(models.Model):
    """
    Safety control for exploitation
    """
    CONTROL_TYPES = [
        ('network_limit', 'Network Limitation'),
        ('time_limit', 'Time Limitation'),
        ('scope_limit', 'Scope Limitation'),
        ('resource_limit', 'Resource Limitation'),
        ('authentication', 'Authentication Required'),
        ('approval', 'Approval Required'),
        ('other', 'Other Control')
    ]
    
    name = models.CharField(max_length=100)
    description = models.TextField()
    control_type = models.CharField(max_length=20, choices=CONTROL_TYPES)
    is_mandatory = models.BooleanField(default=True)
    configuration = models.JSONField(default=dict, blank=True)
    
    class Meta:
        ordering = ['name']
    
    def __str__(self):
        return f"{self.name} ({self.control_type})"


class ExploitationTemplate(models.Model):
    """
    Template for common exploitation scenarios
    """
    name = models.CharField(max_length=255)
    description = models.TextField()
    exploit_type = models.CharField(max_length=100)
    template_code = models.TextField()
    parameters = models.JSONField(default=dict)
    required_safety_controls = models.ManyToManyField(SafetyControl)
    
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    class Meta:
        ordering = ['name']
    
    def __str__(self):
        return f"{self.name} - {self.exploit_type}"# manual_exploitation/views.py
from django.shortcuts import render, get_object_or_404, redirect
from django.views import View
from django.http import JsonResponse, HttpResponse
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from django.utils import timezone
from django.db import transaction
from django.contrib import messages
import json
import logging
import subprocess
import os
import tempfile
import threading
import time
import traceback

from .models import (
    ExploitationSession, ExploitationLog, 
    ExploitationEvidence, SafetyControl, 
    ExploitationTemplate
)
from vulnerability.models import Vulnerability
from exploit_manager.models import Exploit
from exploit_manager.matcher import ExploitMatcher

logger = logging.getLogger(__name__)

class ExploitationDashboardView(View):
    """
    Dashboard for manual exploitation interface
    """
    def get(self, request):
        # Get basic statistics for the dashboard
        active_sessions = ExploitationSession.objects.filter(
            status__in=['in_progress', 'authorized']
        )
        completed_sessions = ExploitationSession.objects.filter(
            status='completed'
        )
        recent_sessions = ExploitationSession.objects.all().order_by('-created_at')[:10]
        
        # Get exploits with high match confidence
        from django.db.models import Max
        from exploit_manager.models import ExploitMatch
        
        top_matches = ExploitMatch.objects.values(
            'vulnerability__id', 'vulnerability__name', 'vulnerability__target'
        ).annotate(
            max_confidence=Max('confidence_score')
        ).filter(
            max_confidence__gte=0.7
        ).order_by('-max_confidence')[:10]
        
        context = {
            'active_sessions': active_sessions,
            'completed_sessions': completed_sessions,
            'recent_sessions': recent_sessions,
            'top_matches': top_matches,
            'total_sessions': ExploitationSession.objects.count(),
            'successful_exploits': ExploitationSession.objects.filter(success=True).count(),
            'failed_exploits': ExploitationSession.objects.filter(success=False).count()
        }
        
        return render(request, 'manual_exploitation/dashboard.html', context)


class ExploitationSessionListView(View):
    """
    List view for exploitation sessions
    """
    def get(self, request):
        status_filter = request.GET.get('status', None)
        target_filter = request.GET.get('target', None)
        
        sessions = ExploitationSession.objects.all()
        
        if status_filter:
            sessions = sessions.filter(status=status_filter)
        if target_filter:
            sessions = sessions.filter(target__icontains=target_filter)
            
        sessions = sessions.order_by('-created_at')
        
        return render(request, 'manual_exploitation/session_list.html', {
            'sessions': sessions,
            'status_filter': status_filter,
            'target_filter': target_filter
        })


@method_decorator(csrf_exempt, name='dispatch')
class ExploitationSessionCreateView(View):
    """
    Create a new exploitation session
    """
    def get(self, request):
        vulnerability_id = request.GET.get('vulnerability_id')
        exploit_id = request.GET.get('exploit_id')
        
        vulnerability = None
        exploit = None
        exploit_matches = []
        
        if vulnerability_id:
            vulnerability = get_object_or_404(Vulnerability, id=vulnerability_id)
            # Get matching exploits
            from exploit_manager.models import ExploitMatch
            matches = ExploitMatch.objects.filter(
                vulnerability_id=vulnerability_id
            ).order_by('-confidence_score')
            
            exploit_matches = [{
                'id': match.exploit.id,
                'title': match.exploit.title,
                'confidence': match.confidence_score,
                'exploit_id': match.exploit.exploit_id
            } for match in matches]
            
        if exploit_id:
            exploit = get_object_or_404(Exploit, id=exploit_id)
        
        # Get safety controls
        safety_controls = SafetyControl.objects.filter(is_mandatory=True)
        
        # Get exploitation templates
        templates = ExploitationTemplate.objects.all()
        
        return render(request, 'manual_exploitation/session_create.html', {
            'vulnerability': vulnerability,
            'exploit': exploit,
            'exploit_matches': exploit_matches,
            'safety_controls': safety_controls,
            'templates': templates
        })
    
    def post(self, request):
        try:
            data = json.loads(request.body) if request.body else request.POST.dict()
            
            # Validate required fields
            required_fields = ['name', 'target', 'vulnerability_id', 'exploit_id']
            for field in required_fields:
                if field not in data:
                    return JsonResponse({
                        'status': 'error',
                        'message': f'Missing required field: {field}'
                    }, status=400)
            
            # Get vulnerability and exploit
            vulnerability = get_object_or_404(Vulnerability, id=data['vulnerability_id'])
            exploit = get_object_or_404(Exploit, id=data['exploit_id'])
            
            # Create session with transaction to ensure atomicity
            with transaction.atomic():
                session = ExploitationSession.objects.create(
                    name=data['name'],
                    target=data['target'],
                    description=data.get('description', ''),
                    vulnerability=vulnerability,
                    exploit=exploit,
                    parameters=data.get('parameters', {}),
                    customized_code=data.get('customized_code', exploit.code),
                    scope_limitations=data.get('scope_limitations', ''),
                    max_execution_time=data.get('max_execution_time', 300)
                )
                
                # Create initial log entry
                ExploitationLog.objects.create(
                    session=session,
                    level='info',
                    message=f"Session created for {session.target} using exploit: {exploit.title}"
                )
                
                # Log safety control status
                safety_checks = data.get('safety_checks', [])
                if safety_checks:
                    safety_message = "Safety controls applied: " + ", ".join(safety_checks)
                    ExploitationLog.objects.create(
                        session=session,
                        level='info',
                        message=safety_message
                    )
                    session.safety_checks_performed = True
                    session.save()
                
            if request.content_type == 'application/json':
                return JsonResponse({
                    'status': 'success',
                    'message': 'Exploitation session created successfully',
                    'session_id': session.id
                })
            else:
                messages.success(request, 'Exploitation session created successfully')
                return redirect('manual_exploitation:session_detail', session_id=session.id)
                
        except Exception as e:
            logger.error(f"Error creating exploitation session: {str(e)}")
            if request.content_type == 'application/json':
                return JsonResponse({
                    'status': 'error',
                    'message': str(e)
                }, status=500)
            else:
                messages.error(request, f'Error creating session: {str(e)}')
                return redirect('manual_exploitation:session_list')


class ExploitationSessionDetailView(View):
    """
    View exploitation session details
    """
    def get(self, request, session_id):
        session = get_object_or_404(ExploitationSession, id=session_id)
        
        # Get logs
        logs = session.logs.all().order_by('timestamp')
        
        # Get evidence
        evidence = session.evidence.all().order_by('-timestamp')
        
        return render(request, 'manual_exploitation/session_detail.html', {
            'session': session,
            'logs': logs,
            'evidence': evidence
        })


@method_decorator(csrf_exempt, name='dispatch')
class ExploitationExecuteView(View):
    """
    Execute an exploitation session
    """
    def post(self, request, session_id):
        session = get_object_or_404(ExploitationSession, id=session_id)
        
        # Check if session can be executed
        if session.status not in ['created', 'authorized']:
            return JsonResponse({
                'status': 'error',
                'message': f'Cannot execute session in {session.status} status'
            }, status=400)
        
        # Ensure safety checks are performed
        if not session.safety_checks_performed:
            return JsonResponse({
                'status': 'error',
                'message': 'Safety checks must be performed before execution'
            }, status=400)
        
        # Run execution in a separate thread
        execution_thread = threading.Thread(
            target=self._execute_session,
            args=(session,)
        )
        execution_thread.daemon = True
        execution_thread.start()
        
        # Update session status
        session.start_session()
        
        return JsonResponse({
            'status': 'success',
            'message': 'Exploitation session started'
        })
    
    def _execute_session(self, session):
        """Execute the exploitation in a controlled environment"""
        try:
            # Log start
            ExploitationLog.objects.create(
                session=session,
                level='info',
                message=f"Starting exploitation execution against {session.target}"
            )
            
            # Prepare execution environment
            with tempfile.TemporaryDirectory() as temp_dir:
                # Write exploit code to file
                exploit_file = os.path.join(temp_dir, 'exploit.py')
                with open(exploit_file, 'w') as f:
                    f.write(session.customized_code)
                
                # Prepare parameters
                params = []
                for key, value in session.parameters.items():
                    if key and value:
                        params.append(f"--{key}={value}")
                
                # Add target to parameters
                params.append(f"--target={session.target}")
                
                # Set a timeout for execution
                timeout = session.max_execution_time
                
                # Log command
                cmd = f"python {exploit_file} {' '.join(params)}"
                ExploitationLog.objects.create(
                    session=session,
                    level='info',
                    message=f"Executing command: {cmd}"
                )
                
                # Execute the exploit with timeout
                start_time = time.time()
                try:
                    process = subprocess.Popen(
                        ['python', exploit_file] + params,
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE,
                        universal_newlines=True
                    )
                    
                    # Monitor the process with timeout
                    output, errors = "", ""
                    while process.poll() is None:
                        # Check if timeout exceeded
                        if time.time() - start_time > timeout:
                            process.kill()
                            ExploitationLog.objects.create(
                                session=session,
                                level='error',
                                message=f"Execution timed out after {timeout} seconds"
                            )
                            session.abort_session("Execution timed out")
                            return
                        
                        # Collect output in chunks
                        stdout_data = process.stdout.readline()
                        if stdout_data:
                            output += stdout_data
                            ExploitationLog.objects.create(
                                session=session,
                                level='info',
                                message=f"Output: {stdout_data.strip()}"
                            )
                            
                        stderr_data = process.stderr.readline()
                        if stderr_data:
                            errors += stderr_data
                            ExploitationLog.objects.create(
                                session=session,
                                level='warning',
                                message=f"Error: {stderr_data.strip()}"
                            )
                            
                        time.sleep(0.1)
                    
                    # Get any remaining output
                    stdout_data, stderr_data = process.communicate()
                    if stdout_data:
                        output += stdout_data
                        ExploitationLog.objects.create(
                            session=session,
                            level='info',
                            message=f"Output: {stdout_data.strip()}"
                        )
                    if stderr_data:
                        errors += stderr_data
                        ExploitationLog.objects.create(
                            session=session,
                            level='warning',
                            message=f"Error: {stderr_data.strip()}"
                        )
                    
                    # Save evidence
                    ExploitationEvidence.objects.create(
                        session=session,
                        name="Execution Output",
                        description="Standard output from exploit execution",
                        evidence_type="output",
                        content=output
                    )
                    
                    if errors:
                        ExploitationEvidence.objects.create(
                            session=session,
                            name="Execution Errors",
                            description="Standard error from exploit execution",
                            evidence_type="output",
                            content=errors
                        )
                    
                    # Check exit code
                    exit_code = process.returncode
                    success = exit_code == 0
                    
                    # Complete session
                    if success:
                        session.complete_session(
                            success=True,
                            summary=f"Exploitation completed successfully in {time.time() - start_time:.1f} seconds"
                        )
                        ExploitationLog.objects.create(
                            session=session,
                            level='success',
                            message="Exploitation completed successfully"
                        )
                    else:
                        session.complete_session(
                            success=False,
                            summary=f"Exploitation failed with exit code {exit_code}"
                        )
                        ExploitationLog.objects.create(
                            session=session,
                            level='error',
                            message=f"Exploitation failed with exit code {exit_code}"
                        )
                
                except Exception as e:
                    error_detail = traceback.format_exc()
                    ExploitationLog.objects.create(
                        session=session,
                        level='error',
                        message=f"Execution error: {str(e)}\n{error_detail}"
                    )
                    session.abort_session(f"Execution error: {str(e)}")
        
        except Exception as e:
            error_detail = traceback.format_exc()
            logger.error(f"Error in exploitation execution: {str(e)}\n{error_detail}")
            ExploitationLog.objects.create(
                session=session,
                level='critical',
                message=f"Critical error: {str(e)}\n{error_detail}"
            )
            session.abort_session(f"Critical error: {str(e)}")


@method_decorator(csrf_exempt, name='dispatch')
class ExploitationAbortView(View):
    """
    Abort an exploitation session
    """
    def post(self, request, session_id):
        session = get_object_or_404(ExploitationSession, id=session_id)
        
        # Check if session can be aborted
        if session.status not in ['in_progress', 'authorized']:
            return JsonResponse({
                'status': 'error',
                'message': f'Cannot abort session in {session.status} status'
            }, status=400)
        
        # Get abort reason
        data = json.loads(request.body) if request.content_type == 'application/json' else request.POST.dict()
        reason = data.get('reason', 'User requested abort')
        
        # Abort session
        session.abort_session(reason)
        
        # Log abort
        ExploitationLog.objects.create(
            session=session,
            level='warning',
            message=f"Session aborted: {reason}"
        )
        
        return JsonResponse({
            'status': 'success',
            'message': 'Exploitation session aborted'
        })


@method_decorator(csrf_exempt, name='dispatch')
class ExploitationAuthorizeView(View):
    """
    Authorize an exploitation session
    """
    def post(self, request, session_id):
        session = get_object_or_404(ExploitationSession, id=session_id)
        
        # Check if session can be authorized
        if session.status != 'created':
            return JsonResponse({
                'status': 'error',
                'message': f'Cannot authorize session in {session.status} status'
            }, status=400)
        
        # Get authorization details
        data = json.loads(request.body) if request.content_type == 'application/json' else request.POST.dict()
        authorizer = data.get('authorized_by', request.user.username if hasattr(request, 'user') else 'Anonymous')
        
        # Update session
        session.status = 'authorized'
        session.authorized_by = authorizer
        session.save()
        
        # Log authorization
        ExploitationLog.objects.create(
            session=session,
            level='info',
            message=f"Session authorized by {authorizer}"
        )
        
        return JsonResponse({
            'status': 'success',
            'message': 'Exploitation session authorized'
        })


@method_decorator(csrf_exempt, name='dispatch')
class ExploitationEvidenceView(View):
    """
    Add evidence to an exploitation session
    """
    def post(self, request, session_id):
        session = get_object_or_404(ExploitationSession, id=session_id)
        
        try:
            # Parse data based on content type
            if request.content_type == 'application/json':
                data = json.loads(request.body)
            else:
                data = request.POST.dict()
                
            # Create evidence
            evidence = ExploitationEvidence.objects.create(
                session=session,
                name=data.get('name', 'Untitled Evidence'),
                description=data.get('description', ''),
                evidence_type=data.get('evidence_type', 'other'),
                content=data.get('content', ''),
                metadata=data.get('metadata', {})
            )
            
            # Log evidence addition
            ExploitationLog.objects.create(
                session=session,
                level='info',
                message=f"Evidence added: {evidence.name}"
            )
            
            return JsonResponse({
                'status': 'success',
                'message': 'Evidence added successfully',
                'evidence_id': evidence.id
            })
            
        except Exception as e:
            logger.error(f"Error adding evidence: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'message': str(e)
            }, status=500)


class ExploitFinderView(View):
    """
    View to find exploits for vulnerabilities
    """
    def get(self, request):
        # Get filter parameters
        vulnerability_id = request.GET.get('vulnerability_id')
        target = request.GET.get('target')
        
        # Get vulnerabilities with matching exploits
        if vulnerability_id:
            vulnerabilities = Vulnerability.objects.filter(id=vulnerability_id)
        elif target:
            vulnerabilities = Vulnerability.objects.filter(target=target, is_fixed=False)
        else:
            # Get top 100 vulnerabilities by severity
            vulnerabilities = Vulnerability.objects.filter(
                is_fixed=False
            ).order_by(
                'severity'
            )[:100]
        
        from exploit_manager.models import ExploitMatch
        
        # Get exploit matches for these vulnerabilities
        vulnerability_ids = vulnerabilities.values_list('id', flat=True)
        matches = ExploitMatch.objects.filter(
            vulnerability_id__in=vulnerability_ids
        ).select_related(
            'vulnerability', 'exploit'
        ).order_by('-confidence_score')
        
        # Group matches by vulnerability
        grouped_matches = {}
        for match in matches:
            vuln_id = match.vulnerability.id
            if vuln_id not in grouped_matches:
                grouped_matches[vuln_id] = {
                    'vulnerability': match.vulnerability,
                    'matches': []
                }
            grouped_matches[vuln_id]['matches'].append(match)
        
        return render(request, 'manual_exploitation/exploit_finder.html', {
            'vulnerabilities': vulnerabilities,
            'grouped_matches': grouped_matches,
            'target_filter': target
        })


class ExploitTemplateView(View):
    """
    View to manage exploitation templates
    """
    def get(self, request, template_id=None):
        if template_id:
            # Get specific template
            template = get_object_or_404(ExploitationTemplate, id=template_id)
            return render(request, 'manual_exploitation/template_detail.html', {
                'template': template
            })
        else:
            # List all templates
            templates = ExploitationTemplate.objects.all()
            return render(request, 'manual_exploitation/template_list.html', {
                'templates': templates
            })
    
    @method_decorator(csrf_exempt)
    def post(self, request, template_id=None):
        try:
            # Parse data
            data = json.loads(request.body) if request.content_type == 'application/json' else request.POST.dict()
            
            if template_id:
                # Update existing template
                template = get_object_or_404(ExploitationTemplate, id=template_id)
                template.name = data.get('name', template.name)
                template.description = data.get('description', template.description)
                template.exploit_type = data.get('exploit_type', template.exploit_type)
                template.template_code = data.get('template_code', template.template_code)
                template.parameters = data.get('parameters', template.parameters)
                template.save()
                
                if request.content_type == 'application/json':
                    return JsonResponse({
                        'status': 'success',
                        'message': 'Template updated successfully'
                    })
                else:
                    messages.success(request, 'Template updated successfully')
                    return redirect('manual_exploitation:template_detail', template_id=template.id)
            else:
                # Create new template
                template = ExploitationTemplate.objects.create(
                    name=data.get('name', 'Untitled Template'),
                    description=data.get('description', ''),
                    exploit_type=data.get('exploit_type', 'custom'),
                    template_code=data.get('template_code', ''),
                    parameters=data.get('parameters', {})
                )
                
                # Add safety controls if specified
                safety_control_ids = data.get('safety_controls', [])
                if safety_control_ids:
                    template.required_safety_controls.set(safety_control_ids)
                
                if request.content_type == 'application/json':
                    return JsonResponse({
                        'status': 'success',
                        'message': 'Template created successfully',
                        'template_id': template.id
                    })
                else:
                    messages.success(request, 'Template created successfully')
                    return redirect('manual_exploitation:template_detail', template_id=template.id)
        
        except Exception as e:
            loggerasgiref==3.8.1
bcrypt==4.2.1
Brotli==1.1.0
certifi==2025.1.31
cffi==1.17.1
charset-normalizer==3.4.1
croniter==6.0.0
cryptography==44.0.1
cssselect2==0.8.0
decorator==5.1.1
defusedxml==0.7.1
Django==5.1.6
django-ratelimit==4.1.0
djangorestframework==3.15.2
djangorestframework_simplejwt==5.4.0
dnspython==2.7.0
docker==7.1.0
fonttools==4.56.0
greenlet==3.1.1
gvm-tools==25.2.0
idna==3.10
Jinja2==3.1.6
lxml==4.9.4
MarkupSafe==3.0.2
mysql-connector-python==9.2.0
paramiko==2.12.0
pdfkit==1.0.0
pillow==11.1.0
py==1.11.0
pycparser==2.22
pydyf==0.11.0
PyJWT==2.10.1
PyNaCl==1.5.0
pyphen==0.17.2
python-dateutil==2.9.0.post0
python-gvm==23.4.2
python-nmap==0.7.1
python-owasp-zap-v2.4==0.0.20
python3-nmap==1.9.1
pytz==2025.1
PyYAML==6.0.2
requests==2.32.3
responses==0.25.6
retry==0.9.2
scapy==2.6.1
simplejson==3.19.3
six==1.17.0
SQLAlchemy==2.0.38
sqlparse==0.5.3
tinycss2==1.4.0
tinyhtml5==2.0.0
typing_extensions==4.12.2
urllib3==2.3.0
weasyprint==64.1
webencodings==0.5.1
zopfli==0.2.3.post1