services:
  zap:
    image: ghcr.io/zaproxy/zaproxy:stable
    container_name: security_platform_zap
    command: zap.sh -daemon -host 0.0.0.0 -port 8080 -config api.addrs.addr.name=.* -config api.addrs.addr.regex=true -config api.key=change_me_please
    ports:
      - "8080:8080"
    networks:
      - security_net
    volumes:
      - ./zap/data:/zap/data
      - ./zap/scripts:/zap/scripts

networks:
  security_net:
    driver: bridgeasgiref==3.8.1
bcrypt==4.2.1
Brotli==1.1.0
certifi==2025.1.31
cffi==1.17.1
charset-normalizer==3.4.1
croniter==6.0.0
cryptography==44.0.1
cssselect2==0.8.0
decorator==5.1.1
defusedxml==0.7.1
Django==5.1.6
django-ratelimit==4.1.0
djangorestframework==3.15.2
djangorestframework_simplejwt==5.4.0
dnspython==2.7.0
docker==7.1.0
fonttools==4.56.0
greenlet==3.1.1
gvm-tools==25.2.0
idna==3.10
Jinja2==3.1.6
lxml==4.9.4
MarkupSafe==3.0.2
mysql-connector-python==9.2.0
paramiko==2.12.0
pdfkit==1.0.0
pillow==11.1.0
py==1.11.0
pycparser==2.22
pydyf==0.11.0
PyJWT==2.10.1
PyNaCl==1.5.0
pyphen==0.17.2
python-dateutil==2.9.0.post0
python-gvm==23.4.2
python-nmap==0.7.1
python-owasp-zap-v2.4==0.0.20
python3-nmap==1.9.1
pytz==2025.1
PyYAML==6.0.2
requests==2.32.3
responses==0.25.6
retry==0.9.2
scapy==2.6.1
simplejson==3.19.3
six==1.17.0
SQLAlchemy==2.0.38
sqlparse==0.5.3
tinycss2==1.4.0
tinyhtml5==2.0.0
typing_extensions==4.12.2
urllib3==2.3.0
weasyprint==64.1
webencodings==0.5.1
zopfli==0.2.3.post1"""
Django settings for security_automation project.

Generated by 'django-admin startproject' using Django 4.2.11.

For more information on this file, see
https://docs.djangoproject.com/en/4.2/topics/settings/

For the full list of settings and their values, see
https://docs.djangoproject.com/en/4.2/ref/settings/
"""

from pathlib import Path

# Build paths inside the project like this: BASE_DIR / 'subdir'.
BASE_DIR = Path(__file__).resolve().parent.parent


# Quick-start development settings - unsuitable for production
# See https://docs.djangoproject.com/en/4.2/howto/deployment/checklist/

# SECURITY WARNING: keep the secret key used in production secret!
SECRET_KEY = 'django-insecure-7u1-7x*e*z3&3d#1f1ktuo#zuu6f($58^=yuz=b4n7(n)6w4kl'

# SECURITY WARNING: don't run with debug turned on in production!
DEBUG = True

ALLOWED_HOSTS = ['localhost', '127.0.0.1']


# Application definition

INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'network_visualization',
    'automation',
    'exploit_manager', 
    'reconnaissance.apps.ReconnaissanceConfig',
    'vulnerability.apps.VulnerabilityConfig',
    'reporting.apps.ReportingConfig',
    'manual_exploitation.apps.ManualExploitationConfig',
  
]

# Update DATABASES in settings.py
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': BASE_DIR / 'db.sqlite3',
        'TIMEOUT': 20,  # Seconds
        'OPTIONS': {
            'timeout': 20,
            'check_same_thread': False,
        }
    }
}

MIDDLEWARE = [
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
    'automation.middleware.AutomationProcessorMiddleware'

]
# OpenVAS Configuration
# In security_automation/settings.py
# Update the OpenVAS configuration section





# Security Settings
SECURE_BROWSER_XSS_FILTER = True
SECURE_CONTENT_TYPE_NOSNIFF = True
X_FRAME_OPTIONS = 'DENY'
SECURE_HSTS_SECONDS = 31536000
SECURE_HSTS_INCLUDE_SUBDOMAINS = True
SECURE_HSTS_PRELOAD = True

# Session Security
SESSION_COOKIE_SECURE = True
CSRF_COOKIE_SECURE = True
SESSION_COOKIE_HTTPONLY = True
CSRF_COOKIE_HTTPONLY = True

# Rate Limiting
RATELIMIT_ENABLE = True
RATELIMIT_USE_CACHE = 'default'
RATELIMIT_DEFAULT_RATES = ['100/h']  # Default rate limit

# Logging Configuration
# Add this to your settings.py



# Logging Configuration

# Logging Configuration
import os
from pathlib import Path

# Build paths inside the project
BASE_DIR = Path(__file__).resolve().parent.parent

# Create logs directory
LOGS_DIR = os.path.join(BASE_DIR, 'logs')
os.makedirs(LOGS_DIR, exist_ok=True)

# ZAP Scanner Configuration
# These settings control the behavior of the OWASP ZAP integration
ZAP_SETTINGS = {
    'API_KEY': 'change_me_please',  # Change this in production
    'HOST': 'localhost',
    'PORT': 8080,
    'TIMEOUT': 300,  # 5 minutes timeout for ZAP operations
    'DEBUG': DEBUG,  # Tie ZAP debug mode to Django's debug setting
    'MAX_RETRIES': 3,  # Maximum number of connection retry attempts
    'RETRY_DELAY': 5,  # Seconds to wait between retries
}

# Logging Configuration
# This defines how the application handles different types of logs
LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'verbose': {
            'format': '[{asctime}] {levelname} {module} {process:d} {thread:d} {message}',
            'style': '{',
            'datefmt': '%Y-%m-%d %H:%M:%S'
        },
        'simple': {
            'format': '[{asctime}] {levelname} {message}',
            'style': '{',
            'datefmt': '%Y-%m-%d %H:%M:%S'
        },
    },
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',
            'formatter': 'simple',
            'level': 'DEBUG' if DEBUG else 'INFO',
        },
        'file': {
            'class': 'logging.FileHandler',
            'filename': os.path.join(LOGS_DIR, 'debug.log'),
            'formatter': 'verbose',
            'level': 'DEBUG',
        },
        'service_file': {
            'class': 'logging.FileHandler',
            'filename': os.path.join(LOGS_DIR, 'services.log'),
            'formatter': 'verbose',
            'level': 'INFO',
        },
        'error_file': {
            'class': 'logging.FileHandler',
            'filename': os.path.join(LOGS_DIR, 'error.log'),
            'formatter': 'verbose',
            'level': 'ERROR',
        },
        'security_file': {
            'class': 'logging.FileHandler',
            'filename': os.path.join(LOGS_DIR, 'security.log'),
            'formatter': 'verbose',
            'level': 'INFO',
        }
    },
    'loggers': {
        'django': {
            'handlers': ['console', 'file'],
            'level': 'INFO',
            'propagate': True,
        },
        'reconnaissance': {
            'handlers': ['console', 'service_file', 'error_file'],
            'level': 'INFO',
            'propagate': True,
        },
        'vulnerability': {
            'handlers': ['console', 'service_file', 'error_file', 'security_file'],
            'level': 'INFO',
            'propagate': True,
        },
        'zap': {
            'handlers': ['console', 'service_file', 'error_file', 'security_file'],
            'level': 'DEBUG' if DEBUG else 'INFO',
            'propagate': True,
        },
        'scanner': {
            'handlers': ['console', 'service_file', 'error_file', 'security_file'],
            'level': 'DEBUG' if DEBUG else 'INFO',
            'propagate': True,
        }
    },
}

ROOT_URLCONF = 'security_automation.urls'

import os

# Build paths inside the project
BASE_DIR = Path(__file__).resolve().parent.parent

TEMPLATES = [
    {
        'BACKEND': 'django.template.backends.django.DjangoTemplates',
        'DIRS': [os.path.join(BASE_DIR, 'templates')],  # Add this line
        'APP_DIRS': True,
        'OPTIONS': {
            'context_processors': [
                'django.template.context_processors.debug',
                'django.template.context_processors.request',
                'django.contrib.auth.context_processors.auth',
                'django.contrib.messages.context_processors.messages',
                'manual_exploitation.context_processors.dict_access_functions',
            ],
        },
    },
]

# Add to settings.py



# Database
# https://docs.djangoproject.com/en/4.2/ref/settings/#databases



# Password validation
# https://docs.djangoproject.com/en/4.2/ref/settings/#auth-password-validators

AUTH_PASSWORD_VALIDATORS = [
    {
        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',
    },
]


# Internationalization
# https://docs.djangoproject.com/en/4.2/topics/i18n/

LANGUAGE_CODE = 'en-us'

TIME_ZONE = 'UTC'

USE_I18N = True

USE_TZ = True

SECRET_KEY = 'p6IOStJ0i0azQfySy0mSCIht7VouYg9RGggv6iDgI4MQUSgADoTcy7SG2Z4kjDboM6Q'  # Remember to use environment variables in production
DEBUG = True 

# Static files (CSS, JavaScript, Images)
# https://docs.djangoproject.com/en/4.2/howto/static-files/

STATIC_URL = 'static/'

# Default primary key field type
# https://docs.djangoproject.com/en/4.2/ref/settings/#default-auto-field

DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'


# ZAP Settings
ZAP_SETTINGS = {
    'API_KEY': 'change_me_please',
    'HOST': 'localhost',
    'PORT': 8080,
    'TIMEOUT': 300,
    'MAX_RETRIES': 3,
    'SPIDER_TIMEOUT': 600,
    'ACTIVE_SCAN_TIMEOUT': 1200
}
# Nuclei Settings
NUCLEI_SETTINGS = {
    'TEMPLATES_DIR': 'nuclei-templates',
    'RESULTS_DIR': 'nuclei-results',
    'DEFAULT_SEVERITY': 'critical,high,medium',
    'RATE_LIMIT': 150,
    'TIMEOUT': 300,
    'GO_PATH': '/usr/local/go/bin',  # Adjust this based on your Go installation
    'BINARY_PATH': str(Path.home() / "go" / "bin" / "nuclei"),  # Default Go installation path
}

# Email Settings
# Email Settings
EMAIL_BACKEND = 'django.core.mail.backends.smtp.EmailBackend'
EMAIL_HOST = 'smtp.gmail.com'  # Changed from example.com to Gmail's SMTP server
EMAIL_PORT = 587
EMAIL_USE_TLS = True
EMAIL_HOST_USER = 'harmeeksingh729@gmail.com' 
EMAIL_HOST_PASSWORD = 'lyel vzou qbuh fvcl'  # App password or regular password
DEFAULT_FROM_EMAIL = 'harmeeksingh729@gmail.com'  # Should match EMAIL_HOST_USER

AUTOMATION_AUTOSTART = True  # Automatically start the processor on application startup
AUTOMATION_PROCESSING_INTERVAL = 60  # Seconds between processing cycles# File: security_automation/urls.py
from django.contrib import admin
from django.urls import path, include

urlpatterns = [
    path('admin/', admin.site.urls),
    path('recon/', include('reconnaissance.urls')),
    path('vulnerability/', include('vulnerability.urls')),
    path('reporting/', include('reporting.urls')),
    path('network/', include('network_visualization.urls')),
    path('automation/', include('automation.urls')),
    path('exploits/', include('exploit_manager.urls')),
    path('exploitation/', include('manual_exploitation.urls')),
# Add this line
]from django.db import models

from django.db import models

class Subdomain(models.Model):
    domain = models.CharField(max_length=255)
    subdomain = models.CharField(max_length=255)
    ip_address = models.GenericIPAddressField(null=True)
    discovered_date = models.DateTimeField(auto_now_add=True)
    is_active = models.BooleanField(default=True)

    class Meta:
        ordering = ['-discovered_date']
        unique_together = ['domain', 'subdomain']
        indexes = [
            models.Index(fields=['domain']),
            models.Index(fields=['subdomain']),
        ]

    def __str__(self):
        return self.subdomain

    def save(self, *args, **kwargs):
        # Update instead of error on duplicate
        try:
            super().save(*args, **kwargs)
        except:
            existing = Subdomain.objects.get(domain=self.domain, subdomain=self.subdomain)
            existing.ip_address = self.ip_address
            existing.is_active = self.is_active
            existing.save()

class Service(models.Model):
    RISK_LEVELS = [
        ('LOW', 'Low'),
        ('MEDIUM', 'Medium'),
        ('HIGH', 'High'),
    ]

    CATEGORIES = [
        ('web', 'Web Services'),
        ('database', 'Database Services'),
        ('mail', 'Mail Services'),
        ('file_transfer', 'File Transfer'),
        ('remote_access', 'Remote Access'),
        ('domain_services', 'Domain Services'),
        ('monitoring', 'Monitoring'),
        ('security', 'Security Services'),
        ('other', 'Other'),
    ]

    PROTOCOLS = [
        ('tcp', 'TCP'),
        ('udp', 'UDP'),
        ('sctp', 'SCTP'),
    ]

    host = models.CharField(max_length=255)
    port = models.IntegerField()
    protocol = models.CharField(max_length=10, choices=PROTOCOLS, default='tcp')
    name = models.CharField(max_length=100)
    product = models.CharField(max_length=100, blank=True)
    version = models.CharField(max_length=100, blank=True)
    extra_info = models.TextField(blank=True)
    category = models.CharField(max_length=50, choices=CATEGORIES)
    risk_level = models.CharField(max_length=10, choices=RISK_LEVELS)
    cpe = models.JSONField(default=list)
    scan_date = models.DateTimeField(auto_now_add=True)
    last_seen = models.DateTimeField(auto_now=True)
    is_active = models.BooleanField(default=True)

    class Meta:
        unique_together = ('host', 'port', 'protocol')
        ordering = ['-scan_date']
        indexes = [
            models.Index(fields=['host', 'port']),
            models.Index(fields=['category']),
            models.Index(fields=['risk_level']),
        ]

    def __str__(self):
        return f"{self.host}:{self.port} - {self.name}"

class PortScan(models.Model):
    STATES = [
        ('pending', 'Pending'),
        ('in_progress', 'In Progress'), 
        ('open', 'Open'),
        ('closed', 'Closed'),
        ('filtered', 'Filtered'),
        ('unfiltered', 'Unfiltered'),
        ('error', 'Error'),
        ('completed', 'Completed')
    ]

    SCAN_STATUS = [
        ('pending', 'Pending'),
        ('in_progress', 'In Progress'),
        ('completed', 'Completed'),
        ('error', 'Error')
    ]

    host = models.CharField(max_length=255)
    port = models.IntegerField()
    service = models.CharField(max_length=100)
    state = models.CharField(max_length=50, choices=STATES)
    scan_status = models.CharField(max_length=50, choices=SCAN_STATUS, default='pending')
    scan_date = models.DateTimeField(auto_now_add=True)
    protocol = models.CharField(max_length=10, choices=Service.PROTOCOLS, default='tcp')
    banner = models.TextField(blank=True)
    notes = models.TextField(blank=True)
    scan_type = models.CharField(max_length=50, default='quick')
    error_message = models.TextField(blank=True)

    class Meta:
        ordering = ['-scan_date']
        indexes = [
            models.Index(fields=['host', 'port']),
            models.Index(fields=['state']),
            models.Index(fields=['scan_date']),
            models.Index(fields=['scan_status'])
        ]

    def __str__(self):
        return f"{self.host}:{self.port} - {self.state}"
class SystemLogEntry(models.Model):
    """Model for system logs admin interface"""
    class Meta:
        managed = False
        verbose_name_plural = 'System Logs'
        default_permissions = ('view',)from django.db import models
from django.core.validators import MinValueValidator, MaxValueValidator
import logging
from django.db import transaction

logger = logging.getLogger(__name__)

class Vulnerability(models.Model):
    SEVERITY_CHOICES = [
        ('LOW', 'Low'),
        ('MEDIUM', 'Medium'),
        ('HIGH', 'High'),
        ('CRITICAL', 'Critical'),
    ]

    # Updated source choices to include more options
    SOURCE_CHOICES = [
        ('internal', 'Internal Scanner'),
        ('zap', 'OWASP ZAP'),
        ('nuclei', 'Nuclei Scanner'),
        ('openvas', 'OpenVAS Scanner'),
        ('manual', 'Manual Entry'),
        ('multiple', 'Multiple Sources')  # For correlated findings
    ]

    # Basic Information
    target = models.CharField(max_length=255, db_index=True)
    name = models.CharField(max_length=255)
    description = models.TextField()
    severity = models.CharField(max_length=10, choices=SEVERITY_CHOICES, db_index=True)
    vuln_type = models.CharField(max_length=50, db_index=True)
    
    # Evidence and Details
    evidence = models.TextField()
    solution = models.TextField(blank=True)
    references = models.JSONField(default=list)
    
    # Source and Confidence - increased max_length to accommodate combined sources
    source = models.CharField(max_length=100)  # Removed choices constraint and increased length
    confidence = models.CharField(max_length=50, default='medium')
    
    # Status and Tracking
    discovery_date = models.DateTimeField(auto_now_add=True)
    is_fixed = models.BooleanField(default=False, db_index=True)
    fix_date = models.DateTimeField(null=True, blank=True)
    notes = models.TextField(blank=True)
    
    # Additional Metadata
    cwe = models.CharField(max_length=50, blank=True)
    cvss_score = models.FloatField(
        null=True, 
        blank=True,
        validators=[MinValueValidator(0.0), MaxValueValidator(10.0)]
    )
    metadata = models.JSONField(default=dict)  # Added metadata field
    
    def __str__(self):
        return f"{self.target} - {self.name} ({self.severity})"

# In the Vulnerability model in vulnerability/models.py

# In the Vulnerability model in vulnerability/models.py

    def save(self, *args, **kwargs):
        # Extract our custom parameter before passing to Django save
        skip_deduplication = kwargs.pop('skip_deduplication', False) if 'skip_deduplication' in kwargs else False
        
        # Normalize target before saving
        self.target = self.__class__.normalize_target(self.target)
        
        # Ensure severity is uppercase
        if self.severity:
            self.severity = self.severity.upper()
        
        # Set fix_date when vulnerability is marked as fixed
        if self.is_fixed and not self.fix_date:
            from django.utils import timezone
            self.fix_date = timezone.now()
        
        # Handle source field for multiple sources
        if ',' in self.source:
            # Store original sources in metadata for reference
            if not self.metadata:
                self.metadata = {}
            self.metadata['original_sources'] = self.source.split(',')
            
        # Call save with clean kwargs
        super().save(*args, **kwargs)
        
        # Skip deduplication if explicitly requested (to avoid infinite recursion)
        # or schedule deduplication to run after save
        if not skip_deduplication:
            # Store target to use in lambda
            target = self.target
            # Use raw lambda to avoid potential reference issues
            transaction.on_commit(lambda t=target: self.__class__.deduplicate_vulnerabilities(t))

    @classmethod
    def normalize_target(cls, target_str):
        """Class method to normalize target URLs"""
        if not target_str:
            return target_str
            
        # Remove protocol prefix
        if '://' in target_str:
            target_str = target_str.split('://', 1)[1]
        
        # Remove path, trailing slash, etc.
        if '/' in target_str:
            target_str = target_str.split('/', 1)[0]
            
        # Remove port if present
        if ':' in target_str:
            target_str = target_str.split(':', 1)[0]
            
        # Remove 'www.' prefix if present
        if target_str.startswith('www.'):
            target_str = target_str[4:]
            
        return target_str.lower()

    @classmethod
    def deduplicate_vulnerabilities(cls, target):
        """
        Deduplicate vulnerabilities for a target by merging duplicates
        
        Args:
            target: The target hostname/domain
            
        Returns:
            dict: Statistics about deduplication
        """
        from django.db.models import Count
        from django.db import transaction
        
        # Use the class method to normalize target
        normalized_target = cls.normalize_target(target)
        
        # Find all targets that might be the same after normalization
        potential_targets = []
        for t in cls.objects.values_list('target', flat=True).distinct():
            if cls.normalize_target(t) == normalized_target:
                potential_targets.append(t)
        
        if not potential_targets:
            return {
                'original_count': 0,
                'merged_count': 0,
                'final_count': 0,
                'message': 'No matching targets found'
            }
        
        # Keep track of statistics
        stats = {
            'original_count': cls.objects.filter(target__in=potential_targets).count(),
            'merged_count': 0,
            'final_count': 0
        }
        
        try:
            with transaction.atomic():
                # First normalize all targets
                for pt in potential_targets:
                    if pt != normalized_target:
                        # Update target to normalized version
                        cls.objects.filter(target=pt).update(target=normalized_target)
                
                # Then deduplicate within the normalized target
                duplicate_groups = cls.objects.filter(target=normalized_target).values(
                    'name', 'vuln_type', 'severity'
                ).annotate(
                    count=Count('id')
                ).filter(count__gt=1)
                
                # Process each group
                for group in duplicate_groups:
                    duplicates = cls.objects.filter(
                        target=normalized_target,
                        name=group['name'],
                        vuln_type=group['vuln_type'],
                        severity=group['severity']
                    ).order_by('discovery_date')
                    
                    # Keep the first (oldest) vulnerability as the canonical one
                    if duplicates.count() > 1:
                        primary_vuln = duplicates.first()
                        
                        # Process other duplicates
                        for dup in duplicates[1:]:
                            # Combine sources if different
                            sources = set(primary_vuln.source.split(','))
                            for source in dup.source.split(','):
                                sources.add(source)
                            primary_vuln.source = ','.join(sorted(sources))
                            
                            # Use the higher CVSS score if available
                            if dup.cvss_score and (not primary_vuln.cvss_score or dup.cvss_score > primary_vuln.cvss_score):
                                primary_vuln.cvss_score = dup.cvss_score
                            
                            # Take the most recent evidence if available
                            if dup.evidence and len(dup.evidence) > len(primary_vuln.evidence):
                                primary_vuln.evidence = dup.evidence
                                
                            # Merge references
                            primary_refs = set(primary_vuln.references)
                            for ref in dup.references:
                                primary_refs.add(ref)
                            primary_vuln.references = list(primary_refs)
                            
                            # Track the merged duplicate in metadata
                            if not primary_vuln.metadata:
                                primary_vuln.metadata = {}
                            if 'merged_duplicates' not in primary_vuln.metadata:
                                primary_vuln.metadata['merged_duplicates'] = []
                            primary_vuln.metadata['merged_duplicates'].append({
                                'id': dup.id,
                                'discovery_date': dup.discovery_date.isoformat(),
                                'source': dup.source
                            })
                            
                            # Delete the duplicate
                            dup.delete()
                            stats['merged_count'] += 1
                        
                        # Save the updated primary vulnerability with skip_deduplication
                        # to avoid infinite recursion
                        primary_vuln.save(skip_deduplication=True)
            
            # Calculate final counts
            stats['final_count'] = cls.objects.filter(target=normalized_target).count()
            return stats
            
        except Exception as e:
            logger.error(f"Error deduplicating vulnerabilities: {str(e)}")
            return {'error': str(e)}    

    @classmethod
    @transaction.atomic
    def remove_duplicates(cls, target=None):
        """
        Remove duplicate vulnerabilities for a target
        
        Args:
            target: Optional target to limit deduplication
            
        Returns:
            int: Number of vulnerabilities removed
        """
        from django.db.models import Count
        
        try:
            # Build query based on if target is provided
            if target:
                # Normalize target first
                normalized_target = cls.normalize_target(target)
                # Find duplicates for specific target
                duplicates = cls.objects.filter(target=normalized_target)
            else:
                # Find duplicates across all targets
                duplicates = cls.objects.all()
            
            # Group by name and target to find duplicates
            duplicate_groups = duplicates.values('name', 'target').annotate(
                count=Count('id')
            ).filter(count__gt=1)
            
            removed_count = 0
            
            # Process each duplicate group
            for group in duplicate_groups:
                # Get all duplicates in this group, ordered by ID (oldest first)
                dupes = cls.objects.filter(
                    name=group['name'],
                    target=group['target']
                ).order_by('id')
                
                # Keep the first one, delete the rest
                keep = dupes.first()
                to_delete = dupes.exclude(id=keep.id)
                
                # Count how many we're deleting
                delete_count = to_delete.count()
                
                # Delete duplicates
                to_delete.delete()
                removed_count += delete_count
            
            return removed_count
            
        except Exception as e:
            logger.error(f"Error removing duplicates: {str(e)}")
            return 0
    
    @property
    def age_in_days(self):
        from django.utils import timezone
        return (timezone.now() - self.discovery_date).days
    
    # Add this method to the Vulnerability model


    @property
    def risk_score(self):
        """Calculate risk score based on CVSS and age"""
        base_score = self.cvss_score if self.cvss_score else {
            'CRITICAL': 9.0,
            'HIGH': 7.0,
            'MEDIUM': 5.0,
            'LOW': 3.0
        }.get(self.severity, 1.0)
        
        # Age factor: 1.0 - 2.0 based on age (caps at 90 days)
        age_factor = min(1 + (self.age_in_days / 90), 2.0)
        
        return base_score * age_factor
    
    @property
    def source_list(self):
        """Return the source as a list for easier filtering"""
        return self.source.split(',')

class NucleiFinding(models.Model):
    SEVERITY_CHOICES = [
        ('CRITICAL', 'Critical'),
        ('HIGH', 'High'),
        ('MEDIUM', 'Medium'),
        ('LOW', 'Low'),
        ('INFO', 'Info'),
    ]

    template_id = models.CharField(max_length=255)
    name = models.CharField(max_length=255)
    severity = models.CharField(max_length=10, choices=SEVERITY_CHOICES)
    finding_type = models.CharField(max_length=50)
    host = models.CharField(max_length=255)
    matched_at = models.URLField(max_length=500)
    description = models.TextField()
    tags = models.JSONField(default=list)
    references = models.JSONField(default=list)
    cwe = models.CharField(max_length=50, blank=True, null=True)
    cvss_score = models.FloatField(null=True, blank=True)
    discovery_date = models.DateTimeField(auto_now_add=True)
    scan_id = models.CharField(max_length=100)
    target = models.CharField(max_length=255)
    
    class Meta:
        indexes = [
            models.Index(fields=['template_id']),
            models.Index(fields=['severity']),
            models.Index(fields=['discovery_date']),
            models.Index(fields=['target']),
        ]# File: exploit_manager/models.py
from django.db import models

class ExploitSource(models.Model):
    """Represents an exploit data source like ExploitDB"""
    name = models.CharField(max_length=100)
    description = models.TextField(blank=True)
    api_url = models.URLField(blank=True)
    last_update = models.DateTimeField(null=True, blank=True)
    is_active = models.BooleanField(default=True)
    
    def __str__(self):
        return self.name

class Exploit(models.Model):
    """Represents an individual exploit"""
    # Core fields
    exploit_id = models.CharField(max_length=50)  # Original ID from source
    title = models.CharField(max_length=255)
    description = models.TextField(blank=True)
    
    # Classification
    type = models.CharField(max_length=100, blank=True)  # webapps, remote, local, etc.
    platform = models.CharField(max_length=100, blank=True)  # Windows, Linux, etc.
    
    # Vulnerability information
    vulnerability_name = models.CharField(max_length=255, blank=True)
    cve_id = models.CharField(max_length=50, blank=True)
    
    # Dates
    date_published = models.DateField(null=True, blank=True)
    date_added = models.DateTimeField(auto_now_add=True)
    date_updated = models.DateTimeField(auto_now=True)
    
    # Source information
    source = models.ForeignKey(ExploitSource, on_delete=models.CASCADE, related_name='exploits')
    source_url = models.URLField(blank=True)
    
    # Code and technical details
    code = models.TextField(blank=True)
    file_path = models.CharField(max_length=255, blank=True)  # If stored as file
    author = models.CharField(max_length=255, blank=True)
    
    # Additional metadata
    verified = models.BooleanField(default=False)
    score = models.FloatField(null=True, blank=True)  # Relevance or quality score
    metadata = models.JSONField(default=dict, blank=True)
    
    class Meta:
        unique_together = ('exploit_id', 'source')
        indexes = [
            models.Index(fields=['cve_id']),
            models.Index(fields=['exploit_id']),
            models.Index(fields=['type']),
        ]
    
    def __str__(self):
        return f"{self.title} ({self.exploit_id})"
    
class ExploitMatch(models.Model):
    """Represents a match between a vulnerability and an exploit"""
    STATUS_CHOICES = [
        ('pending', 'Pending Review'),
        ('confirmed', 'Confirmed Match'),
        ('rejected', 'Rejected Match'),
        ('exploited', 'Successfully Exploited')
    ]
    
    vulnerability = models.ForeignKey('vulnerability.Vulnerability', on_delete=models.CASCADE, related_name='exploit_matches')
    exploit = models.ForeignKey(Exploit, on_delete=models.CASCADE, related_name='vulnerability_matches')
    
    confidence_score = models.FloatField(default=0.5)  # 0.0 to 1.0
    status = models.CharField(max_length=20, choices=STATUS_CHOICES, default='pending')
    
    # Match metadata
    match_reason = models.TextField(blank=True)
    matched_by = models.CharField(max_length=50, blank=True)  # user or system
    match_date = models.DateTimeField(auto_now_add=True)
    last_updated = models.DateTimeField(auto_now=True)
    
    # User feedback
    notes = models.TextField(blank=True)
    
    class Meta:
        unique_together = ('vulnerability', 'exploit')
        indexes = [
            models.Index(fields=['status']),
            models.Index(fields=['confidence_score']),
        ]
    
    def __str__(self):
        return f"Match: {self.vulnerability.name} ↔ {self.exploit.title}"# automation/models.py

from django.db import models
from django.utils import timezone

class ScanWorkflow(models.Model):
    """
    Represents a complete security scanning workflow
    """
    STATUS_CHOICES = [
        ('pending', 'Pending'),
        ('scheduled', 'Scheduled'),
        ('in_progress', 'In Progress'),
        ('completed', 'Completed'),
        ('failed', 'Failed'),
        ('canceled', 'Canceled')
    ]
    
    PROFILE_CHOICES = [
        ('quick', 'Quick Scan'),
        ('standard', 'Standard Scan'),
        ('full', 'Full Scan')
    ]
    
    name = models.CharField(max_length=255)
    target = models.CharField(max_length=255)
    scan_profile = models.CharField(max_length=20, choices=PROFILE_CHOICES, default='standard')
    status = models.CharField(max_length=20, choices=STATUS_CHOICES, default='pending')
    
    # Timing information
    created_at = models.DateTimeField(auto_now_add=True)
    scheduled_time = models.DateTimeField(null=True, blank=True)
    start_time = models.DateTimeField(null=True, blank=True)
    end_time = models.DateTimeField(null=True, blank=True)
    
    # Notification settings
    notification_email = models.EmailField(null=True, blank=True)
    
    # Additional metadata
    metadata = models.JSONField(default=dict, blank=True)
    
    class Meta:
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['status']),
            models.Index(fields=['target']),
            models.Index(fields=['created_at']),
        ]
    
    def __str__(self):
        return f"{self.name} - {self.target} ({self.status})"
    
    @property
    def duration(self):
        """Calculate workflow duration in seconds"""
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return None
    
    @property
    def is_active(self):
        """Check if workflow is active (not in terminal state)"""
        return self.status in ['pending', 'scheduled', 'in_progress']
    
    @property
    def is_scheduled(self):
        """Check if workflow is scheduled for future execution"""
        return self.status == 'scheduled' and self.scheduled_time and self.scheduled_time > timezone.now()


class ScanTask(models.Model):
    """
    Represents an individual task within a scanning workflow
    """
    STATUS_CHOICES = [
        ('pending', 'Pending'),
        ('in_progress', 'In Progress'),
        ('completed', 'Completed'),
        ('failed', 'Failed'),
        ('skipped', 'Skipped'),
        ('canceled', 'Canceled')
    ]
    
    TASK_TYPE_CHOICES = [
        ('subdomain_enumeration', 'Subdomain Enumeration'),
        ('port_scanning', 'Port Scanning'),
        ('service_identification', 'Service Identification'),
        ('vulnerability_scanning', 'Vulnerability Scanning'),
        ('network_mapping', 'Network Mapping'),
        ('report_generation', 'Report Generation')
    ]
    
    workflow = models.ForeignKey(ScanWorkflow, on_delete=models.CASCADE, related_name='tasks')
    task_type = models.CharField(max_length=50, choices=TASK_TYPE_CHOICES)
    name = models.CharField(max_length=255)
    status = models.CharField(max_length=20, choices=STATUS_CHOICES, default='pending')
    
    # Task dependencies
    dependencies = models.ManyToManyField('self', symmetrical=False, related_name='dependents', blank=True)
    order = models.IntegerField(default=0)
    
    # Timing information
    created_at = models.DateTimeField(auto_now_add=True)
    start_time = models.DateTimeField(null=True, blank=True)
    end_time = models.DateTimeField(null=True, blank=True)
    
    # Results
    result = models.TextField(null=True, blank=True)
    
    class Meta:
        ordering = ['workflow', 'order']
        indexes = [
            models.Index(fields=['workflow', 'status']),
            models.Index(fields=['task_type']),
        ]
    
    def __str__(self):
        return f"{self.name} ({self.status})"
    
    @property
    def duration(self):
        """Calculate task duration in seconds"""
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return None
    
    @property
    def has_dependencies(self):
        """Check if task has dependencies"""
        return self.dependencies.exists()
    
    @property
    def is_blocked(self):
        """Check if any dependencies are not completed"""
        return self.dependencies.exclude(status='completed').exists()


class Notification(models.Model):
    """
    Represents a notification sent to a user
    """
    NOTIFICATION_TYPE_CHOICES = [
        ('workflow_scheduled', 'Workflow Scheduled'),
        ('workflow_started', 'Workflow Started'),
        ('workflow_completed', 'Workflow Completed'),
        ('workflow_failed', 'Workflow Failed'),
        ('workflow_canceled', 'Workflow Canceled'),
        ('task_failed', 'Task Failed'),
        ('critical_vulnerabilities', 'Critical Vulnerabilities'),
        ('report_ready', 'Report Ready')
    ]
    
    workflow = models.ForeignKey(ScanWorkflow, on_delete=models.CASCADE, related_name='notifications')
    notification_type = models.CharField(max_length=50, choices=NOTIFICATION_TYPE_CHOICES)
    recipient = models.EmailField()
    subject = models.CharField(max_length=255)
    message = models.TextField()
    
    created_at = models.DateTimeField(auto_now_add=True)
    sent = models.BooleanField(default=False)
    sent_time = models.DateTimeField(null=True, blank=True)
    
    # For tracking email opens, clicks, etc.
    metadata = models.JSONField(default=dict, blank=True)
    
    class Meta:
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['notification_type']),
            models.Index(fields=['created_at']),
            models.Index(fields=['sent']),
        ]
    
    def __str__(self):
        return f"{self.notification_type} for {self.workflow.name}"


class ScheduledTask(models.Model):
    """
    Represents a recurring scheduled task/scan
    """
    FREQUENCY_CHOICES = [
        ('daily', 'Daily'),
        ('weekly', 'Weekly'),
        ('monthly', 'Monthly'),
        ('custom', 'Custom')
    ]
    
    name = models.CharField(max_length=255)
    target = models.CharField(max_length=255)
    scan_profile = models.CharField(max_length=20, choices=ScanWorkflow.PROFILE_CHOICES, default='standard')
    
    # Schedule
    frequency = models.CharField(max_length=20, choices=FREQUENCY_CHOICES)
    cron_expression = models.CharField(max_length=100, null=True, blank=True)
    start_date = models.DateField()
    end_date = models.DateField(null=True, blank=True)
    
    # Active flag
    is_active = models.BooleanField(default=True)
    
    # Notification settings
    notification_email = models.EmailField(null=True, blank=True)
    
    # Created by
    created_at = models.DateTimeField(auto_now_add=True)
    created_by = models.CharField(max_length=100, null=True, blank=True)
    
    # Last execution
    last_execution = models.DateTimeField(null=True, blank=True)
    last_status = models.CharField(max_length=20, null=True, blank=True)
    last_workflow = models.ForeignKey(ScanWorkflow, on_delete=models.SET_NULL, null=True, blank=True, related_name='schedule')
    
    class Meta:
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['is_active']),
            models.Index(fields=['frequency']),
            models.Index(fields=['target']),
        ]
    
    def __str__(self):
        return f"{self.name} - {self.target} ({self.frequency})"from django.db import models

# Create your models here.
class Report(models.Model):
    title = models.CharField(max_length=255)
    creation_date = models.DateTimeField(auto_now_add=True)
    content = models.TextField()
    report_type = models.CharField(max_length=50)from django.db import models

# Create your models here.
from django.db import models
from django.utils import timezone

class ExploitationSession(models.Model):
    """
    Represents a controlled manual exploitation session against a target
    """
    STATUS_CHOICES = [
        ('created', 'Created'),
        ('authorized', 'Authorized'),
        ('in_progress', 'In Progress'),
        ('completed', 'Completed'),
        ('failed', 'Failed'),
        ('aborted', 'Aborted')
    ]
    
    # Session identification
    name = models.CharField(max_length=255)
    target = models.CharField(max_length=255)
    description = models.TextField(blank=True)
    
    # Exploitation details
    vulnerability = models.ForeignKey('vulnerability.Vulnerability', on_delete=models.CASCADE, related_name='exploitation_sessions')
    exploit = models.ForeignKey('exploit_manager.Exploit', on_delete=models.CASCADE, related_name='exploitation_sessions')
    
    # Status tracking
    status = models.CharField(max_length=20, choices=STATUS_CHOICES, default='created')
    created_at = models.DateTimeField(auto_now_add=True)
    started_at = models.DateTimeField(null=True, blank=True)
    completed_at = models.DateTimeField(null=True, blank=True)
    
    # Parameters and customization
    parameters = models.JSONField(default=dict, blank=True)
    customized_code = models.TextField(blank=True)
    
    # Safety controls
    authorized_by = models.CharField(max_length=100, blank=True)
    safety_checks_performed = models.BooleanField(default=False)
    scope_limitations = models.TextField(blank=True)
    max_execution_time = models.IntegerField(default=300)  # in seconds
    
    # Results
    result_summary = models.TextField(blank=True)
    success = models.BooleanField(null=True)
    
    # Metadata
    notes = models.TextField(blank=True)
    tags = models.JSONField(default=list, blank=True)
    
    class Meta:
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['status']),
            models.Index(fields=['target']),
            models.Index(fields=['created_at']),
        ]
    
    def __str__(self):
        return f"{self.name} - {self.target} ({self.status})"
    
    def start_session(self):
        """Start the exploitation session"""
        self.status = 'in_progress'
        self.started_at = timezone.now()
        self.save()
    
    def complete_session(self, success, summary):
        """Complete the exploitation session"""
        self.status = 'completed'
        self.completed_at = timezone.now()
        self.success = success
        self.result_summary = summary
        self.save()
    
    def abort_session(self, reason):
        """Abort the exploitation session"""
        self.status = 'aborted'
        self.completed_at = timezone.now()
        self.success = False
        self.result_summary = f"Aborted: {reason}"
        self.save()


class ExploitationLog(models.Model):
    """
    Log entry for an exploitation session
    """
    LOG_LEVELS = [
        ('info', 'Info'),
        ('warning', 'Warning'),
        ('error', 'Error'),
        ('critical', 'Critical'),
        ('success', 'Success'),
        ('debug', 'Debug')
    ]
    
    session = models.ForeignKey(ExploitationSession, on_delete=models.CASCADE, related_name='logs')
    timestamp = models.DateTimeField(auto_now_add=True)
    level = models.CharField(max_length=10, choices=LOG_LEVELS, default='info')
    message = models.TextField()
    
    class Meta:
        ordering = ['timestamp']
    
    def __str__(self):
        return f"[{self.level.upper()}] {self.timestamp.strftime('%Y-%m-%d %H:%M:%S')} - {self.message[:50]}"


class ExploitationEvidence(models.Model):
    """
    Evidence collected during an exploitation session
    """
    EVIDENCE_TYPES = [
        ('screenshot', 'Screenshot'),
        ('log', 'Log File'),
        ('data', 'Extracted Data'),
        ('traffic', 'Network Traffic'),
        ('response', 'Server Response'),
        ('output', 'Command Output'),
        ('other', 'Other')
    ]
    
    session = models.ForeignKey(ExploitationSession, on_delete=models.CASCADE, related_name='evidence')
    name = models.CharField(max_length=255)
    description = models.TextField(blank=True)
    evidence_type = models.CharField(max_length=20, choices=EVIDENCE_TYPES)
    content = models.TextField()
    metadata = models.JSONField(default=dict, blank=True)
    timestamp = models.DateTimeField(auto_now_add=True)
    
    class Meta:
        ordering = ['-timestamp']
    
    def __str__(self):
        return f"{self.name} ({self.evidence_type})"


class SafetyControl(models.Model):
    """
    Safety control for exploitation
    """
    CONTROL_TYPES = [
        ('network_limit', 'Network Limitation'),
        ('time_limit', 'Time Limitation'),
        ('scope_limit', 'Scope Limitation'),
        ('resource_limit', 'Resource Limitation'),
        ('authentication', 'Authentication Required'),
        ('approval', 'Approval Required'),
        ('other', 'Other Control')
    ]
    
    name = models.CharField(max_length=100)
    description = models.TextField()
    control_type = models.CharField(max_length=20, choices=CONTROL_TYPES)
    is_mandatory = models.BooleanField(default=True)
    configuration = models.JSONField(default=dict, blank=True)
    
    class Meta:
        ordering = ['name']
    
    def __str__(self):
        return f"{self.name} ({self.control_type})"


class ExploitationTemplate(models.Model):
    """
    Template for common exploitation scenarios
    """
    name = models.CharField(max_length=255)
    description = models.TextField()
    exploit_type = models.CharField(max_length=100)
    template_code = models.TextField()
    parameters = models.JSONField(default=dict)
    required_safety_controls = models.ManyToManyField(SafetyControl)
    
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    class Meta:
        ordering = ['name']
    
    def __str__(self):
        return f"{self.name} - {self.exploit_type}"from django.db import models

class NetworkNode(models.Model):
    """Represents a node in the network topology"""
    
    NODE_TYPES = [
        ('host', 'Host'),
        ('subdomain', 'Subdomain'),
        ('service', 'Service'),
        ('gateway', 'Gateway')
    ]
    
    domain = models.CharField(max_length=255, db_index=True)
    name = models.CharField(max_length=255)
    node_type = models.CharField(max_length=50, choices=NODE_TYPES)
    ip_address = models.GenericIPAddressField(null=True, blank=True)
    metadata = models.JSONField(default=dict, blank=True)
    
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    last_seen = models.DateTimeField(auto_now=True)
    is_active = models.BooleanField(default=True)
    
    class Meta:
        unique_together = ['domain', 'name', 'node_type']
        indexes = [
            models.Index(fields=['domain', 'is_active']),
            models.Index(fields=['node_type']),
        ]
    
    def __str__(self):
        return f"{self.name} ({self.node_type})"

class NetworkConnection(models.Model):
    """Represents a connection between two nodes in the network topology"""
    
    CONNECTION_TYPES = [
        ('domain', 'Domain Link'),
        ('service', 'Service Connection'),
        ('subdomain', 'Subdomain Link'),
        ('external', 'External Connection')
    ]
    
    source = models.ForeignKey(NetworkNode, on_delete=models.CASCADE, related_name='outgoing_connections')
    target = models.ForeignKey(NetworkNode, on_delete=models.CASCADE, related_name='incoming_connections')
    connection_type = models.CharField(max_length=50, choices=CONNECTION_TYPES)
    metadata = models.JSONField(default=dict, blank=True)
    
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    is_active = models.BooleanField(default=True)
    
    class Meta:
        unique_together = ['source', 'target', 'connection_type']
        indexes = [
            models.Index(fields=['source', 'is_active']),
            models.Index(fields=['target', 'is_active']),
        ]
    
    def __str__(self):
        return f"{self.source.name} → {self.target.name} ({self.connection_type})"# automation/workflow_orchestrator.py

import logging
import json
import socket
import time
from datetime import datetime, timedelta
from typing import Dict, List
from django.utils import timezone
from django.db import transaction
from django.conf import settings
from urllib.parse import urlparse

from reconnaissance.subdomain_enumerator import SubdomainEnumerator
from reconnaissance.scanner import PortScanner
from reconnaissance.service_identifier import ServiceIdentifier
from vulnerability.unified_scanner import UnifiedVulnerabilityScanner
from network_visualization.topology_mapper import TopologyMapper
from reporting.report_generator import ReportGenerator
from .models import ScanWorkflow, ScanTask, Notification
from .notification_manager import NotificationManager

logger = logging.getLogger(__name__)

class WorkflowOrchestrator:
    """
    Orchestrates the complete scanning workflow including reconnaissance,
    vulnerability scanning, network visualization, and report generation.
    
    Supports automatic scheduling, task dependencies, and failure handling.
    """
    
    # Workflow task types
    TASK_TYPES = {
        'subdomain_enumeration': 'Subdomain Enumeration',
        'port_scanning': 'Port Scanning',
        'service_identification': 'Service Identification',
        'vulnerability_scanning': 'Vulnerability Scanning',
        'exploit_matching': 'Exploit Matching',  # New task type
        'network_mapping': 'Network Mapping',
        'report_generation': 'Report Generation'
    }
    
    # Task dependencies - keys depend on values
    TASK_DEPENDENCIES = {
        'port_scanning': ['subdomain_enumeration'],
        'service_identification': ['port_scanning'],
        'vulnerability_scanning': ['service_identification'],
        'exploit_matching': ['vulnerability_scanning'],  # New dependency
        'network_mapping': ['service_identification'],
        'report_generation': ['vulnerability_scanning', 'network_mapping', 'exploit_matching']  # Updated dependency
    }
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.subdomain_enumerator = SubdomainEnumerator()
        self.port_scanner = PortScanner()
        self.service_identifier = ServiceIdentifier()
        self.vulnerability_scanner = UnifiedVulnerabilityScanner()
        self.topology_mapper = TopologyMapper()
        self.report_generator = ReportGenerator()
        self.notification_manager = NotificationManager()
    
    def parse_target_url(self, url: str) -> str:
        """
        Parse a target URL and extract just the hostname.
        Handles various URL formats including those with protocols, paths, query strings, etc.
        
        Args:
            url: The target URL to parse
            
        Returns:
            str: The clean hostname
        """
        # Handle empty values
        if not url:
            return ""
        
        try:
            # Remove protocol if present by using urlparse
            parsed = urlparse(url)
            
            # If netloc is empty, the URL might not have a protocol
            if not parsed.netloc:
                # Try adding a protocol and parsing again
                parsed = urlparse(f"http://{url}")
            
            # Extract just the hostname (netloc without port)
            hostname = parsed.netloc
            if ':' in hostname:
                hostname = hostname.split(':', 1)[0]
                
            # If still empty, return the original url as a last resort
            if not hostname:
                return url
                
            return hostname
        except Exception as e:
            self.logger.error(f"Error parsing URL {url}: {str(e)}")
            # Return the original URL if parsing fails
            return url
    
    def setup_workflow(self, workflow, target: str, scan_profile: str = 'standard'):
        """
        Set up tasks for an existing workflow
        
        Args:
            workflow: The existing ScanWorkflow object
            target: The domain or IP to scan
            scan_profile: Scan intensity level (quick, standard, full)
            
        Returns:
            Updated workflow
        """
        with transaction.atomic():
            # Create tasks with proper dependencies
            task_ids = {}
            
            # Create all tasks first
            for task_type, task_name in self.TASK_TYPES.items():
                task = ScanTask.objects.create(
                    workflow=workflow,
                    task_type=task_type,
                    name=f"{task_name} - {target}",
                    status='pending',
                    order=list(self.TASK_TYPES.keys()).index(task_type)
                )
                task_ids[task_type] = task.id
            
            # Set dependencies
            for task_type, dependencies in self.TASK_DEPENDENCIES.items():
                task = ScanTask.objects.get(id=task_ids[task_type])
                for dependency in dependencies:
                    dependency_task = ScanTask.objects.get(id=task_ids[dependency])
                    task.dependencies.add(dependency_task)
                task.save()
                
            # If scheduled for the future, create a notification 
            if workflow.scheduled_time and workflow.notification_email:
                Notification.objects.create(
                    workflow=workflow,
                    notification_type='workflow_scheduled',
                    recipient=workflow.notification_email,
                    subject=f"Scan scheduled: {workflow.name}",
                    message=f"A security scan for {target} has been scheduled to start at {workflow.scheduled_time}."
                )
                
            return workflow
        
    def _complete_workflow(self, workflow: ScanWorkflow) -> None:
        """Mark workflow as completed and send notifications"""
        workflow.status = 'completed'
        workflow.end_time = timezone.now()
        workflow.save()
        
        self.logger.info(f"Workflow {workflow.id} for {workflow.target} completed successfully")
        
        # Send completion notification
        if workflow.notification_email:
            self.notification_manager.send_workflow_completion_notification(workflow)
                
    def create_workflow(self, target: str, name: str = None, scan_profile: str = 'standard',
                      scheduled_time: datetime = None, notify_email: str = None) -> ScanWorkflow:
        """
        Create a new scanning workflow for a target
        
        Args:
            target: The domain or IP to scan
            name: Optional name for this workflow
            scan_profile: Scan intensity level (quick, standard, full)
            scheduled_time: When to start the scan (None = immediate)
            notify_email: Email to notify when scan completes
            
        Returns:
            ScanWorkflow object
        """
        if not name:
            name = f"Scan {target} - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
            
        with transaction.atomic():
            # Create the workflow
            workflow = ScanWorkflow.objects.create(
                name=name,
                target=target,
                scan_profile=scan_profile,
                scheduled_time=scheduled_time,
                status='scheduled' if scheduled_time else 'pending',
                notification_email=notify_email
            )
            
            # Create tasks with proper dependencies
            task_ids = {}
            
            # Create all tasks first
            for task_type, task_name in self.TASK_TYPES.items():
                task = ScanTask.objects.create(
                    workflow=workflow,
                    task_type=task_type,
                    name=f"{task_name} - {target}",
                    status='pending',
                    order=list(self.TASK_TYPES.keys()).index(task_type)
                )
                task_ids[task_type] = task.id
            
            # Set dependencies
            for task_type, dependencies in self.TASK_DEPENDENCIES.items():
                task = ScanTask.objects.get(id=task_ids[task_type])
                for dependency in dependencies:
                    dependency_task = ScanTask.objects.get(id=task_ids[dependency])
                    task.dependencies.add(dependency_task)
                task.save()
                
            # If scheduled for the future, create a notification 
            if scheduled_time and notify_email:
                Notification.objects.create(
                    workflow=workflow,
                    notification_type='workflow_scheduled',
                    recipient=notify_email,
                    subject=f"Scan scheduled: {name}",
                    message=f"A security scan for {target} has been scheduled to start at {scheduled_time}."
                )
                
            return workflow
    
    def start_workflow(self, workflow_id: int) -> bool:
        """
        Start a workflow by ID
        
        Args:
            workflow_id: ID of the workflow to start
            
        Returns:
            bool: True if successfully started
        """
        try:
            workflow = ScanWorkflow.objects.get(id=workflow_id)
            
            # Check if it's time to start a scheduled workflow
            if workflow.status == 'scheduled' and workflow.scheduled_time:
                now = timezone.now()
                if now < workflow.scheduled_time:
                    self.logger.info(f"Workflow {workflow_id} is scheduled for {workflow.scheduled_time}, not starting yet")
                    return False
            
            # Update workflow status
            workflow.status = 'in_progress'
            workflow.start_time = timezone.now()
            workflow.save()
            
            self.logger.info(f"Starting workflow {workflow_id} for target {workflow.target}")
            
            # Get tasks with no dependencies (entry points)
            entry_tasks = ScanTask.objects.filter(
                workflow=workflow, 
                dependencies__isnull=True
            ).order_by('order')
            
            # Start entry tasks
            for task in entry_tasks:
                self._execute_task(task)
                
            return True
            
        except ScanWorkflow.DoesNotExist:
            self.logger.error(f"Workflow {workflow_id} not found")
            return False
        except Exception as e:
            self.logger.error(f"Error starting workflow {workflow_id}: {str(e)}")
            return False
    
    def check_pending_workflows(self) -> int:
        """
        Check for scheduled workflows that should be started
        
        Returns:
            int: Number of workflows started
        """
        now = timezone.now()
        
        # Find scheduled workflows that should start now
        scheduled_workflows = ScanWorkflow.objects.filter(
            status='scheduled',
            scheduled_time__lte=now
        )
        
        count = 0
        for workflow in scheduled_workflows:
            if self.start_workflow(workflow.id):
                count += 1
                
        return count
    
    def process_workflow_queue(self) -> int:
        """
        Process the workflow queue - check for tasks that can be started
        
        Returns:
            int: Number of tasks started
        """
        # Find in-progress workflows
        active_workflows = ScanWorkflow.objects.filter(
            status='in_progress'
        )
        
        tasks_started = 0
        
        for workflow in active_workflows:
            # Get pending tasks for this workflow
            pending_tasks = ScanTask.objects.filter(
                workflow=workflow,
                status='pending'
            )
            
            for task in pending_tasks:
                # Check if all dependencies are completed
                dependencies = task.dependencies.all()
                all_completed = all(dep.status == 'completed' for dep in dependencies)
                
                if all_completed:
                    self._execute_task(task)
                    tasks_started += 1
            
            # Check if workflow is complete
            if not ScanTask.objects.filter(workflow=workflow).exclude(status='completed').exists():
                self._complete_workflow(workflow)
                
        return tasks_started
    
    def _execute_task(self, task: ScanTask) -> None:
        """
        Execute a specific workflow task
        
        Args:
            task: The task to execute
        """
        try:
            # Update task status
            task.status = 'in_progress'
            task.start_time = timezone.now()
            task.save()
            
            self.logger.info(f"Executing task {task.id} ({task.task_type}) for workflow {task.workflow.id}")
            
            # Execute appropriate task type
            if task.task_type == 'subdomain_enumeration':
                result = self._run_subdomain_enumeration(task)
            elif task.task_type == 'port_scanning':
                result = self._run_port_scanning(task)
            elif task.task_type == 'service_identification':
                result = self._run_service_identification(task)
            elif task.task_type == 'vulnerability_scanning':
                result = self._run_vulnerability_scanning(task)
            elif task.task_type == 'exploit_matching':  # New task type
                result = self._run_exploit_matching(task)
            elif task.task_type == 'network_mapping':
                result = self._run_network_mapping(task)
            elif task.task_type == 'report_generation':
                result = self._run_report_generation(task)
            else:
                raise ValueError(f"Unknown task type: {task.task_type}")
            
            # Update task status based on result
            if result.get('status') == 'success':
                task.status = 'completed'
                task.result = json.dumps(result)
            else:
                task.status = 'failed'
                task.result = json.dumps({'error': result.get('error', 'Unknown error')})
                
                # Create error notification
                if task.workflow.notification_email:
                    self.notification_manager.send_task_failure_notification(
                        task, result.get('error', 'Unknown error')
                    )
            
            task.end_time = timezone.now()
            task.save()
            
            # Check for critical failures that should stop the workflow
            if task.status == 'failed' and task.task_type in ['subdomain_enumeration', 'port_scanning']:
                self._fail_workflow(task.workflow, f"Critical task {task.task_type} failed")
                
        except Exception as e:
            self.logger.error(f"Error executing task {task.id}: {str(e)}")
            task.status = 'failed'
            task.result = json.dumps({'error': str(e)})
            task.end_time = timezone.now()
            task.save()
            
            # Create error notification
            if task.workflow.notification_email:
                self.notification_manager.send_task_failure_notification(task, str(e))
    
# File: automation/workflow_orchestrator.py
# Update the _run_subdomain_enumeration method to handle subdomain enumeration issues

    def _run_subdomain_enumeration(self, task: ScanTask) -> dict:
        """Run subdomain enumeration task with improved URL handling and reliability"""
        # Get the original target from the task
        original_target = task.workflow.target
        
        # Clean the target URL to get just the domain
        target_url = self.parse_target_url(original_target)
        
        # Remove 'www.' prefix if present for better subdomain enumeration
        if target_url.startswith('www.'):
            search_domain = target_url[4:]  # Remove www. prefix
            self.logger.info(f"Removing www prefix for enumeration, using: {search_domain}")
        else:
            search_domain = target_url
            
        try:
            self.logger.info(f"Starting subdomain enumeration for {search_domain} (original: {original_target})")
            results = self.subdomain_enumerator.enumerate_subdomains(search_domain)
            
            # If no results returned, add the main domain as a fallback
            if not results:
                self.logger.warning(f"No subdomains found for {search_domain}, adding main domain as fallback")
                try:
                    main_ip = socket.gethostbyname(search_domain)
                    results = [{
                        'subdomain': search_domain,
                        'ip_address': main_ip,
                        'is_http': True,
                        'http_status': None,
                        'status': 'active'
                    }]
                except Exception as e:
                    self.logger.error(f"Failed to resolve main domain as fallback: {str(e)}")
                    results = [{
                        'subdomain': search_domain,
                        'ip_address': None,
                        'is_http': None,
                        'http_status': None,
                        'status': 'unknown'
                    }]
            
            # Save results to database to ensure they're available for later steps
            saved_count = 0
            from reconnaissance.models import Subdomain  # Import the model explicitly
            
            for subdomain_data in results:
                # Skip entries without subdomain
                if not subdomain_data.get('subdomain'):
                    continue
                    
                try:
                    sub_obj, created = Subdomain.objects.update_or_create(
                        domain=search_domain,
                        subdomain=subdomain_data['subdomain'],
                        defaults={
                            'ip_address': subdomain_data.get('ip_address'),
                            'is_active': True
                        }
                    )
                    saved_count += 1
                except Exception as save_error:
                    self.logger.error(f"Error saving subdomain: {str(save_error)}")
            
            self.logger.info(f"Saved {saved_count} subdomains to database")
            
            return {
                'status': 'success',
                'target': original_target,  # Return original target for consistency
                'target_domain': search_domain,  # Use the domain without www for lookup
                'subdomains_found': len(results),
                'subdomains': results
            }
        except Exception as e:
            self.logger.error(f"Subdomain enumeration failed: {str(e)}")
            # Create a fallback result with just the main domain
            try:
                # Add the main domain as a subdomain in the database
                from reconnaissance.models import Subdomain  # Import the model explicitly
                
                sub_obj, created = Subdomain.objects.update_or_create(
                    domain=search_domain,
                    subdomain=search_domain,
                    defaults={
                        'ip_address': socket.gethostbyname(search_domain),
                        'is_active': True
                    }
                )
                
                return {
                    'status': 'success',  # Return success to continue workflow
                    'target': original_target,
                    'target_domain': search_domain,
                    'subdomains_found': 1, 
                    'subdomains': [{
                        'subdomain': search_domain,
                        'ip_address': sub_obj.ip_address,
                        'is_http': None,
                        'http_status': None,
                        'status': 'active'
                    }],
                    'warning': f"Error during subdomain scan: {str(e)}. Using main domain only."
                }
            except Exception as fallback_error:
                # Absolute last resort - return minimal data to allow workflow to continue
                return {
                    'status': 'success',  # Return success to continue workflow
                    'target': original_target,
                    'target_domain': search_domain,
                    'subdomains_found': 1,
                    'subdomains': [{
                        'subdomain': search_domain, 
                        'ip_address': None,
                        'is_http': None,
                        'http_status': None,
                        'status': 'unknown'
                    }],
                    'warning': f"Subdomain enumeration failed: {str(e)}. Using main domain as fallback."
                }
    
    def _run_port_scanning(self, task: ScanTask) -> dict:
        """Run port scanning task with improved URL handling and database storage"""
        # Get the original target from the task
        original_target = task.workflow.target
        scan_profile = task.workflow.scan_profile
        
        # Clean the target URL to get just the hostname
        target_url = self.parse_target_url(original_target)
        
        # Map scan profile to scan type
        scan_type = {
            'quick': 'quick',
            'standard': 'partial',
            'full': 'full'
        }.get(scan_profile, 'partial')
        
        self.logger.info(f"Starting port scan for {target_url} (original: {original_target}) with profile: {scan_profile}, type: {scan_type}")
        
        try:
            # Validate target before scanning
            import socket
            try:
                # Try to resolve hostname to ensure it's valid
                socket.gethostbyname(target_url)
            except socket.gaierror:
                self.logger.error(f"Unable to resolve target: {target_url}")
                return {
                    'status': 'error',
                    'error': f"Unable to resolve target: {target_url}. Please check the domain name."
                }
            
            # First run a quick check to see if common ports are open
            # This is more reliable than waiting for nmap
            common_ports = [80, 443, 8080, 8443, 22, 21]
            manual_check_ports = []
            
            for port in common_ports:
                try:
                    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                        sock.settimeout(1)
                        result = sock.connect_ex((target_url, port))
                        if result == 0:
                            manual_check_ports.append(port)
                            self.logger.info(f"Manual port check found open port {port} on {target_url}")
                except Exception as e:
                    self.logger.debug(f"Socket error checking port {port}: {str(e)}")
            
            # Run the actual scan
            results = self.port_scanner.scan(target_url, scan_type)
            
            # Check for success
            if results.get('status') == 'success':
                # Check if the scan found any ports
                ports_found = False
                for host in results.get('results', []):
                    if host.get('ports') and len(host.get('ports', [])) > 0:
                        ports_found = True
                        break
                
                # If we have manual ports but no scan ports, add them to the results
                if not ports_found and manual_check_ports and 'manual_detected' not in results:
                    self.logger.info(f"Adding manually detected ports to scan results: {manual_check_ports}")
                    
                    # Create port data for each manual port
                    manual_ports = []
                    for port in manual_check_ports:
                        service_name = 'https' if port in [443, 8443] else 'http' if port in [80, 8080] else 'unknown'
                        manual_ports.append({
                            'port': port,
                            'state': 'open',
                            'service': service_name,
                            'reason': 'manual check'
                        })
                    
                    # Update the results with our manual ports
                    if len(results.get('results', [])) > 0:
                        results['results'][0]['ports'] = manual_ports
                    else:
                        results['results'] = [{
                            'host': target_url,
                            'state': 'up',
                            'ports': manual_ports
                        }]
                    
                    results['manual_added'] = True
                
                # Save scan results to the database
                from reconnaissance.models import PortScan
                
                # Track which ports we've saved to avoid duplicates
                saved_ports = set()
                saved_count = 0
                
                # Process and save all port results
                for host in results.get('results', []):
                    for port_data in host.get('ports', []):
                        port = port_data.get('port')
                        state = port_data.get('state')
                        service = port_data.get('service', '')
                        
                        # Skip if we already saved this port or if port is invalid
                        if not port or (target_url, port) in saved_ports:
                            continue
                        
                        try:
                            # Create or update port scan record
                            port_scan, created = PortScan.objects.update_or_create(
                                host=target_url,
                                port=port,
                                defaults={
                                    'service': service,
                                    'state': state,
                                    'protocol': 'tcp',
                                    'scan_status': 'completed',
                                    'scan_type': scan_type,
                                    'banner': port_data.get('extrainfo', ''),
                                    'notes': f"Version: {port_data.get('version', 'unknown')}"
                                }
                            )
                            
                            saved_ports.add((target_url, port))
                            saved_count += 1
                            
                            # Check if this port should be flagged as a vulnerability
                            if state == 'open' and service in ['ftp', 'telnet', 'rsh', 'rlogin']:
                                from vulnerability.models import Vulnerability
                                
                                # Create a vulnerability entry for high-risk open ports
                                Vulnerability.objects.get_or_create(
                                    target=target_url,
                                    name=f"Open {service.upper()} Port ({port})",
                                    defaults={
                                        'description': f"Port {port} is open and running {service}, which is potentially insecure.",
                                        'severity': 'HIGH',
                                        'vuln_type': 'open_port',
                                        'evidence': f"Port {port} is open and accessible.",
                                        'source': 'port_scan',
                                        'confidence': 'high',
                                        'cvss_score': 7.5,
                                        'is_fixed': False
                                    }
                                )
                            
                        except Exception as save_error:
                            self.logger.error(f"Error saving port scan result: {str(save_error)}")
                
                self.logger.info(f"Saved {saved_count} port scan results to database")
                
                # Add database save info to results
                results['database_saved'] = {
                    'saved_count': saved_count,
                    'target': target_url
                }
                
                return results
            else:
                error_msg = results.get('error', 'Port scanning failed without specific error')
                
                # If we have manually detected ports, return those instead
                if manual_check_ports:
                    self.logger.info(f"Using manually detected ports after scan error: {manual_check_ports}")
                    
                    # Create port data for each manual port
                    manual_ports = []
                    for port in manual_check_ports:
                        service_name = 'https' if port in [443, 8443] else 'http' if port in [80, 8080] else 'unknown'
                        manual_ports.append({
                            'port': port,
                            'state': 'open',
                            'service': service_name,
                            'reason': 'manual check'
                        })
                    
                    # Save manual results to database
                    from reconnaissance.models import PortScan
                    saved_count = 0
                    
                    for port_data in manual_ports:
                        try:
                            port_scan, created = PortScan.objects.update_or_create(
                                host=target_url,
                                port=port_data['port'],
                                defaults={
                                    'service': port_data['service'],
                                    'state': 'open',
                                    'protocol': 'tcp',
                                    'scan_status': 'completed',
                                    'scan_type': 'manual',
                                    'notes': 'Detected by manual scan'
                                }
                            )
                            saved_count += 1
                        except Exception as save_error:
                            self.logger.error(f"Error saving manual port result: {str(save_error)}")
                    
                    self.logger.info(f"Saved {saved_count} manual port results to database")
                    
                    return {
                        'status': 'success',
                        'scan_info': {
                            'scan_type': 'manual',
                            'command_line': 'Manual port check',
                        },
                        'results': [{
                            'host': target_url,
                            'state': 'up',
                            'ports': manual_ports
                        }],
                        'manual_only': True,
                        'database_saved': {
                            'saved_count': saved_count,
                            'target': target_url
                        }
                    }
                
                self.logger.error(f"Port scanning error: {error_msg}")
                return {
                    'status': 'error',
                    'error': error_msg
                }
        except Exception as e:
            self.logger.error(f"Port scanning failed: {str(e)}")
            return {
                'status': 'error',
                'error': f"Port scanning failed: {str(e)}"
            }
    
    def _run_service_identification(self, task: ScanTask) -> dict:
        """Run service identification task with improved URL handling and database storage"""
        # Get the original target from the task
        original_target = task.workflow.target
        scan_profile = task.workflow.scan_profile
        
        # Clean the target URL to get just the hostname
        target_url = self.parse_target_url(original_target)
        
        # Map scan profile to service ID scan type
        scan_type = {
            'quick': 'quick',
            'standard': 'standard',
            'full': 'standard'  # Use 'standard' for full profile for better reliability
        }.get(scan_profile, 'standard')
        
        # Set time limit based on profile
        time_limit = {
            'quick': 180,    # 3 minutes
            'standard': 300, # 5 minutes
            'full': 600      # 10 minutes 
        }.get(scan_profile, 300)
        
        self.logger.info(f"Starting service identification for {target_url} with type: {scan_type}, timeout: {time_limit}s")
        
        try:
            # First check if we have any port scan results to work with
            port_scan_task = ScanTask.objects.filter(
                workflow=task.workflow,
                task_type='port_scanning',
                status='completed'
            ).first()
            
            if not port_scan_task or not port_scan_task.result:
                self.logger.warning(f"No completed port scan found for service identification")
                
                # Do a quick manual check for common ports
                try:
                    common_ports = [80, 443, 8080, 8443, 22, 21]
                    found_ports = []
                    
                    for port in common_ports:
                        try:
                            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                                sock.settimeout(1)
                                result = sock.connect_ex((target_url, port))
                                if result == 0:
                                    found_ports.append(port)
                        except:
                            pass
                    
                    if found_ports:
                        self.logger.info(f"Manual check found {len(found_ports)} open ports for service identification")
                        # Create simple service details
                        services = []
                        for port in found_ports:
                            service_name = 'https' if port in [443, 8443] else 'http' if port in [80, 8080] else 'unknown'
                            category = 'web' if port in [80, 443, 8080, 8443] else 'other'
                            risk_level = 'LOW' if service_name == 'https' else 'MEDIUM'
                            
                            services.append({
                                'port': port,
                                'protocol': 'tcp',
                                'state': 'open',
                                'service': {
                                    'name': service_name,
                                    'product': '',
                                    'version': '',
                                    'extrainfo': 'Detected by manual scan',
                                },
                                'category': category,
                                'risk_level': risk_level
                            })
                        
                        # Save services to database
                        self._save_services_to_database(target_url, services)
                        
                        return {
                            'status': 'success',
                            'target': original_target,
                            'services': services,
                            'manual_detection': True,
                            'database_saved': True
                        }
                    else:
                        # Return empty results to continue workflow
                        return {
                            'status': 'success',
                            'target': original_target,
                            'services': [],
                            'warning': 'No port scan results available'
                        }
                except Exception as e:
                    self.logger.error(f"Manual port check failed: {str(e)}")
                    # Still return success with empty results to continue workflow
                    return {
                        'status': 'success',
                        'target': original_target,
                        'services': [],
                        'warning': 'No port scan results available'
                    }
            
            # Try to parse port scan results
            try:
                import json
                scan_results = json.loads(port_scan_task.result)
                
                # Check if manually detected ports exist
                if scan_results.get('manual_scan') or scan_results.get('manual_detected') or scan_results.get('manual_only') or scan_results.get('manual_added'):
                    self.logger.info("Using manually detected ports for service identification")
                    
                    services = []
                    for host in scan_results.get('results', []):
                        for port_data in host.get('ports', []):
                            port = port_data.get('port')
                            state = port_data.get('state')
                            service_name = port_data.get('service', 'unknown')
                            
                            if state == 'open':
                                category = 'web' if service_name in ['http', 'https'] else 'other'
                                risk_level = 'LOW' if service_name == 'https' else 'MEDIUM'
                                
                                services.append({
                                    'port': port,
                                    'protocol': 'tcp',
                                    'state': 'open',
                                    'service': {
                                        'name': service_name,
                                        'product': '',
                                        'version': '',
                                        'extrainfo': 'Detected by manual scan',
                                    },
                                    'category': category,
                                    'risk_level': risk_level
                                })
                    
                    # Save services to database
                    self._save_services_to_database(target_url, services)
                    
                    return {
                        'status': 'success',
                        'target': original_target,
                        'services': services,
                        'manual_identification': True,
                        'database_saved': True
                    }
                
                # Regular processing
                if not scan_results.get('results') or not any(host.get('ports') for host in scan_results.get('results', [])):
                    self.logger.warning(f"No open ports found in port scan results")
                    return {
                        'status': 'success',
                        'target': original_target,
                        'services': [],
                        'warning': 'No open ports found for service identification'
                    }
            except Exception as parse_error:
                self.logger.error(f"Error parsing port scan results: {str(parse_error)}")
            
            # Import needed for timeout handling
            import threading
            import time
            import queue
            
            # Create a queue to get results
            result_queue = queue.Queue()
            
            # Define a worker function
            def service_scan_worker():
                try:
                    scan_result = self.service_identifier.identify_services(target_url, scan_type)
                    result_queue.put(scan_result)
                except Exception as e:
                    self.logger.error(f"Service scan worker error: {str(e)}")
                    result_queue.put({
                        'status': 'error',
                        'error': str(e)
                    })
            
            # Start the worker thread
            worker_thread = threading.Thread(target=service_scan_worker)
            worker_thread.daemon = True
            worker_thread.start()
            
            # Wait for result with timeout
            start_time = time.time()
            try:
                result = result_queue.get(timeout=time_limit)
                self.logger.info(f"Service identification completed in {time.time() - start_time:.1f} seconds")
            except queue.Empty:
                self.logger.error(f"Service identification timed out after {time_limit} seconds")
                # Return success with limited info to continue workflow
                return {
                    'status': 'success',
                    'target': original_target,
                    'services': [],
                    'warning': f'Service identification timed out after {time_limit}s'
                }
            
            if result.get('status') == 'success':
                # If target in result doesn't match our original, update it
                if 'target' in result:
                    result['target'] = original_target
                
                # Save services to database
                services = result.get('services', [])
                if services:
                    self._save_services_to_database(target_url, services)
                    result['database_saved'] = True
                
                return result
            else:
                self.logger.error(f"Service identification failed: {result.get('error')}")
                # Even if the scan fails, return success with empty results to continue workflow
                return {
                    'status': 'success',
                    'target': original_target,
                    'services': [],
                    'warning': f"Service identification error: {result.get('error', 'Unknown error')}"
                }
        except Exception as e:
            self.logger.error(f"Service identification failed: {str(e)}")
            # Return success with empty results to continue workflow
            return {
                'status': 'success',
                'target': original_target,
                'services': [],
                'warning': f"Service identification error: {str(e)}"
            }

    def _save_services_to_database(self, target: str, services: List[Dict]) -> int:
        """Save service identification results to database
        
        Args:
            target: The target domain/IP
            services: List of service dictionaries
            
        Returns:
            int: Number of services saved
        """
        if not services:
            return 0
            
        from reconnaissance.models import Service
        from vulnerability.models import Vulnerability
        
        saved_count = 0
        
        # High risk services that should be flagged as vulnerabilities
        high_risk_services = {
            'ftp': 'File Transfer Protocol (FTP)',
            'telnet': 'Telnet Remote Access',
            'rsh': 'Remote Shell (RSH)',
            'rlogin': 'Remote Login (Rlogin)',
            'smb': 'Windows File Sharing (SMB)'
        }
        
        # Medium risk services
        medium_risk_services = {
            'smtp': 'Mail Server (SMTP)',
            'pop3': 'Mail Server (POP3)',
            'vnc': 'VNC Remote Desktop',
            'mysql': 'MySQL Database',
            'mssql': 'Microsoft SQL Server'
        }
        
        for service_data in services:
            try:
                port = service_data.get('port')
                if not port:
                    continue
                    
                # Extract service details
                protocol = service_data.get('protocol', 'tcp')
                state = service_data.get('state', 'open')
                service_info = service_data.get('service', {})
                
                if not service_info:
                    continue
                    
                service_name = service_info.get('name', 'unknown')
                product = service_info.get('product', '')
                version = service_info.get('version', '')
                extra_info = service_info.get('extrainfo', '')
                category = service_data.get('category', 'other')
                risk_level = service_data.get('risk_level', 'MEDIUM')
                
                # Create or update service record
                service_obj, created = Service.objects.update_or_create(
                    host=target,
                    port=port,
                    protocol=protocol,
                    defaults={
                        'name': service_name,
                        'product': product,
                        'version': version,
                        'extra_info': extra_info,
                        'category': category,
                        'risk_level': risk_level,
                        'is_active': True
                    }
                )
                
                saved_count += 1
                
                # Check if this service should be flagged as a vulnerability
                if state == 'open':
                    # For high risk services
                    if service_name.lower() in high_risk_services:
                        service_title = high_risk_services[service_name.lower()]
                        
                        # Create vulnerability entry
                        Vulnerability.objects.get_or_create(
                            target=target,
                            name=f"{service_title} on port {port}",
                            defaults={
                                'description': f"Port {port} is running {service_name}, which is potentially insecure. {product} {version}".strip(),
                                'severity': 'HIGH',
                                'vuln_type': 'insecure_service',
                                'evidence': f"Service detected on port {port}. {extra_info}".strip(),
                                'source': 'service_identification',
                                'confidence': 'high',
                                'cvss_score': 7.5,
                                'is_fixed': False
                            }
                        )
                    
                    # For medium risk services
                    elif service_name.lower() in medium_risk_services:
                        service_title = medium_risk_services[service_name.lower()]
                        
                        # Create vulnerability entry
                        Vulnerability.objects.get_or_create(
                            target=target,
                            name=f"{service_title} on port {port}",
                            defaults={
                                'description': f"Port {port} is running {service_name}, which might pose security risks if not properly configured. {product} {version}".strip(),
                                'severity': 'MEDIUM',
                                'vuln_type': 'potentially_risky_service',
                                'evidence': f"Service detected on port {port}. {extra_info}".strip(),
                                'source': 'service_identification',
                                'confidence': 'medium',
                                'cvss_score': 5.0,
                                'is_fixed': False
                            }
                        )
                        
                    # Flag uncommon open ports    
                    elif port not in [80, 443, 8080, 8443, 22] and port < 1024:
                        # Create vulnerability entry for uncommon open ports
                        Vulnerability.objects.get_or_create(
                            target=target,
                            name=f"Uncommon service on port {port} ({service_name})",
                            defaults={
                                'description': f"Port {port} is open and running {service_name}, which is uncommon and might indicate unnecessary services.",
                                'severity': 'LOW',
                                'vuln_type': 'uncommon_port',
                                'evidence': f"Service {service_name} detected on port {port}.",
                                'source': 'service_identification',
                                'confidence': 'medium',
                                'cvss_score': 3.0,
                                'is_fixed': False
                            }
                        )
                    
            except Exception as e:
                self.logger.error(f"Error saving service to database: {str(e)}")
        
        return saved_count
    
    def _run_vulnerability_scanning(self, task: ScanTask) -> dict:
        """Run vulnerability scanning task with improved URL handling"""
        # Get the original target from the task
        original_target = task.workflow.target
        scan_profile = task.workflow.scan_profile
        
        # Clean the target URL to get just the hostname
        target_url = self.parse_target_url(original_target)
        
        # Determine scanners to use based on scan profile
        include_zap = scan_profile in ['standard', 'full']
        include_nuclei = True  # Always use Nuclei
        nuclei_scan_type = 'advanced' if scan_profile == 'full' else 'basic'
        
        try:
            self.logger.info(f"Starting vulnerability scan for {target_url} (original: {original_target})")
            
            results = self.vulnerability_scanner.scan_target(
                target=target_url,
                scan_type=scan_profile,
                include_zap=include_zap,
                include_nuclei=include_nuclei,
                nuclei_scan_type=nuclei_scan_type,
                use_advanced_correlation=True
            )
            
            if results.get('status') == 'success':
                # Check for critical vulnerabilities
                high_vulns = 0
                critical_vulns = 0
                
                for vuln in results.get('vulnerabilities', []):
                    if vuln.get('severity') == 'CRITICAL':
                        critical_vulns += 1
                    elif vuln.get('severity') == 'HIGH':
                        high_vulns += 1
                
                # Add notification for critical vulnerabilities
                if (critical_vulns > 0 or high_vulns > 2) and task.workflow.notification_email:
                    self.notification_manager.send_critical_vulnerability_notification(
                        task.workflow, critical_vulns, high_vulns
                    )
                
                return results
            else:
                return {
                    'status': 'error',
                    'error': results.get('error', 'Vulnerability scanning failed without specific error')
                }
        except Exception as e:
            self.logger.error(f"Vulnerability scanning failed: {str(e)}")
            return {
                'status': 'error', 
                'error': f"Vulnerability scanning failed: {str(e)}"
            }
    
    def _run_network_mapping(self, task: ScanTask) -> dict:
        """Run network mapping task with improved visualization data"""
        # Get the original target from the task
        original_target = task.workflow.target
        
        # Clean the target URL to get just the hostname
        target_url = self.parse_target_url(original_target)
        
        try:
            self.logger.info(f"Starting network mapping for {target_url} (original: {original_target})")
            
            # Get services from previous tasks to include in the network map
            services_data = []
            try:
                service_task = ScanTask.objects.filter(
                    workflow=task.workflow,
                    task_type='service_identification',
                    status='completed'
                ).first()
                
                if service_task and service_task.result:
                    service_result = json.loads(service_task.result)
                    services_data = service_result.get('services', [])
            except Exception as e:
                self.logger.warning(f"Error fetching service data for network mapping: {str(e)}")
            
            # Get subdomains from previous tasks
            subdomains_data = []
            try:
                subdomain_task = ScanTask.objects.filter(
                    workflow=task.workflow,
                    task_type='subdomain_enumeration',
                    status='completed'
                ).first()
                
                if subdomain_task and subdomain_task.result:
                    subdomain_result = json.loads(subdomain_task.result)
                    subdomains_data = subdomain_result.get('subdomains', [])
            except Exception as e:
                self.logger.warning(f"Error fetching subdomain data for network mapping: {str(e)}")
            
            # Create network map
            results = self.topology_mapper.create_network_map(
                target_url, 
                services=services_data,
                subdomains=subdomains_data
            )
            
            # Get the number of nodes and connections for proper report display
            nodes_count = 0
            connections_count = 0
            
            if results.get('status') == 'success':
                # Try to get network node counts from the database
                from network_visualization.models import NetworkNode, NetworkConnection
                
                try:
                    nodes_count = NetworkNode.objects.filter(domain=target_url, is_active=True).count()
                    connections_count = NetworkConnection.objects.filter(
                        source__domain=target_url,
                        is_active=True
                    ).count()
                    
                    self.logger.info(f"Network map created with {nodes_count} nodes and {connections_count} connections")
                except Exception as db_error:
                    self.logger.error(f"Error counting network nodes from database: {str(db_error)}")
                
                # Add the node and connection counts to the result
                results['nodes'] = nodes_count
                results['connections'] = connections_count
                
                return results
            else:
                return {
                    'status': 'error',
                    'error': results.get('error', 'Network mapping failed without specific error'),
                    'nodes': 0,
                    'connections': 0
                }
        except Exception as e:
            self.logger.error(f"Network mapping failed: {str(e)}")
            return {
                'status': 'error',
                'error': f"Network mapping failed: {str(e)}",
                'nodes': 0,
                'connections': 0
            }
    
    def _run_exploit_matching(self, task: ScanTask) -> dict:
        """Run exploit matching for vulnerabilities identified in the target"""
        # Get the original target from the task
        original_target = task.workflow.target
        
        # Clean the target URL to get just the hostname
        target_url = self.parse_target_url(original_target)
        
        try:
            self.logger.info(f"Starting exploit matching for vulnerabilities in {target_url}")
            
            # Create a new matcher instance
            from exploit_manager.matcher import ExploitMatcher
            matcher = ExploitMatcher()
            
            # Get vulnerabilities for this target
            from vulnerability.models import Vulnerability
            vulnerabilities = Vulnerability.objects.filter(target=target_url, is_fixed=False)
            
            total_vulns = vulnerabilities.count()
            matched_vulns = 0
            total_matches = 0
            match_details = []
            
            # Match each vulnerability with potential exploits
            for vuln in vulnerabilities:
                matches = matcher.match_vulnerability(vuln)
                if matches:
                    matched_vulns += 1
                    total_matches += len(matches)
                    
                    # Add top match to details
                    if matches:
                        top_match = matches[0]  # Assuming matches are sorted by confidence
                        match_details.append({
                            'vulnerability_id': vuln.id,
                            'vulnerability_name': vuln.name,
                            'exploit_title': top_match.exploit.title,
                            'exploit_id': top_match.exploit.exploit_id,
                            'confidence': top_match.confidence_score
                        })
            
            # Create result data
            result = {
                'status': 'success',
                'target': original_target,
                'total_vulnerabilities': total_vulns,
                'vulnerabilities_with_matches': matched_vulns,
                'total_matches': total_matches,
                'match_details': match_details[:5]  # Include top 5 matches in the result
            }
            
            self.logger.info(f"Exploit matching completed: found {total_matches} exploits for {matched_vulns} vulnerabilities")
            return result
            
        except Exception as e:
            self.logger.error(f"Exploit matching failed: {str(e)}")
            return {
                'status': 'error',
                'error': f"Exploit matching failed: {str(e)}"
            }

    
# File: automation/workflow_orchestrator.py
    def _run_report_generation(self, task: ScanTask) -> dict:
        """Run report generation task with improved URL handling and exploit match data"""
        # Get the original target from the task
        original_target = task.workflow.target
        
        # Clean the target URL for DB lookups if needed
        target_url = self.parse_target_url(original_target)
        
        scan_profile = task.workflow.scan_profile
        
        # Map scan profile to report type
        report_type = {
            'quick': 'basic',
            'standard': 'detailed',
            'full': 'executive'
        }.get(scan_profile, 'detailed')
        
        try:
            # Get vulnerability scanning task result
            vuln_scan_task = ScanTask.objects.filter(
                workflow=task.workflow,
                task_type='vulnerability_scanning',
                status='completed'
            ).first()
            
            # Get exploit matching task result
            exploit_match_task = ScanTask.objects.filter(
                workflow=task.workflow,
                task_type='exploit_matching',
                status='completed'
            ).first()
            
            # Parse scan results if available
            scan_results = {'workflow_id': task.workflow.id}
            
            # Initialize variables to track findings
            vuln_count = 0
            processed_vulns = []
            
            if vuln_scan_task and vuln_scan_task.result:
                try:
                    vuln_results = json.loads(vuln_scan_task.result)
                    
                    # Extract the vulnerabilities directly
                    if 'vulnerabilities' in vuln_results:
                        vuln_list = vuln_results['vulnerabilities']
                        vuln_count = len(vuln_list)
                        self.logger.info(f"Found {vuln_count} vulnerabilities in scan results")
                        
                        # Explicitly process and save each vulnerability to ensure they're in the database
                        from vulnerability.models import Vulnerability
                        
                        for vuln_data in vuln_list:
                            try:
                                # Normalize the target
                                vuln_target = target_url
                                
                                # Extract and normalize the severity
                                severity = vuln_data.get('severity', 'MEDIUM')
                                if isinstance(severity, str):
                                    severity = severity.upper()
                                
                                # Create or update the vulnerability in the database
                                vuln, created = Vulnerability.objects.update_or_create(
                                    target=vuln_target,
                                    name=vuln_data.get('name', 'Unknown vulnerability'),
                                    defaults={
                                        'description': vuln_data.get('description', ''),
                                        'severity': severity,
                                        'vuln_type': vuln_data.get('type', vuln_data.get('vuln_type', 'unknown')),
                                        'evidence': vuln_data.get('evidence', ''),
                                        'source': vuln_data.get('source', 'scan'),
                                        'confidence': vuln_data.get('confidence', 'medium'),
                                        'cvss_score': vuln_data.get('cvss_score', 0.0),
                                        'is_fixed': False
                                    }
                                )
                                
                                status = 'created' if created else 'updated'
                                self.logger.info(f"Vulnerability {status} in database: {vuln.name} ({vuln.severity})")
                                processed_vulns.append(vuln)
                                
                            except Exception as ve:
                                self.logger.error(f"Error saving vulnerability: {str(ve)}")
                    
                    # Merge other relevant data into scan_results
                    for key, value in vuln_results.items():
                        if key not in ['status', 'vulnerabilities']:
                            scan_results[key] = value
                    
                    self.logger.info(f"Processed vulnerability scan results for report generation")
                except Exception as parse_error:
                    self.logger.error(f"Failed to parse vulnerability scan results: {str(parse_error)}")
            
            # Parse exploit matching results if available
            if exploit_match_task and exploit_match_task.result:
                try:
                    exploit_results = json.loads(exploit_match_task.result)
                    if exploit_results.get('status') == 'success':
                        # Add exploit matching data to scan results
                        scan_results['exploit_matching'] = {
                            'total_vulnerabilities': exploit_results.get('total_vulnerabilities', 0),
                            'vulnerabilities_with_matches': exploit_results.get('vulnerabilities_with_matches', 0),
                            'total_matches': exploit_results.get('total_matches', 0),
                            'match_details': exploit_results.get('match_details', [])
                        }
                        self.logger.info(f"Added exploit matching data to report: {exploit_results.get('total_matches', 0)} matches")
                    else:
                        self.logger.warning(f"Exploit matching task didn't complete successfully: {exploit_results.get('error', 'Unknown error')}")
                except Exception as parse_error:
                    self.logger.error(f"Failed to parse exploit matching results: {str(parse_error)}")
            else:
                self.logger.warning("No completed exploit matching task found for report generation")
                
                # If no exploit match task, try to get data from database directly
                try:
                    from exploit_manager.models import ExploitMatch
                    from vulnerability.models import Vulnerability
                    
                    vulnerabilities = Vulnerability.objects.filter(target=target_url, is_fixed=False)
                    
                    # Count total matches
                    total_matches = ExploitMatch.objects.filter(
                        vulnerability__target=target_url,
                        vulnerability__is_fixed=False
                    ).count()
                    
                    # Get vulnerabilities with matches
                    vulns_with_matches = vulnerabilities.filter(
                        exploit_matches__isnull=False
                    ).distinct().count()
                    
                    # Get top matches by confidence
                    top_matches = ExploitMatch.objects.filter(
                        vulnerability__target=target_url,
                        vulnerability__is_fixed=False
                    ).order_by('-confidence_score')[:5]
                    
                    match_details = []
                    for match in top_matches:
                        match_details.append({
                            'vulnerability_id': match.vulnerability.id,
                            'vulnerability_name': match.vulnerability.name,
                            'exploit_title': match.exploit.title,
                            'exploit_id': match.exploit.exploit_id,
                            'id': match.exploit.id,  # Add database ID for URL construction
                            'confidence': match.confidence_score,
                            'cve_id': match.exploit.cve_id or "None"
                        })
                    
                    # Add to scan results
                    scan_results['exploit_matching'] = {
                        'total_vulnerabilities': vulnerabilities.count(),
                        'vulnerabilities_with_matches': vulns_with_matches,
                        'total_matches': total_matches,
                        'match_details': match_details
                    }
                    
                    self.logger.info(f"Added exploit match data from database: {total_matches} matches")
                except Exception as db_error:
                    self.logger.error(f"Failed to get exploit matches from database: {str(db_error)}")
            
            # Double-check the vulnerability counts in the database
            from vulnerability.models import Vulnerability
            db_vulns = Vulnerability.objects.filter(target=target_url, is_fixed=False)
            db_vuln_count = db_vulns.count()
            
            self.logger.info(f"Vulnerabilities in database for {target_url}: {db_vuln_count}")
            
            # If we don't have vulnerability data in scan_results yet, add it from the database
            if 'vulnerabilities' not in scan_results or not scan_results['vulnerabilities']:
                scan_results['vulnerabilities'] = []
                for vuln in db_vulns:
                    scan_results['vulnerabilities'].append({
                        'id': vuln.id,
                        'name': vuln.name,
                        'description': vuln.description,
                        'severity': vuln.severity,
                        'type': vuln.vuln_type,
                        'evidence': vuln.evidence,
                        'source': vuln.source,
                        'confidence': vuln.confidence,
                        'cvss_score': vuln.cvss_score,
                    })
            
            # Generate different report formats
            self.logger.info(f"Generating {report_type} report for {original_target} with {db_vuln_count} vulnerabilities")
            
            # Generate HTML report
            report_html = self.report_generator.generate_report(report_type, original_target, 'html', scan_results)
            
            # Generate PDF report as well
            try:
                report_pdf = self.report_generator.generate_report(report_type, original_target, 'pdf', scan_results)
                pdf_id = report_pdf.id
            except Exception as pdf_error:
                self.logger.error(f"Error generating PDF report: {str(pdf_error)}")
                pdf_id = None
            
            # Only send a single notification email
            if task.workflow.notification_email:
                self.notification_manager.send_workflow_completion_notification(
                    task.workflow, 
                    report_id=report_html.id
                )
            
            return {
                'status': 'success',
                'target': original_target,
                'workflow_id': task.workflow.id,
                'report_types': [report_type],
                'report_formats': ['html', 'pdf'] if pdf_id else ['html'],
                'report_ids': {
                    'html': report_html.id,
                    'pdf': pdf_id
                },
                'vulnerability_count': db_vuln_count  # Include the count for verification
            }
        except Exception as e:
            self.logger.error(f"Report generation failed: {str(e)}")
            return {
                'status': 'error',
                'error': f"Report generation failed: {str(e)}"
            }
    
    def _fail_workflow(self, workflow: ScanWorkflow, reason: str) -> None:
        """Mark workflow as failed and send notifications"""
        workflow.status = 'failed'
        workflow.end_time = timezone.now()
        workflow.save()
        
        logger.error(f"Workflow {workflow.id} for {workflow.target} failed: {reason}")
        
        # Update pending tasks to skipped
        ScanTask.objects.filter(workflow=workflow, status='pending').update(
            status='skipped',
            result=json.dumps({'skipped_reason': reason})
        )
        
        # Send failure notification
        if workflow.notification_email:
            self.notification_manager.send_workflow_failure_notification(workflow, reason)
    
    def cancel_workflow(self, workflow_id: int) -> bool:
        """
        Cancel a running or scheduled workflow
        
        Args:
            workflow_id: ID of the workflow to cancel
            
        Returns:
            bool: True if successfully canceled
        """
        try:
            workflow = ScanWorkflow.objects.get(id=workflow_id)
            
            if workflow.status in ['completed', 'failed', 'canceled']:
                logger.warning(f"Workflow {workflow_id} already in terminal state: {workflow.status}")
                return False
            
            # Update workflow status
            original_status = workflow.status
            workflow.status = 'canceled'
            workflow.end_time = timezone.now()
            workflow.save()
            
            # Update in-progress tasks to canceled
            ScanTask.objects.filter(workflow=workflow, status='in_progress').update(
                status='canceled',
                end_time=timezone.now(),
                result=json.dumps({'canceled_reason': 'Workflow canceled by user'})
            )
            
            # Update pending tasks to skipped
            ScanTask.objects.filter(workflow=workflow, status='pending').update(
                status='skipped',
                result=json.dumps({'skipped_reason': 'Workflow canceled by user'})
            )
            
            logger.info(f"Workflow {workflow_id} canceled (was {original_status})")
            
            # Send cancellation notification
            if workflow.notification_email:
                self.notification_manager.send_workflow_cancellation_notification(workflow)
                
            return True
            
        except ScanWorkflow.DoesNotExist:
            logger.error(f"Workflow {workflow_id} not found")
            return False
        except Exception as e:
            logger.error(f"Error canceling workflow {workflow_id}: {str(e)}")
            return False
    
    def get_workflow_status(self, workflow_id: int) -> dict:
        """
        Get detailed status of a workflow
        
        Args:
            workflow_id: ID of the workflow
            
        Returns:
            dict: Workflow status details
        """
        try:
            workflow = ScanWorkflow.objects.get(id=workflow_id)
            tasks = ScanTask.objects.filter(workflow=workflow).order_by('order')
            
            # Calculate progress percentage
            total_tasks = tasks.count()
            completed_tasks = tasks.filter(status__in=['completed', 'skipped', 'canceled']).count()
            progress = int(completed_tasks / total_tasks * 100) if total_tasks > 0 else 0
            
            # Format task results
            task_results = []
            for task in tasks:
                result_data = {}
                if task.result:
                    try:
                        result_data = json.loads(task.result)
                    except:
                        result_data = {'error': 'Invalid JSON result'}
                
                task_results.append({
                    'id': task.id,
                    'name': task.name,
                    'type': task.task_type,
                    'status': task.status,
                    'start_time': task.start_time.isoformat() if task.start_time else None,
                    'end_time': task.end_time.isoformat() if task.end_time else None,
                    'duration': str(task.end_time - task.start_time) if task.start_time and task.end_time else None,
                    'result_summary': self._summarize_task_result(task.task_type, result_data)
                })
            
            return {
                'id': workflow.id,
                'name': workflow.name,
                'target': workflow.target,
                'status': workflow.status,
                'scan_profile': workflow.scan_profile,
                'scheduled_time': workflow.scheduled_time.isoformat() if workflow.scheduled_time else None,
                'start_time': workflow.start_time.isoformat() if workflow.start_time else None,
                'end_time': workflow.end_time.isoformat() if workflow.end_time else None,
                'duration': str(workflow.end_time - workflow.start_time) if workflow.start_time and workflow.end_time else None,
                'progress': progress,
                'tasks': task_results,
                'notification_email': workflow.notification_email
            }
            
        except ScanWorkflow.DoesNotExist:
            logger.error(f"Workflow {workflow_id} not found")
            return {'error': 'Workflow not found'}
        except Exception as e:
            logger.error(f"Error getting workflow status: {str(e)}")
            return {'error': str(e)}
    
    def _summarize_task_result(self, task_type: str, result: dict) -> dict:
        """Generate a summary of task results for display"""
        summary = {}
        
        if task_type == 'subdomain_enumeration':
            summary['subdomains_found'] = result.get('subdomains_found', 0)
        elif task_type == 'port_scanning':
            hosts = result.get('results', [])
            open_ports = 0
            for host in hosts:
                open_ports += len([p for p in host.get('ports', []) if p.get('state') == 'open'])
            summary['hosts_scanned'] = len(hosts)
            summary['open_ports'] = open_ports
        elif task_type == 'service_identification':
            summary['services_found'] = len(result.get('services', []))
        elif task_type == 'vulnerability_scanning':
            vulns = result.get('vulnerabilities', [])
            severity_counts = {
                'critical': len([v for v in vulns if v.get('severity') == 'CRITICAL']),
                'high': len([v for v in vulns if v.get('severity') == 'HIGH']),
                'medium': len([v for v in vulns if v.get('severity') == 'MEDIUM']),
                'low': len([v for v in vulns if v.get('severity') == 'LOW'])
            }
            summary['vulnerabilities_found'] = len(vulns)
            summary['severity_counts'] = severity_counts
        elif task_type == 'network_mapping':
            summary['nodes'] = result.get('nodes', 0)
            summary['connections'] = result.get('connections', 0)
        elif task_type == 'report_generation':
            summary['report_types'] = result.get('report_types', [])
            summary['report_formats'] = result.get('report_formats', [])
            summary['report_ids'] = result.get('report_ids', {})
            
        return summaryimport dns.resolver
import dns.zone
import socket
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
from typing import List, Dict
import requests
import re

class SubdomainEnumerator:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.resolver = dns.resolver.Resolver()
        self.resolver.timeout = 1
        self.resolver.lifetime = 1
        
        # Common subdomain prefixes
        self.common_subdomains = [
            'www', 'mail', 'ftp', 'smtp', 'pop', 'ns1', 'ns2', 'dns1', 'dns2',
            'webmail', 'admin', 'secure', 'vpn', 'remote', 'test', 'dev', 'host',
            'support', 'api', 'dev', 'staging', 'app', 'portal', 'beta'
        ]

# File: reconnaissance/subdomain_enumerator.py
# Updates needed in enumerate_subdomains method

    def enumerate_subdomains(self, target: str) -> List[Dict]:
        """Main subdomain enumeration method combining multiple techniques"""
        # Clean target - remove protocol and path to get just the domain
        domain = self._extract_domain(target)
        if not domain:
            self.logger.error(f"Invalid domain provided: {target}")
            return []
            
        self.logger.info(f"Starting subdomain enumeration for domain: {domain}")
        
        # Add a basic check to ensure the domain is valid
        try:
            socket.gethostbyname(domain)
        except socket.gaierror:
            # Add the domain itself as a subdomain if we can't resolve it
            # This allows the workflow to continue
            self.logger.warning(f"Domain {domain} could not be resolved, but continuing enumeration")
        
        discovered_subdomains = set()
        results = []

        # Always add the domain itself to results
        try:
            main_domain_ip = socket.gethostbyname(domain)
            discovered_subdomains.add(domain)
            results.append({
                'subdomain': domain,
                'ip_address': main_domain_ip,
                'is_http': True,  # Assume main domain has HTTP
                'http_status': None,
                'status': 'active'
            })
        except Exception as e:
            self.logger.warning(f"Couldn't resolve main domain {domain}: {str(e)}")
        
        # 1. DNS enumeration
        dns_results = self._dns_enumeration(domain)
        for subdomain in dns_results:
            discovered_subdomains.add(subdomain)

        # 2. Brute force common subdomains
        brute_results = self._brute_force_subdomains(domain)
        for subdomain in brute_results:
            discovered_subdomains.add(subdomain)

        # Process and validate all discovered subdomains
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_subdomain = {
                executor.submit(self._validate_subdomain, subdomain): subdomain 
                for subdomain in discovered_subdomains
            }
            
            for future in as_completed(future_to_subdomain):
                subdomain = future_to_subdomain[future]
                try:
                    result = future.result()
                    if result:
                        results.append(result)
                except Exception as e:
                    self.logger.error(f"Error validating {subdomain}: {str(e)}")

        # Ensure we have at least the main domain in results
        if not results and domain:
            results.append({
                'subdomain': domain,
                'ip_address': None,
                'is_http': None,
                'http_status': None,
                'status': 'unknown'
            })

        return results

    def _extract_domain(self, url: str) -> str:
        """Extract root domain from a URL or domain string"""
        # Remove protocol if present
        if '://' in url:
            url = url.split('://', 1)[1]
            
        # Remove path, query params, and fragment
        url = url.split('/', 1)[0]
        url = url.split('?', 1)[0]
        url = url.split('#', 1)[0]
        
        # Remove port if present
        url = url.split(':', 1)[0]
        
        # Validate domain format
        domain_pattern = r'^([a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]{2,}$'
        if re.match(domain_pattern, url):
            return url
        
        return None

    def _dns_enumeration(self, domain: str) -> set:
        """Enumerate subdomains using DNS queries"""
        discovered = set()
        
        try:
            # Try zone transfer first
            try:
                ns_records = self.resolver.resolve(domain, 'NS')
                for ns in ns_records:
                    try:
                        zone = dns.zone.from_xfr(dns.query.xfr(str(ns), domain))
                        for name, _ in zone.nodes.items():
                            subdomain = str(name) + '.' + domain
                            discovered.add(subdomain)
                    except Exception as zone_error:
                        # Zone transfers often fail due to security restrictions, this is expected
                        continue
            except dns.resolver.NoAnswer:
                self.logger.info(f"No NS records found for {domain} - this is normal for many domains")
            except dns.resolver.NXDOMAIN:
                self.logger.info(f"Domain {domain} does not exist in DNS")
            except Exception as e:
                self.logger.info(f"NS record query failed for {domain}: {str(e)}")

            # Try to get common DNS records
            for record_type in ['A', 'AAAA', 'CNAME', 'MX', 'NS', 'TXT']:
                try:
                    answers = self.resolver.resolve(domain, record_type)
                    for rdata in answers:
                        if record_type == 'MX':
                            discovered.add(str(rdata.exchange).rstrip('.'))
                        elif record_type == 'NS':
                            discovered.add(str(rdata).rstrip('.'))
                        elif record_type == 'CNAME':
                            discovered.add(str(rdata.target).rstrip('.'))
                except dns.resolver.NoAnswer:
                    # This is normal - not all record types exist for all domains
                    continue
                except Exception:
                    # Other DNS errors are also common and shouldn't stop enumeration
                    continue

        except Exception as e:
            # Only log as warning since this is one of multiple enumeration techniques
            self.logger.warning(f"DNS enumeration had issues for {domain}: {str(e)}")

        return discovered

    def _brute_force_subdomains(self, domain: str) -> set:
        """Brute force subdomains using common prefixes"""
        discovered = set()
        
        with ThreadPoolExecutor(max_workers=20) as executor:
            future_to_subdomain = {
                executor.submit(self._check_subdomain, f"{prefix}.{domain}"): prefix 
                for prefix in self.common_subdomains
            }
            
            for future in as_completed(future_to_subdomain):
                try:
                    result = future.result()
                    if result:
                        discovered.add(result)
                except Exception as e:
                    continue

        return discovered

    def _check_subdomain(self, subdomain: str) -> str:
        """Check if a subdomain exists"""
        try:
            self.resolver.resolve(subdomain, 'A')
            return subdomain
        except:
            return None

    def _validate_subdomain(self, subdomain: str) -> Dict:
        """Validate and get information about a subdomain"""
        try:
            ip_address = socket.gethostbyname(subdomain)
            
            # Basic HTTP check
            is_http = False
            http_status = None
            try:
                response = requests.get(f"http://{subdomain}", timeout=3)
                is_http = True
                http_status = response.status_code
            except:
                try:
                    response = requests.get(f"https://{subdomain}", timeout=3)
                    is_http = True
                    http_status = response.status_code
                except:
                    pass

            return {
                'subdomain': subdomain,
                'ip_address': ip_address,
                'is_http': is_http,
                'http_status': http_status,
                'status': 'active'
            }
        except Exception as e:
            return Nonefrom typing import Dict, List
import logging
from datetime import datetime
from .scanner import VulnerabilityScanner
from .zap_manager import ZAPManager
from .models import Vulnerability, NucleiFinding
from .nuclei_scanner import NucleiScanner
from .correlation import VulnerabilityCorrelator  # Import the enhanced correlator

class UnifiedVulnerabilityScanner:
    # Define valid scan types
    VALID_SCAN_TYPES = {
        'quick': 'Fast scan with basic checks',
        'standard': 'Standard comprehensive scan',
        'full': 'Full detailed scan with all checks'
    }

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.internal_scanner = VulnerabilityScanner()
        self.zap_manager = ZAPManager()
        self.correlator = VulnerabilityCorrelator()  # Initialize the correlator
        try:
            self.nuclei_scanner = NucleiScanner()
            self.logger.info("Successfully initialized Nuclei scanner")
        except Exception as e:
            self.logger.error(f"Failed to initialize Nuclei scanner: {str(e)}")
            self.nuclei_scanner = None
            # Don't raise the exception, just log it and continue with other scanners

    def validate_scan_type(self, scan_type: str) -> str:
        """Validate scan type and return normalized version"""
        if not scan_type:
            return 'standard'  # Default scan type
            
        normalized_type = scan_type.lower().strip()
        if normalized_type not in self.VALID_SCAN_TYPES:
            raise ValueError(
                f"Invalid scan type: '{scan_type}'. Valid types are: {list(self.VALID_SCAN_TYPES.keys())}"
            )
        return normalized_type

    def get_scan_types(self) -> Dict[str, str]:
        """Return available scan types and their descriptions"""
        return self.VALID_SCAN_TYPES

    def scan_target(self, target: str, scan_type: str = 'standard', 
                   include_zap: bool = True, include_nuclei: bool = True,
                   nuclei_scan_type: str = 'basic',
                   use_advanced_correlation: bool = True) -> Dict:
        """
        Perform a comprehensive scan using multiple scanners based on parameters
        """
        try:
            # Validate scan type first
            validated_scan_type = self.validate_scan_type(scan_type)
            
            results = {
                'target': target,
                'scan_start': datetime.now().isoformat(),
                'vulnerabilities': [],
                'scanners_used': ['internal'],
                'summary': {
                    'high': 0,
                    'medium': 0,
                    'low': 0,
                    'total': 0
                },
                'correlation': {}
            }

            # Track findings from each scanner for correlation
            internal_results = []
            zap_results = []
            nuclei_results = []

            # Run internal scanner with validated scan type
            self.logger.info(f"Starting internal scanner with scan type: {validated_scan_type}")
            internal_scan = self.internal_scanner.scan_target(target, validated_scan_type)
            
            if internal_scan.get('vulnerabilities'):
                internal_results = internal_scan['vulnerabilities']
                results['scanners_used'].append('internal')
                self.logger.info(f"Internal scanner found {len(internal_results)} vulnerabilities")

            # Run ZAP scan if requested
            if include_zap:
                self.logger.info("Starting ZAP scanner")
                if self.zap_manager.ensure_zap_running():
                    results['scanners_used'].append('zap')
                    zap_scan = self.zap_manager.run_scan(target)
                    if zap_scan.get('status') == 'success' and zap_scan.get('alerts'):
                        zap_results = zap_scan['alerts']
                        self.logger.info(f"ZAP scanner found {len(zap_results)} vulnerabilities")
                else:
                    self.logger.warning("ZAP scanner not available")

            # Run Nuclei scan if requested
            # In the scan_target method of UnifiedVulnerabilityScanner class:
            # Run Nuclei scan if requested
            if include_nuclei:
                if self.nuclei_scanner:
                    try:
                        self.logger.info(f"Starting Nuclei scan with type: {nuclei_scan_type}")
                        if nuclei_scan_type.lower() == 'advanced':
                            nuclei_scan = self.nuclei_scanner.run_advanced_scan(target)
                        else:
                            nuclei_scan = self.nuclei_scanner.run_basic_scan(target)
                        
                        if nuclei_scan.get('status') == 'success' and nuclei_scan.get('findings'):
                            results['scanners_used'].append('nuclei')
                            nuclei_results = nuclei_scan['findings']
                            self.logger.info(f"Nuclei scanner found {len(nuclei_results)} vulnerabilities")
                        else:
                            self.logger.warning(f"Nuclei scan completed but returned no findings or had an error: {nuclei_scan.get('error', 'No error specified')}")
                    except Exception as e:
                        self.logger.error(f"Nuclei scan failed: {str(e)}")
                        self.logger.error("Continuing with other scanners")
                else:
                    self.logger.warning("Nuclei scanner not available or failed to initialize")

            # Use enhanced correlation if enabled
            if use_advanced_correlation:
                self.logger.info("Using advanced correlation for findings")
                
                # Normalize target before passing to correlator
                normalized_target = self._normalize_target(target)
                
                correlation_result = self.correlator.correlate_findings(
                    internal_results=internal_results,
                    zap_results=zap_results,
                    nuclei_results=nuclei_results,
                    target=normalized_target  # Pass normalized target
                )
                              
                if correlation_result.get('status') == 'success':
                    results['vulnerabilities'] = correlation_result.get('findings', [])
                    results['correlation'] = {
                        'original_count': correlation_result.get('original_count', 0),
                        'correlated_count': correlation_result.get('correlated_count', 0),
                        'reduction_percentage': correlation_result.get('statistics', {}).get('reduction_percentage', 0),
                        'stats': correlation_result.get('statistics', {})
                    }
                    self.logger.info(f"Correlation reduced findings from {correlation_result.get('original_count', 0)} to {correlation_result.get('correlated_count', 0)}")
                else:
                    # Fallback to basic processing if correlation failed
                    self.logger.warning("Advanced correlation failed, falling back to basic processing")
                    self._process_scanner_results(results, internal_results, zap_results, nuclei_results)
            else:
                # Use basic processing
                self.logger.info("Using basic processing for findings")
                self._process_scanner_results(results, internal_results, zap_results, nuclei_results)
            
            # Update summary
            self._update_summary(results)
            
            # Add scan configuration to results
            results['scan_config'] = {
                'scan_type': validated_scan_type,
                'scanners': {
                    'internal': True,
                    'zap': include_zap,
                    'nuclei': {
                        'enabled': include_nuclei,
                        'type': nuclei_scan_type if include_nuclei else 'disabled'
                    }
                },
                'correlation': {
                    'advanced': use_advanced_correlation
                }
            }
            
            results['scan_end'] = datetime.now().isoformat()
            results['status'] = 'success'
            
            # Deduplicate vulnerabilities
            deduplication_stats = Vulnerability.deduplicate_vulnerabilities(target)
            
            # Add deduplication stats to results
            results['deduplication_stats'] = deduplication_stats
            
            return results

        except ValueError as e:
            # Handle validation errors
            error_msg = str(e)
            self.logger.error(f"Validation error: {error_msg}")
            return {
                'status': 'error',
                'error': error_msg,
                'valid_scan_types': list(self.VALID_SCAN_TYPES.keys())
            }
        except Exception as e:
            self.logger.error(f"Unified scan failed: {str(e)}")
            return {
                'status': 'error',
                'error': str(e)
            }
            
# Modify how vulnerabilities are processed:

    def _process_scanner_results(self, results: Dict, internal_results: List, 
                                zap_results: List, nuclei_results: List) -> None:
        """Process results from different scanners using basic approach"""
        
        # Normalize the target - extract just the domain from the URL
        target = results.get('target', '')
        if '://' in target:
            from urllib.parse import urlparse
            parsed = urlparse(target)
            normalized_target = parsed.netloc
        else:
            normalized_target = target
        
        # Process internal scanner results with normalized target
        for vuln in internal_results:
            results['vulnerabilities'].append({
                'source': 'internal',
                'name': vuln.get('name', ''),
                'description': vuln.get('description', ''),
                'severity': vuln.get('severity', 'LOW'),
                'type': vuln.get('type', ''),
                'evidence': vuln.get('evidence', ''),
                'confidence': vuln.get('confidence', 'medium'),
                'cvss_score': vuln.get('cvss', 0.0),
                'target': normalized_target  # Use normalized target
            })

        # Process ZAP scanner results with normalized target
        for alert in zap_results:
            results['vulnerabilities'].append({
                'source': 'zap',
                'name': alert.get('name', ''),
                'description': alert.get('description', ''),
                'severity': self._normalize_severity(alert.get('risk')),
                'type': 'web',
                'evidence': alert.get('evidence', ''),
                'confidence': alert.get('confidence', 'medium'),
                'solution': alert.get('solution', ''),
                'cwe': alert.get('cweid', ''),
                'target': normalized_target,  # Use normalized target
                'metadata': {
                    'url': alert.get('url', ''),
                    'parameter': alert.get('parameter', '')
                }
            })

        # Process Nuclei scanner results with normalized target
        for finding in nuclei_results:
            results['vulnerabilities'].append({
                'source': 'nuclei',
                'name': finding.get('name', ''),
                'description': finding.get('description', ''),
                'severity': finding.get('severity', 'LOW'),
                'type': finding.get('type', 'nuclei'),
                'evidence': finding.get('evidence', ''),
                'confidence': 'high',
                'cvss_score': finding.get('cvss_score', 0.0),
                'cwe': finding.get('cwe', ''),
                'references': finding.get('references', []),
                'target': normalized_target,  # Use normalized target
                'metadata': {
                    'template_id': finding.get('template_id', ''),
                    'tags': finding.get('tags', []),
                    'matched_at': finding.get('matched', ''),
                    'host': finding.get('host', '')
                }
            })
        
    def _basic_deduplicate(self, vulnerabilities: List[Dict]) -> List[Dict]:
        """Simple deduplication of vulnerabilities by name and severity"""
        unique_vulns = {}
        
        for vuln in vulnerabilities:
            # Create a key for deduplication
            key = f"{vuln['name']}_{vuln['severity']}"
            
            if key in unique_vulns:
                existing = unique_vulns[key]
                # Merge sources
                sources = set([existing['source']])
                sources.add(vuln['source'])
                existing['source'] = ','.join(sources)
                # Take highest confidence
                if vuln.get('confidence') == 'high':
                    existing['confidence'] = 'high'
            else:
                unique_vulns[key] = vuln
                
        return list(unique_vulns.values())

    def _normalize_severity(self, severity: str) -> str:
        """Normalize severity ratings across different scanners"""
        severity = str(severity).lower()
        
        if severity in ['critical', 'high', '3', '4']:
            return 'HIGH'
        elif severity in ['medium', 'warning', '2']:
            return 'MEDIUM'
        elif severity in ['low', 'info', '1']:
            return 'LOW'
        return 'INFO'

    def _update_summary(self, results: Dict) -> None:
        """Update the summary counts"""
        summary = {'high': 0, 'medium': 0, 'low': 0, 'total': 0}
        
        for vuln in results['vulnerabilities']:
            severity = vuln['severity'].lower()
            if severity in summary:
                summary[severity] += 1
            summary['total'] += 1

        results['summary'] = summary

    def get_scanner_status(self) -> Dict:
        """Get status of all scanners"""
        status = {
            'internal': {
                'status': 'available',
                'checks': list(self.internal_scanner.checks.keys())
            },
            'zap': self.zap_manager.get_status()
        }

        # Add Nuclei status if initialized
        if self.nuclei_scanner:
            try:
                nuclei_info = self.nuclei_scanner.get_template_info()
                status['nuclei'] = {
                    'status': 'available',
                    'templates': nuclei_info.get('total_templates', 0),
                    'template_types': nuclei_info.get('template_types', {})
                }
            except Exception as e:
                status['nuclei'] = {
                    'status': 'error',
                    'error': str(e)
                }

        return status

    def _save_findings(self, vulnerabilities: List[Dict], target: str) -> None:
        """Save findings to database"""
        for vuln in vulnerabilities:
            # Extract metadata
            metadata = {
                'url': vuln.get('url'),
                'parameter': vuln.get('parameter'),
                'extra_info': vuln.get('metadata', {})
            }
            
            try:
                Vulnerability.objects.create(
                    target=target,
                    name=vuln['name'],
                    description=vuln.get('description', ''),
                    severity=vuln['severity'],
                    vuln_type=vuln.get('type', 'unknown'),
                    evidence=vuln.get('evidence', ''),
                    source=vuln['source'],
                    confidence=vuln['confidence'],
                    solution=vuln.get('solution', ''),
                    cwe=vuln.get('cwe', ''),
                    cvss_score=vuln.get('cvss_score'),
                    references=vuln.get('references', []),
                    metadata=metadata
                )
            except Exception as e:
                self.logger.error(f"Error saving vulnerability: {str(e)}")
                self.logger.error(f"Vulnerability data: {vuln}")
                
    # File: vulnerability/unified_scanner.py
# Add a method to match vulnerabilities with exploits

    def match_vulnerabilities_with_exploits(self, vulnerabilities: list) -> dict:
        """
        Match vulnerabilities with potential exploits
        
        Args:
            vulnerabilities: List of vulnerability dictionaries
            
        Returns:
            dict: Statistics about matches
        """
        try:
            from exploit_manager.matcher import ExploitMatcher
            from vulnerability.models import Vulnerability
            
            matcher = ExploitMatcher()
            stats = {
                'total_vulnerabilities': len(vulnerabilities),
                'vulnerabilities_with_matches': 0,
                'total_matches': 0
            }
            
            # Process each vulnerability
            for vuln_data in vulnerabilities:
                try:
                    # Get vulnerability from database
                    vuln_id = vuln_data.get('id')
                    if not vuln_id:
                        continue
                        
                    vulnerability = Vulnerability.objects.get(id=vuln_id)
                    
                    # Find matches
                    matches = matcher.match_vulnerability(vulnerability)
                    
                    if matches:
                        stats['vulnerabilities_with_matches'] += 1
                        stats['total_matches'] += len(matches)
                        
                except Exception as e:
                    self.logger.error(f"Error matching vulnerability: {str(e)}")
                    
            return stats
            
        except Exception as e:
            self.logger.error(f"Error in match_vulnerabilities_with_exploits: {str(e)}")
            return {
                'total_vulnerabilities': len(vulnerabilities),
                'vulnerabilities_with_matches': 0,
                'total_matches': 0,
                'error': str(e)
            }
        


    def _normalize_target(self, target: str) -> str:
        """Normalize target URL to consistent format"""
        if not target:
            return ""
            
        # Remove protocol prefix
        if '://' in target:
            target = target.split('://', 1)[1]
        
        # Remove path, trailing slash, etc.
        if '/' in target:
            target = target.split('/', 1)[0]
            
        # Remove port if present
        if ':' in target:
            target = target.split(':', 1)[0]
            
        # Remove 'www.' prefix if present
        if target.startswith('www.'):
            target = target[4:]
            
        return target.lower()
            

                
    # exploit_manager/matcher.py
import logging
import re
from typing import List, Tuple, Dict, Optional

from django.db.models import Q
from django.utils import timezone

from vulnerability.models import Vulnerability
from .models import Exploit, ExploitMatch

class ExploitMatcher:
    """
    Matches vulnerabilities with potential exploits from the database
    using a generalized approach that works with any vulnerability type
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def match_vulnerability(self, vulnerability: Vulnerability) -> List[ExploitMatch]:
        """
        Find potential exploits for a vulnerability using a flexible matching approach
        
        Args:
            vulnerability: The vulnerability to match
            
        Returns:
            List of ExploitMatch objects
        """
        try:
            self.logger.info(f"Matching vulnerability: {vulnerability.name} [{vulnerability.severity}]")
            
            # First attempt classification-based matching
            matches = self._classify_and_match(vulnerability)
            
            # If no matches found, fall back to generic content-based matching
            if not matches:
                self.logger.info(f"No matches found via classification, trying generic matching")
                matches = self._generic_match(vulnerability)
            
            # If still no matches, try a last-resort approach
            if not matches:
                self.logger.info(f"No matches found via generic matching, using fallback method")
                matches = self._fallback_match(vulnerability)
            
            # Save matches to database
            saved_matches = self.save_matches(vulnerability, matches)
            
            self.logger.info(f"Found {len(saved_matches)} potential exploits for vulnerability ID {vulnerability.id}")
            return saved_matches
            
        except Exception as e:
            self.logger.error(f"Error matching vulnerability: {str(e)}")
            return []
    
    def _classify_and_match(self, vulnerability: Vulnerability) -> List[Tuple[Exploit, float, str]]:
        """
        Classify vulnerability type and apply appropriate matching strategy
        """
        vuln_name = vulnerability.name.lower()
        vuln_type = vulnerability.vuln_type.lower() if vulnerability.vuln_type else ""
        vuln_desc = vulnerability.description.lower()
        
        # Match aggregation
        matches = []
        
        # Category 1: Port-related vulnerabilities
        if ('port' in vuln_name or 'port' in vuln_type) or re.search(r'port\s+\d+', vuln_name):
            port_matches = self._match_port_vulnerabilities(vulnerability)
            matches.extend(port_matches)
        
        # Category 2: Header-related vulnerabilities
        header_terms = ['header', 'csp', 'hsts', 'clickjacking', 'content-type', 'frame-options',
                        'x-frame', 'x-content', 'strict-transport', 'csrf', 'security policy']
        if any(term in vuln_name or term in vuln_desc for term in header_terms):
            header_matches = self._match_header_vulnerabilities(vulnerability)
            matches.extend(header_matches)
        
        # Category 3: Web-related vulnerabilities
        web_terms = ['web', 'http', 'https', 'webapp', 'website', 'javascript', 'dom', 'nginx', 'apache', 
                    'iis', 'cdn', 'waf', 'wappalyzer', 'redirect', 'aspnet']
        if any(term in vuln_name or term in vuln_desc or term in vuln_type for term in web_terms):
            web_matches = self._match_web_vulnerabilities(vulnerability)
            matches.extend(web_matches)
        
        # Category 4: Authentication and access-related vulnerabilities
        auth_terms = ['auth', 'access', 'login', 'password', 'credential', 'session', 'token', 'csrf', 'authentication']
        if any(term in vuln_name or term in vuln_desc for term in auth_terms):
            auth_matches = self._match_auth_vulnerabilities(vulnerability)
            matches.extend(auth_matches)
        
        # Category 5: Information disclosure vulnerabilities
        disclosure_terms = ['disclosure', 'leak', 'information', 'debug', 'error', 'version', 'sensitive']
        if any(term in vuln_name or term in vuln_desc for term in disclosure_terms):
            disclosure_matches = self._match_disclosure_vulnerabilities(vulnerability)
            matches.extend(disclosure_matches)
        
        # Category 6: XSS and other injection vulnerabilities
        injection_terms = ['xss', 'script', 'injection', 'cross site', 'cross-site', 'sql', 'command', 'xxe']
        if any(term in vuln_name or term in vuln_desc for term in injection_terms):
            injection_matches = self._match_injection_vulnerabilities(vulnerability)
            matches.extend(injection_matches)
        
        # Category 7: SSH-related vulnerabilities
        ssh_terms = ['ssh', 'secure shell', 'openssh']
        if any(term in vuln_name or term in vuln_desc or term in vuln_type for term in ssh_terms):
            ssh_matches = self._match_ssh_vulnerabilities(vulnerability)
            matches.extend(ssh_matches)
        
        return matches
    
    def _generic_match(self, vulnerability: Vulnerability) -> List[Tuple[Exploit, float, str]]:
        """
        Generic content-based matching using keywords from vulnerability name and description
        """
        # Extract key terms from vulnerability name and description
        vuln_name = vulnerability.name.lower() if vulnerability.name else ""
        vuln_desc = vulnerability.description.lower() if vulnerability.description else ""
        vuln_type = vulnerability.vuln_type.lower() if vulnerability.vuln_type else ""
        
        # Extract keywords from name and description
        name_keywords = self._extract_keywords(vuln_name)
        desc_keywords = self._extract_keywords(vuln_desc, max_words=5)
        type_keywords = self._extract_keywords(vuln_type, max_words=2)
        
        # Combine keywords, ensuring no duplicates
        all_keywords = list(set(name_keywords + desc_keywords + type_keywords))
        
        self.logger.info(f"Generic matching with keywords: {all_keywords}")
        
        if not all_keywords:
            return []
            
        # Build query using keywords
        query = Q()
        
        # Each keyword adds to the OR query
        for keyword in all_keywords:
            if len(keyword) > 3:  # Only use keywords with more than 3 chars
                query |= Q(title__icontains=keyword) | Q(description__icontains=keyword)
        
        # Filter by exploit type based on severity for better relevance
        if vulnerability.severity == 'CRITICAL' or vulnerability.severity == 'HIGH':
            # For high severity, prioritize remote exploits
            exploits = Exploit.objects.filter(query).filter(
                Q(type__icontains='remote') | Q(type__icontains='webapps')
            ).order_by('-date_published')[:20]
        else:
            # For medium/low severity, use broader criteria
            exploits = Exploit.objects.filter(query).order_by('-date_published')[:20]
        
        matches = []
        for exploit in exploits:
            # Calculate match score based on keyword presence
            score = self._calculate_keyword_match_score(exploit, all_keywords)
            matches.append((exploit, score, "Keyword match"))
        
        return matches
    
    def _fallback_match(self, vulnerability: Vulnerability) -> List[Tuple[Exploit, float, str]]:
        """
        Last-resort matching when other strategies fail to find matches
        """
        # For fallback, we'll use a very broad approach based on severity and type
        matches = []
        
        # Categorize by severity
        if vulnerability.severity in ['CRITICAL', 'HIGH']:
            # For high severity, prioritize exploits for critical vulnerabilities
            exploits = Exploit.objects.filter(
                Q(title__icontains='critical') | 
                Q(description__icontains='critical')
            ).order_by('-date_published')[:10]
            
            for exploit in exploits:
                matches.append((exploit, 0.3, f"Severity-based match ({vulnerability.severity})"))
                
        elif vulnerability.severity == 'MEDIUM':
            # For medium severity, look for common vulnerability types
            exploits = Exploit.objects.filter(
                Q(type__icontains='remote') | 
                Q(type__icontains='webapps')
            ).order_by('-date_published')[:10]
            
            for exploit in exploits:
                matches.append((exploit, 0.2, "General exploit match"))
                
        else:  # LOW or INFO severity
            # For low severity, just get some reasonable matches
            exploits = Exploit.objects.all().order_by('-date_published')[:5]
            
            for exploit in exploits:
                matches.append((exploit, 0.1, "General vulnerability match"))
        
        return matches
    
    def _match_port_vulnerabilities(self, vulnerability: Vulnerability) -> List[Tuple[Exploit, float, str]]:
        """Match port-related vulnerabilities"""
        matches = []
        vuln_name = vulnerability.name.lower()
        
        # Extract port number and service
        port_match = re.search(r'port\s+(\d+)\s*\(?(.*?)\)?', vuln_name, re.IGNORECASE)
        if port_match:
            port_number = port_match.group(1)
            service_name = port_match.group(2).strip() if port_match.group(2) else ""
            
            self.logger.info(f"Matching port vulnerability: Port {port_number} ({service_name})")
            
            # Special handling based on port/service combinations
            if port_number in ['80', '443', '8080', '8443'] or 'http' in service_name.lower() or 'web' in service_name.lower():
                # HTTP/Web ports
                http_query = (
                    Q(title__icontains='http') | 
                    Q(description__icontains='http') | 
                    Q(title__icontains='web') | 
                    Q(description__icontains='web') | 
                    Q(type__icontains='webapps') |
                    Q(platform__icontains='php')
                )
                
                exploits = Exploit.objects.filter(http_query).order_by('-date_published')[:20]
                
                for exploit in exploits:
                    score = 0.5 if exploit.type.lower() == 'webapps' else 0.4
                    matches.append((exploit, score, f"Web/HTTP exploit for port {port_number}"))
                
            elif port_number == '22' or 'ssh' in service_name.lower():
                # SSH port
                ssh_query = (
                    Q(title__icontains='ssh') | 
                    Q(description__icontains='ssh') | 
                    Q(title__icontains='remote') | 
                    Q(description__icontains='remote access')
                )
                
                exploits = Exploit.objects.filter(ssh_query).order_by('-date_published')[:15]
                
                for exploit in exploits:
                    matches.append((exploit, 0.4, f"SSH exploit for port {port_number}"))
                
            elif port_number == '21' or 'ftp' in service_name.lower():
                # FTP port
                ftp_query = (
                    Q(title__icontains='ftp') | 
                    Q(description__icontains='ftp') | 
                    Q(title__icontains='file transfer')
                )
                
                exploits = Exploit.objects.filter(ftp_query).order_by('-date_published')[:15]
                
                for exploit in exploits:
                    matches.append((exploit, 0.4, f"FTP exploit for port {port_number}"))
                
            else:
                # Other ports - generic approach
                general_query = Q(title__icontains='port') | Q(type__icontains='remote')
                
                # Try to include service name in query if available
                if service_name:
                    service_terms = service_name.lower().split()
                    for term in service_terms:
                        if len(term) > 3:
                            general_query |= Q(title__icontains=term) | Q(description__icontains=term)
                
                exploits = Exploit.objects.filter(general_query).order_by('-date_published')[:15]
                
                for exploit in exploits:
                    matches.append((exploit, 0.3, f"Service/port exploit for port {port_number}"))
                
        elif 'open port' in vuln_name:
            # Generic open port without specific port in title
            # Try to find port info in description
            desc = vulnerability.description.lower()
            port_in_desc = re.search(r'port\s+(\d+)', desc)
            
            if port_in_desc:
                port_number = port_in_desc.group(1)
                # Recursively call with the port number we found
                dummy_vuln = Vulnerability(
                    name=f"Port {port_number} (Extracted)",
                    description=vulnerability.description,
                    severity=vulnerability.severity,
                    vuln_type=vulnerability.vuln_type
                )
                return self._match_port_vulnerabilities(dummy_vuln)
            else:
                # No specific port found, use generic remote exploits
                exploits = Exploit.objects.filter(
                    Q(type__icontains='remote') |
                    Q(title__icontains='network')
                ).order_by('-date_published')[:15]
                
                for exploit in exploits:
                    matches.append((exploit, 0.3, "Generic port/network exploit"))
        
        return matches
    
    def _match_web_vulnerabilities(self, vulnerability: Vulnerability) -> List[Tuple[Exploit, float, str]]:
        """Match web-related vulnerabilities"""
        matches = []
        vuln_name = vulnerability.name.lower()
        vuln_desc = vulnerability.description.lower()
        
        # Map of web vulnerability types to search terms
        web_vuln_types = {
            'redirect': ['redirect', 'http to https', 'forwarding'],
            'webapp': ['web application', 'webapp', 'website', 'modern web'],
            'server': ['apache', 'nginx', 'iis', 'frontend', 'server'],
            'waf': ['waf', 'web application firewall', 'firewall detection'],
            'tech': ['wappalyzer', 'technology', 'detection', 'framework'],
            'cdn': ['cdn', 'content delivery', 'akamai']
        }
        
        # Determine web vulnerability type
        web_types = []
        for wtype, terms in web_vuln_types.items():
            if any(term in vuln_name or term in vuln_desc for term in terms):
                web_types.append(wtype)
        
        # If no specific type identified, use general web type
        if not web_types:
            web_types = ['webapp']
        
        self.logger.info(f"Web vulnerability types: {web_types}")
        
        # Build query based on identified types
        query = Q(type__icontains='webapps')
        
        for wtype in web_types:
            if wtype == 'redirect':
                query |= Q(title__icontains='redirect') | Q(description__icontains='http')
            elif wtype == 'server':
                for server in ['apache', 'nginx', 'iis']:
                    if server in vuln_name or server in vuln_desc:
                        query |= Q(title__icontains=server) | Q(description__icontains=server)
            elif wtype == 'waf':
                query |= Q(title__icontains='firewall') | Q(description__icontains='waf')
            elif wtype == 'tech':
                query |= Q(title__icontains='framework') | Q(description__icontains='technology')
            elif wtype == 'cdn':
                query |= Q(title__icontains='cdn') | Q(description__icontains='content delivery')
        
        # Get exploits matching the query
        exploits = Exploit.objects.filter(query).order_by('-date_published')[:20]
        
        for exploit in exploits:
            # Score based on type match
            if exploit.type.lower() == 'webapps':
                score = 0.4
            else:
                score = 0.3
                
            matches.append((exploit, score, "Web vulnerability match"))
        
        # If we didn't find enough matches, broaden the search
        if len(matches) < 5:
            general_web_query = Q(type__icontains='webapps') | Q(title__icontains='web')
            general_exploits = Exploit.objects.filter(general_web_query).order_by('-date_published')[:10]
            
            for exploit in general_exploits:
                if not any(exploit.id == match[0].id for match in matches):  # Avoid duplicates
                    matches.append((exploit, 0.3, "General web exploit"))
        
        return matches
    
    def _match_header_vulnerabilities(self, vulnerability: Vulnerability) -> List[Tuple[Exploit, float, str]]:
        """Match header-related vulnerabilities (HSTS, CSP, X-Frame-Options, etc.)"""
        matches = []
        vuln_name = vulnerability.name.lower()
        vuln_desc = vulnerability.description.lower()
        
        # Map header types to related search terms
        header_types = {
            'hsts': ['hsts', 'strict-transport-security', 'transport security'],
            'csp': ['csp', 'content security policy', 'content-security-policy'],
            'clickjacking': ['clickjacking', 'x-frame-options', 'frame-options', 'anti-clickjacking'],
            'content-type': ['content-type-options', 'x-content-type-options', 'mime', 'sniff'],
            'csrf': ['csrf', 'cross-site request forgery', 'anti-csrf'],
            'powered-by': ['x-powered-by', 'server leaks', 'information header'],
            'version': ['version information', 'server header', 'version detect']
        }
        
        # Determine header type(s)
        detected_types = []
        for htype, terms in header_types.items():
            if any(term in vuln_name or term in vuln_desc for term in terms):
                detected_types.append(htype)
        
        self.logger.info(f"Header vulnerability types: {detected_types}")
        
        # If we identified specific types, build a targeted query
        if detected_types:
            query = Q()
            
            for htype in detected_types:
                if htype == 'hsts':
                    query |= Q(title__icontains='transport') | Q(description__icontains='https')
                elif htype == 'csp':
                    query |= Q(title__icontains='security policy') | Q(description__icontains='csp')
                elif htype == 'clickjacking':
                    query |= Q(title__icontains='clickjacking') | Q(description__icontains='frame')
                elif htype == 'content-type':
                    query |= Q(title__icontains='mime') | Q(description__icontains='sniff')
                elif htype == 'csrf':
                    query |= Q(title__icontains='csrf') | Q(description__icontains='forgery')
                elif htype in ['powered-by', 'version']:
                    query |= Q(title__icontains='disclosure') | Q(description__icontains='information')
            
            # Get exploits for specific header types
            specific_exploits = Exploit.objects.filter(query & Q(type__icontains='webapps')).order_by('-date_published')[:15]
            
            for exploit in specific_exploits:
                matches.append((exploit, 0.4, f"Header security match: {', '.join(detected_types)}"))
        
        # Always add some generic web security exploits as these often address header issues
        general_query = (
            Q(title__icontains='web security') | 
            Q(description__icontains='header') |
            Q(title__icontains='misconfiguration') |
            Q(description__icontains='security headers')
        )
        
        general_exploits = Exploit.objects.filter(general_query).order_by('-date_published')[:10]
        
        for exploit in general_exploits:
            # Avoid duplicates
            if not any(exploit.id == match[0].id for match in matches):
                matches.append((exploit, 0.3, "Web security configuration exploit"))
        
        return matches
    
    def _match_auth_vulnerabilities(self, vulnerability: Vulnerability) -> List[Tuple[Exploit, float, str]]:
        """Match authentication and access control vulnerabilities"""
        matches = []
        
        # Build query for auth-related vulnerabilities
        auth_query = (
            Q(title__icontains='auth') | 
            Q(description__icontains='authentication') |
            Q(title__icontains='login') | 
            Q(description__icontains='access control') |
            Q(title__icontains='bypass') | 
            Q(description__icontains='credential')
        )
        
        # Get exploits
        auth_exploits = Exploit.objects.filter(auth_query).order_by('-date_published')[:15]
        
        for exploit in auth_exploits:
            matches.append((exploit, 0.4, "Authentication/access vulnerability match"))
        
        return matches
    
    def _match_disclosure_vulnerabilities(self, vulnerability: Vulnerability) -> List[Tuple[Exploit, float, str]]:
        """Match information disclosure vulnerabilities"""
        matches = []
        vuln_name = vulnerability.name.lower()
        
        # Build query for disclosure vulnerabilities
        disclosure_query = (
            Q(title__icontains='disclosure') | 
            Q(description__icontains='information leak') |
            Q(title__icontains='sensitive') | 
            Q(description__icontains='exposure')
        )
        
        # Refine if it's debug/error related
        if 'debug' in vuln_name or 'error' in vuln_name:
            disclosure_query &= (
                Q(title__icontains='debug') | 
                Q(description__icontains='error') |
                Q(title__icontains='exception') | 
                Q(description__icontains='message')
            )
        
        # Get exploits
        disclosure_exploits = Exploit.objects.filter(disclosure_query).order_by('-date_published')[:15]
        
        for exploit in disclosure_exploits:
            matches.append((exploit, 0.4, "Information disclosure vulnerability match"))
        
        return matches
    
    def _match_injection_vulnerabilities(self, vulnerability: Vulnerability) -> List[Tuple[Exploit, float, str]]:
        """Match XSS and other injection vulnerabilities"""
        matches = []
        vuln_name = vulnerability.name.lower()
        vuln_desc = vulnerability.description.lower()
        
        # Determine injection type
        injection_type = None
        if 'xss' in vuln_name or 'cross site scripting' in vuln_name or 'cross-site scripting' in vuln_name:
            injection_type = 'xss'
            
            # Check for XSS type
            if 'dom' in vuln_name or 'dom based' in vuln_name or 'dom' in vuln_desc:
                injection_type = 'dom-xss'
            elif 'stored' in vuln_name or 'persistent' in vuln_name:
                injection_type = 'stored-xss'
            elif 'reflected' in vuln_name:
                injection_type = 'reflected-xss'
                
        elif 'sql' in vuln_name or 'sql injection' in vuln_name:
            injection_type = 'sqli'
            
            # Check for SQL injection type
            if 'blind' in vuln_name or 'blind' in vuln_desc:
                injection_type = 'blind-sqli'
            elif 'time' in vuln_name or 'time based' in vuln_name:
                injection_type = 'time-sqli'
                
        elif 'command' in vuln_name and 'injection' in vuln_name:
            injection_type = 'cmdi'
        elif 'xxe' in vuln_name or 'xml' in vuln_name:
            injection_type = 'xxe'
        
        # Build query based on injection type
        query = Q()
        
        if injection_type in ['xss', 'dom-xss', 'stored-xss', 'reflected-xss']:
            query = (
                Q(title__icontains='xss') | 
                Q(description__icontains='cross site scripting') |
                Q(title__icontains='cross-site') | 
                Q(description__icontains='script injection')
            )
            
            # Refine for specific XSS types
            if injection_type == 'dom-xss':
                query &= Q(title__icontains='dom') | Q(description__icontains='dom')
            elif injection_type == 'stored-xss':
                query &= Q(title__icontains='stored') | Q(description__icontains='persistent')
            elif injection_type == 'reflected-xss':
                query &= Q(title__icontains='reflected') | Q(description__icontains='reflected')
                
        elif injection_type in ['sqli', 'blind-sqli', 'time-sqli']:
            query = (
                Q(title__icontains='sql') | 
                Q(description__icontains='sql injection') |
                Q(title__icontains='sqli')
            )
            
            # Refine for specific SQL injection types
            if injection_type == 'blind-sqli':
                query &= Q(title__icontains='blind') | Q(description__icontains='blind')
            elif injection_type == 'time-sqli':
                query &= Q(title__icontains='time') | Q(description__icontains='time based')
                
        elif injection_type == 'cmdi':
            query = (
                Q(title__icontains='command') | 
                Q(description__icontains='command injection') |
                Q(title__icontains='os command') | 
                Q(description__icontains='shell command')
            )
            
        elif injection_type == 'xxe':
            query = (
                Q(title__icontains='xxe') | 
                Q(description__icontains='xml external entity') |
                Q(title__icontains='xml injection')
            )
            
        else:
            # Generic injection query
            query = (
                Q(title__icontains='injection') | 
                Q(description__icontains='injection')
            )
        
        # Get matching exploits
        injection_exploits = Exploit.objects.filter(query).order_by('-date_published')[:15]
        
        for exploit in injection_exploits:
            if injection_type:
                match_reason = f"{injection_type.upper()} vulnerability match"
            else:
                match_reason = "Injection vulnerability match"
                
            matches.append((exploit, 0.5, match_reason))  # Higher score for injection vulnerabilities
        
        return matches
    
    def _match_ssh_vulnerabilities(self, vulnerability: Vulnerability) -> List[Tuple[Exploit, float, str]]:
        """Match SSH-related vulnerabilities"""
        matches = []
        vuln_name = vulnerability.name.lower()
        vuln_desc = vulnerability.description.lower()
        
        # Build query for SSH vulnerabilities
        ssh_query = (
            Q(title__icontains='ssh') | 
            Q(description__icontains='secure shell') |
            Q(title__icontains='openssh') | 
            Q(platform__icontains='ssh')
        )
        
        # Enhance query based on specifics
        if 'auth' in vuln_name or 'authentication' in vuln_name or 'auth' in vuln_desc:
            ssh_query &= (
                Q(title__icontains='auth') | 
                Q(description__icontains='authentication') |
                Q(title__icontains='login')
            )
            
            auth_exploits = Exploit.objects.filter(ssh_query).order_by('-date_published')[:15]
            
            for exploit in auth_exploits:
                matches.append((exploit, 0.5, "SSH authentication vulnerability match"))
        else:
            # General SSH exploits
            general_ssh_exploits = Exploit.objects.filter(ssh_query).order_by('-date_published')[:15]
            
            for exploit in general_ssh_exploits:
                matches.append((exploit, 0.4, "SSH service vulnerability match"))
        
        # If no SSH-specific exploits, try broader remote access exploits
        if not matches:
            remote_query = (
                Q(type__icontains='remote') & 
                (Q(title__icontains='authentication') | Q(description__icontains='login'))
            )
            
            remote_exploits = Exploit.objects.filter(remote_query).order_by('-date_published')[:10]
            
            for exploit in remote_exploits:
                matches.append((exploit, 0.3, "Remote access vulnerability match"))
        
        return matches
    
    def _extract_keywords(self, text: str, max_words: int = 6) -> List[str]:
        """Extract keywords from text, excluding common words"""
        if not text:
            return []
            
        # Common words to exclude
        common_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', 'be', 'been',
            'has', 'have', 'had', 'do', 'does', 'did', 'not', 'on', 'in', 'at', 'to', 'for',
            'with', 'by', 'about', 'against', 'between', 'into', 'through', 'during', 'before',
            'after', 'above', 'below', 'from', 'up', 'down', 'of', 'this', 'that', 'these', 
            'those', 'should', 'could', 'would', 'will', 'can', 'may', 'might', 'must', 'server',
            'vulnerability', 'issue', 'security', 'missing', 'detection', 'found', 'version'
        }
        
        # Split text into words, convert to lowercase, and filter
        words = re.findall(r'\b\w+\b', text.lower())
        # Split text into words, convert to lowercase, and filter
        words = re.findall(r'\b\w+\b', text.lower())
        keywords = [word for word in words if word not in common_words and len(word) > 2]
        
        # Limit number of keywords
        return keywords[:max_words]
    
    def _calculate_keyword_match_score(self, exploit: Exploit, keywords: List[str]) -> float:
        """Calculate match score based on keyword presence in exploit"""
        if not keywords:
            return 0.2  # Baseline score
            
        title = exploit.title.lower()
        description = exploit.description.lower() if exploit.description else ""
        
        # Count matches in title (weighted higher)
        title_matches = sum(1 for keyword in keywords if keyword in title)
        
        # Count matches in description
        desc_matches = sum(1 for keyword in keywords if keyword in description)
        
        # Calculate weighted score
        total_keywords = len(keywords)
        if total_keywords == 0:
            return 0.2
            
        score = (title_matches * 2 + desc_matches) / (total_keywords * 3)
        
        # Scale to appropriate range (0.2 - 0.8)
        return min(0.8, max(0.2, score * 0.8))
    
    def save_matches(self, vulnerability: Vulnerability, matches: List[Tuple[Exploit, float, str]]) -> List[ExploitMatch]:
        """Save matches to database and return saved objects"""
        saved_matches = []
        
        # Sort by confidence score (highest first)
        sorted_matches = sorted(matches, key=lambda x: x[1], reverse=True)
        
        # Limit number of matches to save
        max_matches = 10
        sorted_matches = sorted_matches[:max_matches]
        
        for exploit, confidence, reason in sorted_matches:
            try:
                # Skip exploits with very low confidence
                if confidence < 0.1:
                    continue
                    
                # Get or create match object
                match, created = ExploitMatch.objects.get_or_create(
                    vulnerability=vulnerability,
                    exploit=exploit,
                    defaults={
                        'confidence_score': confidence,
                        'match_reason': reason,
                        'status': 'pending',
                        # match_date is auto_now_add - no need to set it explicitly
                    }
                )
                
                # Update if it already existed
                if not created:
                    match.confidence_score = confidence
                    match.match_reason = reason
                    # last_updated has auto_now=True - no need to set it explicitly
                    match.save()
                
                saved_matches.append(match)
                
            except Exception as e:
                self.logger.error(f"Error saving match: {str(e)}")
        
        return saved_matchesimport logging
from datetime import datetime
import json
from django.core.serializers.json import DjangoJSONEncoder
from django.forms.models import model_to_dict
from .models import Report
from reconnaissance.models import Subdomain, PortScan
from vulnerability.models import Vulnerability

class ReportGenerator:
    def __init__(self):
        # Add logger initialization
        self.logger = logging.getLogger(__name__)
        
# File: reporting/report_generator.py
    def generate_report(self, report_type: str, target: str, output_format: str = 'json', scan_results: dict = None) -> Report:
        """
        Generate a security report with improved vulnerability handling
        
        Args:
            report_type: Type of report ('basic', 'detailed', 'executive')
            target: Target hostname/domain
            output_format: Format to generate ('json', 'html', 'pdf')
            scan_results: Optional dictionary containing scan results to include
            
        Returns:
            Report: The generated report object
        """
        # Clean target string
        target = target.strip()
        
        if report_type not in ['basic', 'detailed', 'executive']:
            report_type = 'basic'
        
        # Generate report content based on type
        try:
            self.logger.info(f"Starting report generation for {target}, type: {report_type}, format: {output_format}")
            
            # First, ensure vulnerabilities from scan_results are saved to the database
            if scan_results and 'vulnerabilities' in scan_results:
                vuln_list = scan_results['vulnerabilities']
                vuln_count = len(vuln_list)
                self.logger.info(f"Processing {vuln_count} vulnerabilities from scan results")
                
                from vulnerability.models import Vulnerability
                saved_count = 0
                
                # Save each vulnerability to database
                for vuln_data in vuln_list:
                    try:
                        # Ensure we have basic required fields
                        name = vuln_data.get('name', 'Unknown Vulnerability')
                        
                        # Normalize severity
                        severity = vuln_data.get('severity', 'LOW')
                        if isinstance(severity, str):
                            severity = severity.upper()
                        
                        # Get vuln_type from either 'type' or 'vuln_type' field
                        vuln_type = vuln_data.get('type', vuln_data.get('vuln_type', 'unknown'))
                        
                        # Create or update vulnerability in database
                        vuln, created = Vulnerability.objects.update_or_create(
                            target=target,
                            name=name,
                            defaults={
                                'description': vuln_data.get('description', ''),
                                'severity': severity,
                                'vuln_type': vuln_type,
                                'evidence': vuln_data.get('evidence', ''),
                                'source': vuln_data.get('source', 'scan'),
                                'confidence': vuln_data.get('confidence', 'medium'),
                                'cvss_score': vuln_data.get('cvss_score', 0.0),
                                'is_fixed': False
                            }
                        )
                        
                        status = "Created" if created else "Updated"
                        self.logger.debug(f"{status} vulnerability in database: {name}")
                        saved_count += 1
                        
                    except Exception as e:
                        self.logger.error(f"Error saving vulnerability '{vuln_data.get('name', 'unknown')}': {str(e)}")
                
                self.logger.info(f"Saved {saved_count} vulnerabilities to database")
                
                # Get fresh data from database to ensure report accuracy
                from vulnerability.models import Vulnerability
                db_vulns = Vulnerability.objects.filter(target=target, is_fixed=False)
                db_vuln_count = db_vulns.count()
                self.logger.info(f"Found {db_vuln_count} vulnerabilities in database for report")
            
            # Generate the appropriate report type
            if report_type == 'detailed':
                content = self.generate_detailed_report(target, scan_results)
            elif report_type == 'executive':
                content = self.generate_executive_report(target, scan_results)
            else:
                content = self.generate_basic_report(target, scan_results)
            
            # Make sure severity counts are properly calculated
            self._update_severity_counts(content)
            
            # Add workflow_id to content if provided in scan_results
            if scan_results and 'workflow_id' in scan_results:
                content['workflow_id'] = scan_results['workflow_id']
            
            # If output format is PDF, ensure proper formatting
            if output_format == 'pdf':
                content = self._format_for_pdf(content)
            
            # Verify vulnerability data is present
            if 'vulnerabilities' in content:
                vuln_count = len(content['vulnerabilities'])
                self.logger.info(f"Report contains {vuln_count} vulnerabilities")
            else:
                self.logger.warning("No vulnerabilities included in the report")
            
            # Properly serialize content to JSON string
            json_content = json.dumps(content, cls=DjangoJSONEncoder)
            
            # Create and save the report
            report = Report.objects.create(
                title=f"{report_type.capitalize()} Security Report - {target}",
                content=json_content,
                report_type=f"{report_type}_{output_format}"
            )
            
            self.logger.info(f"Generated {report_type} report for {target} with ID {report.id}")
            return report
                
        except Exception as e:
            self.logger.error(f"Report generation failed: {str(e)}")
            raise
    
    
    def _update_severity_counts(self, content: dict) -> None:
        """
        Ensure vulnerability severity counts are accurate with detailed logging
        """
        if 'vulnerabilities' not in content:
            self.logger.warning("No 'vulnerabilities' key found in content - cannot update severity counts")
            return
        
        # Reset counters
        severity_counts = {
            'critical': 0,
            'high': 0,
            'medium': 0,
            'low': 0,
            'info': 0
        }
        
        # Count each vulnerability by severity
        self.logger.info(f"Counting {len(content['vulnerabilities'])} vulnerabilities by severity")
        
        for vuln in content['vulnerabilities']:
            severity = vuln.get('severity', '').lower()
            self.logger.debug(f"Vulnerability: {vuln.get('name')}, Severity: {severity}")
            
            if severity in severity_counts:
                severity_counts[severity] += 1
            else:
                self.logger.warning(f"Unknown severity level: {severity} for vulnerability {vuln.get('name')}")
        
        # Update summary with accurate counts
        if 'summary' in content:
            for severity, count in severity_counts.items():
                content['summary'][severity] = count
                
            self.logger.info(f"Updated severity counts: {severity_counts}")
        else:
            self.logger.warning("No 'summary' key found in content - cannot update severity counts")
        
        return

    def _format_for_pdf(self, content: dict) -> dict:
        """
        Format report content specifically for PDF output
        
        Args:
            content: The raw report content
            
        Returns:
            dict: Report content formatted for PDF output
        """
        # Create a copy to avoid modifying the original
        pdf_content = content.copy()
        
        # Ensure summary data is accessible at both top level and within report_data
        # This addresses the template variable lookup issue
        if 'summary' in pdf_content:
            # Add a copy at the top level for direct template access
            summary = pdf_content['summary'].copy()
            
            # Ensure exploit matching data is included in the report
            if 'executive_summary' in pdf_content and 'exploit_matches' in pdf_content['executive_summary']:
                summary['exploit_matches'] = pdf_content['executive_summary']['exploit_matches']
            
            # Make severity counts accessible at top level if available
            if 'detailed_info' in pdf_content and 'vulnerability_severity' in pdf_content['detailed_info']:
                for key, value in pdf_content['detailed_info']['vulnerability_severity'].items():
                    summary[key] = value
        
        return pdf_content

    def _calculate_risk_level_from_summary(self, summary):
        """Calculate risk level based on summary data"""
        if summary.get('critical', 0) > 0:
            return 'CRITICAL'
        elif summary.get('high', 0) > 2:
            return 'HIGH'
        elif summary.get('high', 0) > 0 or summary.get('open_ports_count', 0) > 5:
            return 'MEDIUM'
        else:
            return 'LOW'

    def _format_network_data_for_pdf(self, network_data: dict) -> dict:
        """Format network visualization data for PDF output"""
        # Simplify network data to avoid rendering issues
        if not network_data:
            return {}
            
        # Limit number of nodes and links to avoid overcrowding
        if 'nodes' in network_data and len(network_data['nodes']) > 20:
            # Keep only the most important 20 nodes
            network_data['nodes'] = sorted(
                network_data['nodes'], 
                key=lambda x: self._get_node_importance(x)
            )[:20]
            
            # Keep only links between these nodes
            node_ids = {node['id'] for node in network_data['nodes']}
            network_data['links'] = [
                link for link in network_data.get('links', [])
                if link['source'] in node_ids and link['target'] in node_ids
            ]
        
        return network_data

    def _get_node_importance(self, node: dict) -> int:
        """Calculate node importance for filtering"""
        # Host nodes are most important
        if node.get('type') == 'host':
            return 100
            
        # Next subdomains
        if node.get('type') == 'subdomain':
            return 90
            
        # Important service nodes
        if node.get('type') == 'service':
            # Web services are more important
            if 'http' in node.get('name', '').lower():
                return 80
            return 70
            
        # High and critical vulnerabilities
        if node.get('type') == 'vulnerability':
            if 'critical' in node.get('name', '').lower():
                return 85
            if 'high' in node.get('name', '').lower():
                return 75
                
        # Default importance
        return 0

    # Existing methods below
    
    def generate_basic_report(self, target: str, scan_results: dict = None) -> dict:
        """Generate a basic security report with more robust handling of scan results"""
        # Fetch existing data from the database
        subdomains = Subdomain.objects.filter(domain=target)
        port_scans = PortScan.objects.filter(host=target)
        vulnerabilities = Vulnerability.objects.filter(target=target, is_fixed=False)
        open_ports = self._get_open_ports(target)

        # Use provided scan_results if available
        processed_vulns = []
        if scan_results and 'vulnerabilities' in scan_results:
            # Process incoming vulnerabilities from scan results
            vuln_count = len(scan_results['vulnerabilities'])
            self.logger.info(f"Processing {vuln_count} vulnerabilities from scan results")
            
            # Save vulnerabilities to database for reporting
            for vuln_data in scan_results['vulnerabilities']:
                try:
                    # Extract core vulnerability data
                    vuln_name = vuln_data.get('name', 'Unknown Vulnerability')
                    vuln_severity = vuln_data.get('severity', 'LOW')
                    
                    # Normalize severity
                    if isinstance(vuln_severity, str):
                        vuln_severity = vuln_severity.upper()
                    
                    # Create/update vulnerability record
                    vuln, created = Vulnerability.objects.update_or_create(
                        target=target,
                        name=vuln_name,
                        defaults={
                            'description': vuln_data.get('description', ''),
                            'severity': vuln_severity,
                            'vuln_type': vuln_data.get('type', vuln_data.get('vuln_type', 'unknown')),
                            'evidence': vuln_data.get('evidence', ''),
                            'source': vuln_data.get('source', 'scan'),
                            'confidence': vuln_data.get('confidence', 'medium'),
                            'cvss_score': vuln_data.get('cvss_score', 0.0),
                            'is_fixed': False
                        }
                    )
                    
                    # Add to processed list
                    processed_vulns.append(vuln)
                    self.logger.info(f"Processed vulnerability: {vuln_name} ({vuln_severity})")
                    
                except Exception as e:
                    self.logger.error(f"Error saving vulnerability from scan results: {str(e)}")
            
            # Refresh vulnerabilities from database to include the ones we just added
            if processed_vulns:
                self.logger.info(f"Successfully processed {len(processed_vulns)} vulnerabilities")
                # Use a combination of existing and newly added vulnerabilities
                vulnerabilities = Vulnerability.objects.filter(target=target, is_fixed=False)
                self.logger.info(f"Total vulnerabilities in database: {vulnerabilities.count()}")

        # Create the report structure
        report = {
            'target': target,
            'scan_date': datetime.now().isoformat(),
            'summary': {
                'total_subdomains': subdomains.count(),
                'total_ports_scanned': port_scans.count(),
                'total_vulnerabilities': vulnerabilities.count(),
                'open_ports_count': len(open_ports)
            },
            'subdomains': [model_to_dict(sub, exclude=['id']) for sub in subdomains],
            'open_ports': open_ports,
            'port_scan_summary': {
                'total_scanned': port_scans.count(),
                'open': port_scans.filter(state='open').count(),
                'closed': port_scans.filter(state='closed').count(),
                'filtered': port_scans.filter(state='filtered').count()
            },
            'vulnerabilities': [self._serialize_vulnerability(vuln) for vuln in vulnerabilities]
        }
        
        # Log key metrics
        self.logger.info(f"Generated basic report for {target} with {vulnerabilities.count()} vulnerabilities")
        
        return report

    def generate_detailed_report(self, target: str, scan_results: dict = None) -> dict:
        """Generate a detailed security report with exploit matches"""
        basic_report = self.generate_basic_report(target, scan_results)
        vulnerabilities = Vulnerability.objects.filter(target=target, is_fixed=False)
        
        # Enhanced port analysis
        open_ports = self._get_open_ports(target)
        port_risks = self._analyze_port_risks(open_ports)
        
        # Calculate vulnerability severity counts
        vulnerability_severity = {
            'critical': vulnerabilities.filter(severity='CRITICAL').count(),
            'high': vulnerabilities.filter(severity='HIGH').count(),
            'medium': vulnerabilities.filter(severity='MEDIUM').count(),
            'low': vulnerabilities.filter(severity='LOW').count()
        }
        
        # Calculate source counts
        vulnerability_sources = {
            'internal': vulnerabilities.filter(source='internal').count(),
            'zap': vulnerabilities.filter(source='zap').count(),
            'nuclei': vulnerabilities.filter(source='nuclei').count(),
            'multiple': vulnerabilities.exclude(source__in=['internal', 'zap', 'nuclei']).count()
        }
        
        # Add exploit matching statistics
        exploit_matching = {}
        try:
            # Try to get exploit matching info from scan results first
            if scan_results and 'exploit_matching' in scan_results:
                exploit_matching = scan_results['exploit_matching']
            else:
                # Otherwise calculate it from the database
                from exploit_manager.models import ExploitMatch
                
                # Get total matches
                total_matches = ExploitMatch.objects.filter(
                    vulnerability__target=target,
                    vulnerability__is_fixed=False
                ).count()
                
                # Get vulnerabilities with matches
                vulns_with_matches = vulnerabilities.filter(
                    exploit_matches__isnull=False
                ).distinct().count()
                
                # Get top matches by confidence
                top_matches = ExploitMatch.objects.filter(
                    vulnerability__target=target,
                    vulnerability__is_fixed=False
                ).order_by('-confidence_score')[:5]
                
                match_details = []
                for match in top_matches:
                    match_details.append({
                        'vulnerability_id': match.vulnerability.id,
                        'vulnerability_name': match.vulnerability.name,
                        'exploit_title': match.exploit.title,
                        'exploit_id': match.exploit.exploit_id,
                        'confidence': match.confidence_score,
                        'source_url': match.exploit.source_url,
                        'cve_id': match.exploit.cve_id
                    })
                
                exploit_matching = {
                    'total_vulnerabilities': vulnerabilities.count(),
                    'vulnerabilities_with_matches': vulns_with_matches,
                    'total_matches': total_matches,
                    'match_details': match_details
                }
                
        except Exception as e:
            self.logger.error(f"Error getting exploit match data: {str(e)}")
        
        detailed_info = {
            'port_analysis': {
                'high_risk_ports': port_risks['high_risk'],
                'medium_risk_ports': port_risks['medium_risk'],
                'low_risk_ports': port_risks['low_risk']
            },
            'vulnerability_severity': vulnerability_severity,
            'vulnerability_sources': vulnerability_sources,
            'exploit_matching': exploit_matching
        }
        
        # Log key metrics
        self.logger.info(f"Vulnerability severity counts: {vulnerability_severity}")
        if exploit_matching:
            self.logger.info(f"Exploit matching: {exploit_matching.get('total_matches', 0)} matches for {exploit_matching.get('vulnerabilities_with_matches', 0)} vulnerabilities")
        
        return {**basic_report, 'detailed_info': detailed_info}

    def generate_executive_report(self, target: str, scan_results: dict = None) -> dict:
        """Generate an executive summary report with exploit matches"""
        basic_report = self.generate_basic_report(target, scan_results)
        vulnerabilities = Vulnerability.objects.filter(target=target, is_fixed=False)
        high_vulns = vulnerabilities.filter(severity='HIGH')
        critical_vulns = vulnerabilities.filter(severity='CRITICAL')
        
        # Enhanced metrics
        risk_metrics = {
            'critical_severity_vulns': critical_vulns.count(),
            'high_severity_vulns': high_vulns.count(),
            'open_ports': len(basic_report['open_ports']),
            'total_vulnerabilities': vulnerabilities.count(),
            'high_risk_ports': len(self._analyze_port_risks(basic_report['open_ports'])['high_risk'])
        }
        
        # Get exploit matching information
        exploit_data = {}
        try:
            # Try to get exploit matching info from scan results first
            if scan_results and 'exploit_matching' in scan_results:
                exploit_data = scan_results['exploit_matching']
            else:
                # Otherwise calculate it from the database
                from exploit_manager.models import ExploitMatch
                
                # Count matches
                total_matches = ExploitMatch.objects.filter(
                    vulnerability__target=target,
                    vulnerability__is_fixed=False
                ).count()
                
                # Count vulnerabilities with matches
                vulns_with_matches = vulnerabilities.filter(
                    exploit_matches__isnull=False
                ).distinct().count()
                
                exploit_data = {
                    'total_matches': total_matches,
                    'vulnerabilities_with_matches': vulns_with_matches
                }
            
            # Add to risk metrics
            if exploit_data:
                risk_metrics['vulnerabilities_with_exploits'] = exploit_data.get('vulnerabilities_with_matches', 0)
                risk_metrics['total_exploit_matches'] = exploit_data.get('total_matches', 0)
        except Exception as e:
            self.logger.error(f"Error getting exploit match data for executive report: {str(e)}")
        
        # Combine critical and high vulnerabilities in findings
        top_findings = []
        for vuln in critical_vulns:
            top_findings.append(self._serialize_vulnerability(vuln))
        if len(top_findings) < 5:  # Limit to 5 findings total
            for vuln in high_vulns[:5-len(top_findings)]:
                top_findings.append(self._serialize_vulnerability(vuln))
        
        # Get top exploit matches
        top_exploit_matches = []
        try:
            if 'match_details' in exploit_data:
                top_exploit_matches = exploit_data['match_details']
            else:
                # Get from database if not in scan results
                from exploit_manager.models import ExploitMatch
                matches = ExploitMatch.objects.filter(
                    vulnerability__target=target,
                    vulnerability__is_fixed=False,
                    confidence_score__gte=0.4  # Only high confidence matches for executive report
                ).order_by('-confidence_score')[:3]
                
                for match in matches:
                    top_exploit_matches.append({
                        'vulnerability_name': match.vulnerability.name,
                        'exploit_title': match.exploit.title,
                        'confidence': match.confidence_score,
                        'cve_id': match.exploit.cve_id or "None",
                        'source_url': match.exploit.source_url
                    })
        except Exception as e:
            self.logger.error(f"Error getting top exploit matches: {str(e)}")
        
        executive_summary = {
            'risk_level': self._calculate_risk_level(risk_metrics),
            'critical_findings': top_findings,
            'risk_metrics': risk_metrics,
            'recommendations': self._generate_recommendations(risk_metrics, basic_report),
            'exploit_matches': top_exploit_matches
        }
        
        # Add extra recommendations for exploits if needed
        if risk_metrics.get('vulnerabilities_with_exploits', 0) > 0:
            executive_summary['recommendations'].insert(0, {
                'title': 'Address Vulnerabilities with Known Exploits',
                'description': f"Fix the {risk_metrics.get('vulnerabilities_with_exploits', 0)} vulnerabilities that have known public exploits as highest priority."
            })
        
        # Log key metrics
        self.logger.info(f"Executive report metrics: Critical={critical_vulns.count()}, High={high_vulns.count()}, Total={vulnerabilities.count()}")
        self.logger.info(f"Executive report exploit matches: {risk_metrics.get('total_exploit_matches', 0)}")
        
        return {**basic_report, 'executive_summary': executive_summary}

    def _serialize_port_scan(self, port_scan):
        """Serialize port scan data"""
        return {
            'port': port_scan.port,
            'service': port_scan.service,
            'state': port_scan.state,
            'protocol': port_scan.protocol,
            'banner': port_scan.banner if port_scan.banner else '',
            'scan_date': port_scan.scan_date.isoformat()
        }

    def _get_open_ports(self, target: str) -> list:
        """Get all open ports with details"""
        open_ports = PortScan.objects.filter(
            host=target,
            state='open'
        ).order_by('port')
        return [self._serialize_port_scan(port) for port in open_ports]

    def _analyze_port_risks(self, open_ports: list) -> dict:
        """Analyze risks associated with open ports"""
        high_risk_ports = [21, 23, 445, 3389]  # FTP, Telnet, SMB, RDP
        medium_risk_ports = [22, 25, 110, 143]  # SSH, SMTP, POP3, IMAP
        
        port_risks = {
            'high_risk': [],
            'medium_risk': [],
            'low_risk': []
        }
        
        for port_data in open_ports:
            port = port_data['port']
            if port in high_risk_ports:
                port_risks['high_risk'].append(port_data)
            elif port in medium_risk_ports:
                port_risks['medium_risk'].append(port_data)
            else:
                port_risks['low_risk'].append(port_data)
                
        return port_risks

    def _calculate_risk_level(self, metrics):
        """Calculate overall risk level based on metrics"""
        if metrics['critical_severity_vulns'] > 0:
            return 'CRITICAL'
        elif metrics['high_severity_vulns'] > 2:
            return 'HIGH'
        elif metrics['high_severity_vulns'] > 0 or metrics['high_risk_ports'] > 0:
            return 'MEDIUM'
        else:
            return 'LOW'
    
    def _generate_recommendations(self, metrics, basic_report):
        """Generate security recommendations based on findings"""
        recommendations = []
        
        # Add recommendations based on findings
        if metrics['critical_severity_vulns'] > 0 or metrics['high_severity_vulns'] > 0:
            recommendations.append({
                'title': 'Address High and Critical Vulnerabilities',
                'description': 'Fix identified critical and high vulnerabilities as a priority.'
            })
            
        if metrics['high_risk_ports'] > 0:
            recommendations.append({
                'title': 'Secure High Risk Ports',
                'description': 'Restrict access to high risk services or replace with more secure alternatives.'
            })
            
        # Always add general recommendations
        recommendations.append({
            'title': 'Regular Security Testing',
            'description': 'Perform regular security assessments to identify new vulnerabilities.'
        })
        
        return recommendations
    
# File: reporting/report_generator.py
# In the _serialize_vulnerability method, add exploit match information

# File: reporting/report_generator.py
# In the _serialize_vulnerability method, add exploit match information

    def _serialize_vulnerability(self, vuln):
        """
        Properly serialize a vulnerability instance with correct ID handling for exploits
        
        Args:
            vuln: Vulnerability model instance
            
        Returns:
            dict: Serialized vulnerability data with exploit details
        """
        # Clean up description and evidence using helper methods
        description = self._clean_description(vuln.description)
        evidence = self._clean_evidence(vuln.evidence)
        
        # Get exploit matches with proper ID handling for links
        exploit_matches = []
        try:
            from exploit_manager.models import ExploitMatch
            matches = ExploitMatch.objects.filter(vulnerability=vuln)
            
            # Log the number of matches found for debugging
            match_count = matches.count()
            self.logger.info(f"Found {match_count} exploit matches for vulnerability {vuln.id}")
            
            if match_count > 0:
                for match in matches:
                    exploit = match.exploit
                    # Include both database ID and exploit_id to support proper linking
                    exploit_data = {
                        'id': exploit.id,  # Database ID for URL construction
                        'exploit_id': exploit.exploit_id,  # ExploitDB ID for reference
                        'title': exploit.title,
                        'description': exploit.description[:100] + '...' if len(exploit.description) > 100 else exploit.description,
                        'confidence': match.confidence_score,
                        'status': match.status,
                        'source_url': exploit.source_url,
                        'cve_id': exploit.cve_id or "None"
                    }
                    exploit_matches.append(exploit_data)
                    self.logger.debug(f"Added exploit match: Database ID={exploit.id}, ExploitDB ID={exploit.exploit_id}")
        except Exception as e:
            self.logger.error(f"Error retrieving exploit matches for vulnerability {vuln.id}: {str(e)}")
        
        # Build the complete vulnerability data object
        vuln_data = {
            'id': vuln.id,
            'target': vuln.target,
            'name': vuln.name,
            'description': description,
            'severity': vuln.severity,
            'vuln_type': vuln.vuln_type,
            'type': vuln.vuln_type,  # Duplicate field for template compatibility
            'evidence': evidence,
            'discovery_date': vuln.discovery_date.isoformat(),
            'is_fixed': vuln.is_fixed,
            'fix_date': vuln.fix_date.isoformat() if vuln.fix_date else None,
            'source': vuln.source,
            'confidence': vuln.confidence,
            'cvss_score': vuln.cvss_score,
            'solution': vuln.solution if vuln.solution else '',
            'references': list(vuln.references) if vuln.references else [],
            'cwe': vuln.cwe if vuln.cwe else '',
            'metadata': dict(vuln.metadata) if vuln.metadata else {},
            'exploit_matches': exploit_matches  # Include properly formatted exploit matches
        }
        
        return vuln_data
# File: reporting/report_generator.py

    def _clean_description(self, description):
        """Clean up repetitive content in descriptions"""
        if not description:
            return ""
        
        # Remove prefixes like "zap:" that appear at the beginning
        if description.startswith("zap:"):
            description = description[4:].strip()
        
        # Split by paragraphs first (handles cases like the CSP text)
        paragraphs = description.split('\n\n')
        
        # Store unique paragraphs preserving order
        unique_paragraphs = []
        seen_paragraphs = set()
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            # Use the first 100 chars as a fingerprint to identify similar paragraphs
            fingerprint = paragraph[:100].lower()
            
            if fingerprint not in seen_paragraphs:
                unique_paragraphs.append(paragraph)
                seen_paragraphs.add(fingerprint)
        
        # For each paragraph, clean duplicate sentences
        cleaned_paragraphs = []
        for paragraph in unique_paragraphs:
            # Split by sentences
            sentences = paragraph.split('. ')
            
            # Remove duplicate sentences
            unique_sentences = []
            seen_sentences = set()
            
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                
                # Create a fingerprint for the sentence
                fingerprint = sentence.lower()
                
                # Only add if not already seen
                if fingerprint not in seen_sentences:
                    unique_sentences.append(sentence)
                    seen_sentences.add(fingerprint)
            
            # Rejoin sentences
            cleaned_paragraph = '. '.join(unique_sentences)
            if not cleaned_paragraph.endswith('.'):
                cleaned_paragraph += '.'
            
            cleaned_paragraphs.append(cleaned_paragraph)
        
        # Join the cleaned paragraphs, limit to a reasonable length
        result = '\n\n'.join(cleaned_paragraphs)
        
        # If still too long, truncate with an indicator
        if len(result) > 2000:
            result = result[:1997] + '...'
            
        return result

    def _clean_evidence(self, evidence):
        """Clean up repetitive content in evidence"""
        if not evidence:
            return ""
        
        # Split by newlines
        lines = evidence.split('\n')
        
        # Store unique evidence items with counts
        unique_lines = []
        seen_patterns = {}
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Extract the pattern (e.g., "nginx/1.19.0" from "zap: nginx/1.19.0")
            if ': ' in line:
                prefix, pattern = line.split(': ', 1)
                processed_line = f"{prefix}: {pattern}"
            else:
                prefix, pattern = "", line
                processed_line = line
            
            # Use lowercase for matching but preserve original case for display
            pattern_key = pattern.lower()
            
            # Track this pattern
            if pattern_key in seen_patterns:
                seen_patterns[pattern_key]['count'] += 1
                
                # Only keep a maximum of 2 examples per pattern
                if seen_patterns[pattern_key]['count'] <= 2:
                    unique_lines.append(processed_line)
            else:
                seen_patterns[pattern_key] = {
                    'count': 1,
                    'line': processed_line
                }
                unique_lines.append(processed_line)
        
        # Add counts for patterns with more than 2 occurrences
        result_lines = []
        
        # First add all the unique lines we want to keep
        for line in unique_lines:
            result_lines.append(line)
        
        # Then add summary counts for patterns with more occurrences
        for pattern, info in seen_patterns.items():
            if info['count'] > 2:
                extra_count = info['count'] - 2
                if extra_count > 0:
                    # Extract prefix from the line to maintain consistency
                    if ': ' in info['line']:
                        prefix = info['line'].split(': ', 1)[0]
                        result_lines.append(f"{prefix}: ... and {extra_count} more similar items")
                    else:
                        result_lines.append(f"... and {extra_count} more similar items")
        
        # Rejoin lines, limit overall size
        result = '\n'.join(result_lines)
        
        # If still too long, truncate
        if len(result) > 500:
            result = result[:497] + '...'
            
        return result
        
    def _similarity(self, str1, str2):
        """Calculate similarity between two strings"""
        # Simple similarity check based on word overlap
        words1 = set(str1.lower().split())
        words2 = set(str2.lower().split())
        
        if not words1 or not words2:
            return 0.0
            
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)# automation/management/commands/run_automation.py

import time
import logging
from django.core.management.base import BaseCommand
from django.db import connection
from django.utils import timezone
from datetime import datetime, timedelta

from automation.workflow_orchestrator import WorkflowOrchestrator
from automation.scheduler import ScanScheduler

logger = logging.getLogger(__name__)

class Command(BaseCommand):
    help = 'Run the security automation system with scheduling and workflow processing'

    def add_arguments(self, parser):
        parser.add_argument(
            '--daemon', 
            action='store_true',
            help='Run continuously in daemon mode'
        )
        parser.add_argument(
            '--interval', 
            type=int,
            default=60,
            help='Interval in seconds between daemon runs'
        )
        parser.add_argument(
            '--one-time',
            action='store_true',
            help='Process all current pending workflows and scheduled tasks, then exit'
        )
        parser.add_argument(
            '--only-scheduler',
            action='store_true',
            help='Only run the scheduler component'
        )
        parser.add_argument(
            '--only-workflows',
            action='store_true',
            help='Only process pending workflows'
        )

    def handle(self, *args, **options):
        daemon_mode = options.get('daemon', False)
        interval = options.get('interval', 60)
        one_time = options.get('one_time', False)
        only_scheduler = options.get('only_scheduler', False)
        only_workflows = options.get('only_workflows', False)
        
        # Initialize components
        orchestrator = WorkflowOrchestrator()
        scheduler = ScanScheduler()
        
        self.stdout.write(self.style.SUCCESS(f"Starting security automation system"))
        
        if daemon_mode:
            self.stdout.write(f"Running in daemon mode with {interval} second interval")
            self._run_daemon(orchestrator, scheduler, interval, only_scheduler, only_workflows)
        elif one_time:
            self.stdout.write("Running in one-time mode")
            self._run_once(orchestrator, scheduler, only_scheduler, only_workflows)
        else:
            self.stdout.write("Running in one-time mode (default)")
            self._run_once(orchestrator, scheduler, only_scheduler, only_workflows)
    
    def _run_once(self, orchestrator, scheduler, only_scheduler, only_workflows):
        """Run the automation components once and exit"""
        try:
            # Process scheduled tasks
            if not only_workflows:
                scheduled_tasks = scheduler.process_scheduled_tasks()
                self.stdout.write(f"Processed {scheduled_tasks} scheduled tasks")
            
            # Process workflows
            if not only_scheduler:
                # Start pending workflows
                started_workflows = orchestrator.check_pending_workflows()
                self.stdout.write(f"Started {started_workflows} pending workflows")
                
                # Process workflow queue
                processed_tasks = orchestrator.process_workflow_queue()
                self.stdout.write(f"Processed {processed_tasks} workflow tasks")
                
        except Exception as e:
            self.stderr.write(self.style.ERROR(f"Error in automation system: {str(e)}"))
        finally:
            # Close database connections
            connection.close()
    
    def _run_daemon(self, orchestrator, scheduler, interval, only_scheduler, only_workflows):
        """Run the automation components in daemon mode"""
        try:
            while True:
                start_time = time.time()
                self.stdout.write(f"Running automation cycle at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                
                try:
                    # Process scheduled tasks
                    if not only_workflows:
                        scheduled_tasks = scheduler.process_scheduled_tasks()
                        if scheduled_tasks > 0:
                            self.stdout.write(f"Processed {scheduled_tasks} scheduled tasks")
                    
                    # Process workflows
                    if not only_scheduler:
                        # Start pending workflows
                        started_workflows = orchestrator.check_pending_workflows()
                        if started_workflows > 0:
                            self.stdout.write(f"Started {started_workflows} pending workflows")
                        
                        # Process workflow queue
                        processed_tasks = orchestrator.process_workflow_queue()
                        if processed_tasks > 0:
                            self.stdout.write(f"Processed {processed_tasks} workflow tasks")
                    
                except Exception as e:
                    self.stderr.write(self.style.ERROR(f"Error in automation cycle: {str(e)}"))
                finally:
                    # Close database connections
                    connection.close()
                
                # Calculate sleep time to maintain interval
                elapsed = time.time() - start_time
                sleep_time = max(0, interval - elapsed)
                
                if sleep_time > 0:
                    self.stdout.write(f"Sleeping for {sleep_time:.2f} seconds...")
                    time.sleep(sleep_time)
                
        except KeyboardInterrupt:
            self.stdout.write(self.style.SUCCESS("\nAutomation system gracefully stopped"))# File: exploit_manager/management/commands/sync_exploits.py
from django.core.management.base import BaseCommand
from exploit_manager.exploit_db import ExploitDBManager

class Command(BaseCommand):
    help = 'Sync recent exploits from ExploitDB'

    def add_arguments(self, parser):
        parser.add_argument('--limit', type=int, default=100, help='Maximum number of exploits to sync')

    def handle(self, *args, **options):
        limit = options['limit']
        
        manager = ExploitDBManager()
        stats = manager.sync_recent_exploits(limit=limit)
        
        self.stdout.write(self.style.SUCCESS(
            f"Synced {stats['new']} new exploits, updated {stats['updated']} existing exploits, "
            f"failed: {stats['failed']}"
        ))# File: vulnerability/management/commands/deduplicate_vulnerabilities.py
from django.core.management.base import BaseCommand
from vulnerability.models import Vulnerability

class Command(BaseCommand):
    help = 'Remove duplicate vulnerabilities'

    def add_arguments(self, parser):
        parser.add_argument('--target', type=str, help='Optional target to filter vulnerabilities')

    def handle(self, *args, **options):
        target = options.get('target')
        
        # Run deduplication
        removed = Vulnerability.remove_duplicates(target)
        
        self.stdout.write(self.style.SUCCESS(
            f"Removed {removed} duplicate vulnerabilities"
        ))