# File to be updated: vulnerability/zap_manager.py

import docker 
import logging
import time
from django.conf import settings
import requests
from typing import Dict, List, Optional, Set
from zapv2 import ZAPv2
import re
from urllib.parse import urlparse

class ZAPManager:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.client = docker.from_env()
        self.container_name = 'security_platform_zap'
        self.api_key = getattr(settings, 'ZAP_API_KEY', 'change_me_please')
        self.zap_host = getattr(settings, 'ZAP_HOST', 'localhost')
        self.zap_port = getattr(settings, 'ZAP_PORT', 8080)
        self.zap_address = f"http://{self.zap_host}:{self.zap_port}"
        self.zap = None

    def ensure_zap_running(self) -> bool:
        """Ensure ZAP container is running, start if not"""
        try:
            container = self.client.containers.get(self.container_name)
            if container.status != 'running':
                self.logger.info(f"ZAP container found but not running. Starting container {self.container_name}")
                container.start()
                time.sleep(10)  # Wait for ZAP to initialize
            else:
                self.logger.info(f"ZAP container is already running: {self.container_name}")
            
            return self._initialize_zap()
            
        except docker.errors.NotFound:
            self.logger.info("ZAP container not found, starting new one")
            return self._start_zap_container()
        except Exception as e:
            self.logger.error(f"Error ensuring ZAP is running: {str(e)}")
            return False

    def _start_zap_container(self) -> bool:
        """Start a new ZAP container"""
        try:
            self.logger.info(f"Starting new ZAP container with name: {self.container_name}")
            self.client.containers.run(
                'ghcr.io/zaproxy/zaproxy:stable',
                command=f'zap.sh -daemon -host 0.0.0.0 -port 8080 -config api.addrs.addr.name=.* -config api.addrs.addr.regex=true -config api.key={self.api_key}',
                ports={'8080/tcp': self.zap_port},
                name=self.container_name,
                detach=True
            )
            # Increase wait time to ensure ZAP is fully initialized
            self.logger.info("Waiting for ZAP to initialize...")
            time.sleep(15)  
            return self._initialize_zap()
        except Exception as e:
            self.logger.error(f"Error starting ZAP container: {str(e)}")
            return False

    def _initialize_zap(self) -> bool:
        """Initialize ZAP API connection"""
        try:
            self.logger.info(f"Initializing ZAP API connection to {self.zap_address}")
            self.zap = ZAPv2(
                apikey=self.api_key,
                proxies={'http': self.zap_address, 'https': self.zap_address}
            )
            version = self.zap.core.version
            self.logger.info(f"Successfully connected to ZAP API, version: {version}")
            return True
        except Exception as e:
            self.logger.error(f"Error initializing ZAP: {str(e)}")
            return False

    def get_status(self) -> Dict:
        """Get ZAP status"""
        status = {
            'available': False,
            'version': None,
            'error': None
        }

        try:
            if not self.zap:
                self._initialize_zap()
            
            if self.zap:
                version = self.zap.core.version
                status.update({
                    'available': True,
                    'version': version
                })
                self.logger.info(f"ZAP status: available, version {version}")
            else:
                self.logger.warning("ZAP not initialized, reporting as unavailable")
        except Exception as e:
            self.logger.error(f"Error getting ZAP status: {str(e)}")
            status['error'] = str(e)

        return status

    def _normalize_url(self, url: str) -> str:
        """Normalize URL to ensure it has a protocol prefix"""
        if not url:
            return ""
            
        # Strip any trailing slashes for consistency
        url = url.rstrip('/')
        
        # Check if URL already has a protocol
        if url.startswith(('http://', 'https://')):
            return url
        
        # Add http:// as default protocol
        return f"http://{url}"
    
    def _extract_hostname(self, url: str) -> str:
        """Extract hostname from URL in a robust way"""
        try:
            # Use urlparse to properly handle URL components
            parsed = urlparse(url)
            # If netloc is empty, the URL might not have a protocol
            if not parsed.netloc:
                # Try adding a protocol and parsing again
                parsed = urlparse(f"http://{url}")
            
            # Return just the hostname without port if present
            hostname = parsed.netloc
            if ':' in hostname:
                hostname = hostname.split(':', 1)[0]
                
            return hostname
        except Exception as e:
            self.logger.error(f"Error extracting hostname from {url}: {str(e)}")
            # If parsing fails, try a simpler approach as fallback
            match = re.search(r'(?:https?://)?([^:/]+)', url)
            if match:
                return match.group(1)
            return url

    def run_scan(self, target: str) -> Dict:
        """
        Run an optimized ZAP scan with smart targeting and faster processing
        """
        try:
            if not self.zap:
                self.logger.info("ZAP not initialized, attempting to initialize")
                if not self._initialize_zap():
                    raise Exception("Could not initialize ZAP")

            # Normalize the target URL - ensure it has a protocol
            original_target = target
            normalized_target = self._normalize_url(target)
            
            # Extract the hostname for context and filtering
            hostname = self._extract_hostname(normalized_target)
            self.logger.info(f"Extracted hostname: {hostname} from target: {normalized_target}")
            
            # Log the start of the scan
            self.logger.info(f"Starting scan for {normalized_target}")

            # Clear previous session
            self.logger.info("Creating new ZAP session")
            self.zap.core.new_session()
            time.sleep(2)

            # Create a new context
            context_name = "scan_context"
            try:
                self.zap.context.remove_context(context_name)
            except:
                pass
                
            self.logger.info("Configuring scan context")
            context_id = self.zap.context.new_context(context_name)
            
            # Make the context pattern more permissive to include all sub-paths
            context_pattern = f".*{hostname}.*"
            self.zap.context.include_in_context(context_name, context_pattern)
            
            # Try to access the primary URL
            self.logger.info(f"Accessing target URL: {normalized_target}")
            self.zap.core.access_url(normalized_target)
            time.sleep(3)
            
            # FASTER URL DISCOVERY - Start by looking at site tree
            site_urls = self._get_site_urls(hostname)
            
            # Only use heavier discovery if needed
            if not site_urls or len(site_urls) < 5:
                self.logger.info("Limited URLs found, using targeted discovery")
                # Add common paths directly
                site_urls = self._discover_high_value_urls(normalized_target, hostname)
            
            self.logger.info(f"Discovered {len(site_urls)} URLs for scanning")
            
            # SMARTER SCANNING - Focus on high-value targets
            scan_urls = self._prioritize_urls_for_scanning(site_urls, hostname)
            
            # Process URLs in batches for better performance
            batch_size = 5
            batches = [scan_urls[i:i+batch_size] for i in range(0, len(scan_urls), batch_size)]
            
            scan_id = None
            url_count = 0
            
            # Process each batch
            for batch in batches:
                for url in batch:
                    try:
                        # Access the URL to ensure it's in the site tree
                        self.zap.core.access_url(url)
                        url_count += 1
                    except Exception as e:
                        self.logger.debug(f"Error accessing {url}: {str(e)}")
                
                # Use a single scan for the batch
                try:
                    # Use only the most basic scan parameters
                    if batches.index(batch) == 0:  # First batch gets the active scan
                        scan_id = self.zap.ascan.scan(normalized_target)
                        self.logger.info(f"Started active scan with ID {scan_id}")
                except Exception as e:
                    self.logger.warning(f"Error starting active scan: {str(e)}")
            
            self.logger.info(f"Processed {url_count} URLs for scanning")
            
            # Wait for active scan with increased timeout but better completion logic
            if scan_id:
                scan_complete = self._wait_for_efficient_scan_completion(scan_id)
                if scan_complete:
                    self.logger.info("Active scan completed successfully")
                else:
                    self.logger.warning("Active scan did not complete, but continuing with results")
            
            # Get scan results with improved handling
            self.logger.info("Collecting scan results")
            alerts = []
            try:
                # Try with the normalized target first
                alerts = self.zap.core.alerts(baseurl=normalized_target)
                self.logger.info(f"Found {len(alerts)} alerts for {normalized_target}")
                
                # If no results, try wider search
                if not alerts:
                    # Try getting all alerts and filter by hostname
                    all_alerts = self.zap.core.alerts()
                    alerts = [a for a in all_alerts if hostname in a.get('url', '')]
                    self.logger.info(f"Retrieved {len(alerts)} alerts by filtering all results")
                    
                # Process alerts to remove duplicate descriptions
                alerts = self._deduplicate_alert_descriptions(alerts)
            except Exception as e:
                self.logger.error(f"Error getting alerts: {str(e)}")
            
            return {
                'status': 'success',
                'alerts': alerts,
                'scan_details': {
                    'hostname': hostname,
                    'normalized_target': normalized_target,
                    'urls_discovered': len(site_urls),
                    'urls_processed': url_count,
                    'scan_id': scan_id
                },
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }

        except Exception as e:
            self.logger.error(f"ZAP scan failed: {str(e)}")
            return {
                'status': 'error',
                'error': str(e)
            }

    def _discover_high_value_urls(self, target: str, hostname: str) -> List[str]:
        """Quickly discover high-value URLs without heavy crawling"""
        urls = set()
        
        # Start with the target itself
        urls.add(target)
        
        # Add common high-value paths
        base_url = target.rstrip('/')
        common_paths = [
            '/login', '/admin', '/dashboard', '/account', '/profile',
            '/user', '/api', '/about', '/contact', '/sitemap.xml',
            '/robots.txt', '/search', '/index.php', '/main', '/home',
            '/register', '/signup', '/auth', '/forgot-password',
            '/reset-password', '/settings', '/config', '/help'
        ]
        
        for path in common_paths:
            urls.add(f"{base_url}{path}")
        
        # Try to get any existing URLs from ZAP site tree
        try:
            site_tree_urls = self._get_site_urls(hostname)
            urls.update(site_tree_urls)
        except Exception as e:
            self.logger.debug(f"Error getting URLs from site tree: {str(e)}")
        
        # Try basic URL extraction from page content
        try:
            content_urls = self._extract_urls_from_content(target, hostname)
            urls.update(content_urls)
        except Exception as e:
            self.logger.debug(f"Error extracting URLs from content: {str(e)}")
        
        return list(urls)

    def _prioritize_urls_for_scanning(self, urls: List[str], hostname: str) -> List[str]:
        """Prioritize URLs for scanning based on security relevance"""
        if not urls:
            return []
            
        high_value = []
        medium_value = []
        low_value = []
        
        # Patterns indicating high-value security targets
        high_patterns = [
            r'/login', r'/admin', r'/dashboard', r'/account', r'/profile',
            r'/user', r'/auth', r'/signup', r'/register', r'/settings',
            r'/password', r'/reset', r'/token', r'/session'
        ]
        
        # Patterns indicating medium-value targets
        medium_patterns = [
            r'/api', r'/search', r'\?', r'=', r'\.php', r'\.asp',
            r'\.jsp', r'/upload', r'/download', r'/file', r'/contact'
        ]
        
        # Categorize URLs
        for url in urls:
            if any(re.search(pattern, url, re.I) for pattern in high_patterns):
                high_value.append(url)
            elif any(re.search(pattern, url, re.I) for pattern in medium_patterns):
                medium_value.append(url)
            else:
                low_value.append(url)
        
        # Build prioritized list - all high, some medium, few low
        prioritized = high_value.copy()
        
        # Take at most 10 medium value URLs
        medium_sample = medium_value[:min(10, len(medium_value))]
        prioritized.extend(medium_sample)
        
        # Take at most 5 low value URLs
        low_sample = low_value[:min(5, len(low_value))]
        prioritized.extend(low_sample)
        
        # Ensure the main domain is included
        main_url = f"https://{hostname}"
        alt_main_url = f"http://{hostname}"
        
        if main_url not in prioritized and alt_main_url not in prioritized:
            prioritized.insert(0, main_url)
        
        # Limit to at most 20 URLs for efficient scanning
        return prioritized[:20]

    def _extract_urls_from_content(self, url: str, hostname: str) -> Set[str]:
        """Extract URLs from page content without spidering"""
        urls = set()
        try:
            # Get page content through ZAP
            response = self.zap.core.send_request(
                request=f"GET {url} HTTP/1.1\r\nHost: {hostname}\r\n\r\n"
            )
            
            # Extract URLs using regex pattern
            url_pattern = re.compile(r'href=[\'"]?([^\'" >]+)', re.IGNORECASE)
            src_pattern = re.compile(r'src=[\'"]?([^\'" >]+)', re.IGNORECASE)
            
            # Add found URLs
            if response and 'responseBody' in response:
                body = response.get('responseBody', '')
                matches = url_pattern.findall(body) + src_pattern.findall(body)
                
                for match in matches:
                    # Handle relative URLs
                    if match.startswith('/'):
                        full_url = f"{url.rstrip('/')}{match}"
                        urls.add(full_url)
                    # Handle absolute URLs with same hostname
                    elif hostname in match:
                        urls.add(match)
        except Exception as e:
            self.logger.debug(f"Error extracting URLs from content: {str(e)}")
        
        return urls
    
    def _get_site_urls(self, hostname: str) -> List[str]:
        """Get URLs for a specific hostname from the site tree"""
        try:
            all_urls = self.zap.core.urls()
            if all_urls:
                return [url for url in all_urls if hostname in url]
            return []
        except Exception as e:
            self.logger.error(f"Error getting URLs: {str(e)}")
            return []

    def _deduplicate_alert_descriptions(self, alerts: List[Dict]) -> List[Dict]:
        """
        Extremely aggressive deduplication for ZAP findings, especially for CSP evidence
        """
        for alert in alerts:
            # Handle evidence field (the main problem area)
            evidence = alert.get('evidence', '')
            if evidence and evidence.startswith('zap: '):
                # Special handling for CSP and other repeating patterns
                if '\n\nzap: ' in evidence:
                    # Split the evidence into parts
                    parts = evidence.split('\n\nzap: ')
                    first_part = parts[0].replace('zap: ', '')  # Clean up the first part
                    occurrences = len(parts)
                    
                    # Replace the entire evidence with just the first occurrence plus count
                    alert['evidence'] = f"zap: {first_part}"
                    if occurrences > 1:
                        alert['evidence'] += f"\n\n[+ {occurrences-1} identical entries omitted]"
            
            # Handle description field
            description = alert.get('description', '')
            if description and description.startswith('zap: '):
                # Split description by paragraphs
                paragraphs = description.split('\n\n')
                
                # Get unique paragraphs preserving order
                seen = set()
                unique_paragraphs = []
                for p in paragraphs:
                    normalized = p.strip()
                    if normalized and normalized not in seen:
                        seen.add(normalized)
                        unique_paragraphs.append(p)
                
                # Replace with deduplicated content
                if len(unique_paragraphs) < len(paragraphs):
                    # Use at most first 3 unique paragraphs
                    shortened = unique_paragraphs[:min(3, len(unique_paragraphs))]
                    if len(unique_paragraphs) > 3:
                        alert['description'] = '\n\n'.join(shortened) + f"\n\n[+ {len(unique_paragraphs) - 3} additional paragraphs omitted]"
                    else:
                        alert['description'] = '\n\n'.join(shortened)
            
        return alerts

    def _normalize_content(self, content: str) -> str:
        """Helper method to normalize content for comparison"""
        # Remove all whitespace variations and convert to lowercase
        return re.sub(r'\s+', '', content.lower())

    def _wait_for_efficient_scan_completion(self, scan_id, max_timeout=900):
        """
        Wait for active scan completion with smarter timeout handling and
        better progress detection for faster scanning
        """
        try:
            start_time = time.time()
            last_progress = 0
            steady_progress_count = 0
            rapid_progress = False
            progress_samples = []
            
            # Dynamic timeout adjustment based on early progress
            timeout = max_timeout
            progress_check_interval = 5  # seconds
            
            while True:
                try:
                    # Check current progress
                    status = self.zap.ascan.status(scan_id)
                    progress = int(status) if isinstance(status, int) else int(status) if status.isdigit() else 0
                    
                    # Track progress samples for rate analysis
                    elapsed = time.time() - start_time
                    progress_samples.append((elapsed, progress))
                    if len(progress_samples) > 10:
                        progress_samples.pop(0)
                    
                    # Check for completion
                    if progress >= 100:
                        self.logger.info(f"Active scan completed (100%)")
                        return True
                    
                    # Log progress changes with direction indicator
                    if progress != last_progress:
                        direction = "↑" if progress > last_progress else "↓"
                        self.logger.info(f"Active scan progress: {progress}% {direction} (elapsed: {elapsed:.1f}s)")
                        last_progress = progress
                        steady_progress_count = 0
                        
                        # Check for rapid early progress (>50% within first 3 minutes)
                        if elapsed < 180 and progress > 50:
                            rapid_progress = True
                    else:
                        steady_progress_count += 1
                    
                    # Early success detection
                    if len(progress_samples) >= 5:
                        # If we reach 85%+ and progress stalls, consider it good enough
                        if progress >= 85 and steady_progress_count >= 5:
                            self.logger.info(f"Scan at {progress}% with no changes for {steady_progress_count * progress_check_interval}s - considering successful")
                            return True
                            
                        # Rapid progress scenario - if we're moving fast and hit a high percentage
                        if rapid_progress and progress >= 75:
                            # Check if we've found significant alerts already
                            alerts_count = len(self.zap.core.alerts())
                            if alerts_count > 10:
                                self.logger.info(f"Rapid scan progress ({progress}%) with {alerts_count} alerts found - considering successful")
                                return True
                    
                    # Check timeout with dynamic adjustment
                    if elapsed > timeout:
                        self.logger.warning(f"Scan timed out after {elapsed:.1f}s at {progress}%")
                        # If progress is high, consider it successful anyway
                        return progress >= 60
                        
                    # Sleep before next check
                    time.sleep(progress_check_interval)
                    
                except Exception as e:
                    self.logger.error(f"Error checking scan status: {str(e)}")
                    if time.time() - start_time > timeout:
                        return False
                    time.sleep(progress_check_interval)
                    
        except Exception as e:
            self.logger.error(f"Error waiting for scan completion: {str(e)}")
            return False