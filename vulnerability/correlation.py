# vulnerability/correlation.py

from typing import List, Dict, Tuple, Set, Optional
import logging
from .models import Vulnerability
from datetime import datetime
import hashlib
import re
from django.db import transaction

class VulnerabilityCorrelator:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # Correlation thresholds
        self.title_similarity_threshold = 0.8  # Title similarity threshold
        self.description_similarity_threshold = 0.6  # Description similarity threshold
        
        # Define correlation rules
        self.correlation_rules = [
            self._correlation_by_target_and_name,  # Match by normalized target and name
            self._correlation_by_name_and_type,    # Exact name + type match
            self._correlation_by_fingerprint,      # Using vulnerability fingerprint
            self._correlation_by_cve,              # By CVE/reference match
            self._correlation_by_similarity        # By content similarity
        ]

    def _normalize_target(self, target: str) -> str:
        """Normalize target URL to consistent format"""
        if not target:
            return ""
            
        # Remove protocol prefix
        if '://' in target:
            target = target.split('://', 1)[1]
        
        # Remove path, trailing slash, etc.
        if '/' in target:
            target = target.split('/', 1)[0]
            
        # Remove port if present
        if ':' in target:
            target = target.split(':', 1)[0]
            
        # Remove 'www.' prefix if present
        if target.startswith('www.'):
            target = target[4:]
            
        return target.lower()

    def correlate_findings(self, 
                           internal_results: List[Dict] = None, 
                           zap_results: List[Dict] = None, 
                           nuclei_results: List[Dict] = None,
                           openvas_results: List[Dict] = None, 
                           manual_results: List[Dict] = None,
                           target: str = None) -> Dict:
        """
        Correlate findings from multiple scanners
        Returns a structured result with correlated findings and statistics
        """
        try:
            # Initialize empty lists if not provided
            internal_results = internal_results or []
            zap_results = zap_results or []
            nuclei_results = nuclei_results or []
            openvas_results = openvas_results or []
            manual_results = manual_results or []
            
            # Normalize findings from each scanner
            normalized_findings = []
            
            # Process results from each scanner
            for finding in internal_results:
                normalized = self._normalize_internal_finding(finding)
                normalized_findings.append(normalized)

            for finding in zap_results:
                normalized = self._normalize_zap_finding(finding)
                normalized_findings.append(normalized)
                
            for finding in nuclei_results:
                normalized = self._normalize_nuclei_finding(finding)
                normalized_findings.append(normalized)

            for finding in openvas_results:
                normalized = self._normalize_openvas_finding(finding)
                normalized_findings.append(normalized)
                
            for finding in manual_results:
                normalized = self._normalize_manual_finding(finding)
                normalized_findings.append(normalized)

            # Apply correlation to identify related findings
            self.logger.info(f"Correlating {len(normalized_findings)} findings from multiple scanners")
            correlated_findings = self._correlate_findings(normalized_findings)
            
            # Save correlated findings if target is provided
            saved_findings = []
            if target:
                # Normalize target before saving
                normalized_target = self._normalize_target(target)
                saved_findings = self._save_findings(correlated_findings, normalized_target)
            
            # Generate correlation statistics
            stats = self._generate_correlation_stats(normalized_findings, correlated_findings)
            
            return {
                'status': 'success',
                'original_count': len(normalized_findings),
                'correlated_count': len(correlated_findings),
                'findings': saved_findings if target else correlated_findings,
                'statistics': stats,
                'correlation_timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            self.logger.error(f"Error correlating findings: {str(e)}")
            return {
                'status': 'error',
                'error': str(e)
            }

    def _normalize_internal_finding(self, finding: Dict) -> Dict:
        """Normalize internal scanner findings"""
        # Get and normalize target from finding
        target = finding.get('target', '')
        normalized_target = self._normalize_target(target)
        
        return {
            'name': finding.get('name', 'Unknown Finding'),
            'description': finding.get('description', ''),
            'severity': finding.get('severity', 'LOW'),
            'evidence': finding.get('evidence', ''),
            'source': 'internal',
            'confidence': finding.get('confidence', 'medium'),
            'type': finding.get('type', 'unknown'),
            'references': finding.get('references', []),
            'cve': self._extract_cve_references(finding.get('references', [])),
            'cvss_score': finding.get('cvss', 0.0),
            'cwe': finding.get('cwe', ''),
            'fingerprint': self._generate_finding_fingerprint(finding),
            'target': normalized_target,  # Add normalized target
            'metadata': {
                'original_source': 'internal_scanner',
                'original_id': finding.get('id'),
                'scan_date': datetime.now().isoformat(),
                'original_target': target  # Store original target
            }
        }

    def _normalize_zap_finding(self, finding: Dict) -> Dict:
        """Normalize ZAP findings"""
        name = finding.get('name', 'Unknown ZAP Finding')
        description = finding.get('description', '')
        
        # Extract any CVEs mentioned in the description or alerts
        cves = self._extract_cve_from_text(description)
        if 'alertItems' in finding:
            for item in finding['alertItems']:
                item_desc = item.get('description', '')
                cves.update(self._extract_cve_from_text(item_desc))
        
        refs = finding.get('references', [])
        if isinstance(refs, str):
            refs = [refs]
        
        # Extract and normalize target from URL
        url = finding.get('url', '')
        normalized_target = self._normalize_target(url)
        
        return {
            'name': name,
            'description': description,
            'severity': self._map_zap_severity(finding.get('risk')),
            'evidence': finding.get('evidence', ''),
            'source': 'zap',
            'confidence': self._map_zap_confidence(finding.get('confidence', '')),
            'type': 'web',
            'references': refs,
            'cve': list(cves),
            'cwe': str(finding.get('cweid', '')),
            'cvss_score': self._calculate_cvss_from_severity(self._map_zap_severity(finding.get('risk'))),
            'fingerprint': self._generate_finding_fingerprint(finding),
            'target': normalized_target,  # Add normalized target
            'metadata': {
                'original_source': 'zap',
                'original_id': finding.get('id'),
                'url': url,
                'parameter': finding.get('parameter', ''),
                'attack': finding.get('attack', ''),
                'wascid': finding.get('wascid', ''),
                'solution': finding.get('solution', ''),
                'scan_date': datetime.now().isoformat()
            }
        }

    def _normalize_nuclei_finding(self, finding: Dict) -> Dict:
        """Normalize Nuclei findings"""
        # Extract CVEs from template ID or name
        cves = set()
        template_id = finding.get('template_id', '')
        name = finding.get('name', '')
        
        # Check if template_id or name contains CVE reference
        cves.update(self._extract_cve_from_text(template_id))
        cves.update(self._extract_cve_from_text(name))
        
        # Add any CVEs from references
        if finding.get('references'):
            for ref in finding['references']:
                cves.update(self._extract_cve_from_text(ref))
        
        # Extract and normalize target from host field
        host = finding.get('host', '')
        normalized_target = self._normalize_target(host)
        
        return {
            'name': finding.get('name', 'Unknown Nuclei Finding'),
            'description': finding.get('description', ''),
            'severity': finding.get('severity', 'LOW'),
            'evidence': finding.get('evidence', finding.get('matched', '')),
            'source': 'nuclei',
            'confidence': 'high',  # Nuclei typically has high confidence due to specific template matching
            'type': finding.get('type', 'nuclei'),
            'references': finding.get('references', []),
            'cve': list(cves),
            'cwe': finding.get('cwe', ''),
            'cvss_score': finding.get('cvss_score', 0.0),
            'fingerprint': self._generate_finding_fingerprint(finding),
            'target': normalized_target,  # Add normalized target
            'metadata': {
                'original_source': 'nuclei',
                'template_id': finding.get('template_id', ''),
                'matched_at': finding.get('matched', ''),
                'host': host,
                'tags': finding.get('tags', []),
                'scan_date': datetime.now().isoformat()
            }
        }

    def _normalize_openvas_finding(self, finding: Dict) -> Dict:
        """Normalize OpenVAS findings"""
        cves = set()
        
        # Extract CVEs from direct field or references
        if 'cve' in finding:
            if isinstance(finding['cve'], list):
                for cve in finding['cve']:
                    cves.update(self._extract_cve_from_text(cve))
            else:
                cves.update(self._extract_cve_from_text(finding['cve']))
        
        # Extract and normalize target from host field
        host = finding.get('host', '')
        normalized_target = self._normalize_target(host)
                
        return {
            'name': finding.get('name', 'Unknown OpenVAS Finding'),
            'description': finding.get('description', ''),
            'severity': self._map_openvas_severity(finding.get('severity')),
            'evidence': f"Port: {finding.get('port')} - Host: {host}",
            'source': 'openvas',
            'confidence': 'high',
            'type': 'network',
            'references': finding.get('references', []),
            'cve': list(cves),
            'cwe': finding.get('cwe', ''),
            'cvss_score': float(finding.get('cvss', 0.0)) if finding.get('cvss') else 0.0,
            'fingerprint': self._generate_finding_fingerprint(finding),
            'target': normalized_target,  # Add normalized target
            'metadata': {
                'original_source': 'openvas',
                'solution': finding.get('solution'),
                'port': finding.get('port'),
                'scan_date': datetime.now().isoformat()
            }
        }
        
    def _normalize_manual_finding(self, finding: Dict) -> Dict:
        """Normalize manually entered findings"""
        # Extract and normalize target
        target = finding.get('target', '')
        normalized_target = self._normalize_target(target)
        
        return {
            'name': finding.get('name', 'Manual Finding'),
            'description': finding.get('description', ''),
            'severity': finding.get('severity', 'LOW'),
            'evidence': finding.get('evidence', ''),
            'source': 'manual',
            'confidence': finding.get('confidence', 'high'),
            'type': finding.get('type', 'manual'),
            'references': finding.get('references', []),
            'cve': self._extract_cve_from_text(finding.get('description', '')),
            'cwe': finding.get('cwe', ''),
            'cvss_score': finding.get('cvss_score', 0.0),
            'fingerprint': self._generate_finding_fingerprint(finding),
            'target': normalized_target,  # Add normalized target
            'metadata': {
                'original_source': 'manual',
                'author': finding.get('author', 'Unknown'),
                'notes': finding.get('notes', ''),
                'scan_date': datetime.now().isoformat(),
                'original_target': target  # Store original target
            }
        }

    #
    # Correlation methods
    #
    
    def _correlate_findings(self, findings: List[Dict]) -> List[Dict]:
        """
        Apply correlation rules to findings to identify related issues
        """
        if not findings:
            return []
            
        self.logger.info(f"Starting correlation of {len(findings)} findings")
        
        # Create groups of correlated findings
        groups = []
        processed = set()
        
        for i, finding in enumerate(findings):
            if i in processed:
                continue
                
            # Create a new group with this finding
            group = [finding]
            processed.add(i)
            
            # Check all other findings for correlation
            for j, other in enumerate(findings):
                if j in processed or i == j:
                    continue
                    
                # Apply correlation rules
                if self._are_findings_related(finding, other):
                    group.append(other)
                    processed.add(j)
            
            groups.append(group)
        
        # Merge findings in each group
        correlated_findings = []
        for group in groups:
            if len(group) == 1:
                correlated_findings.append(group[0])
            else:
                merged = self._merge_findings(group)
                correlated_findings.append(merged)
                
        self.logger.info(f"Correlation complete. Reduced {len(findings)} findings to {len(correlated_findings)}")
        return correlated_findings

    def _are_findings_related(self, finding1: Dict, finding2: Dict) -> bool:
        """
        Check if two findings are related using multiple correlation rules
        Returns True if any correlation rule matches
        """
        for rule in self.correlation_rules:
            if rule(finding1, finding2):
                return True
        return False

    def _correlation_by_target_and_name(self, finding1: Dict, finding2: Dict) -> bool:
        """Correlation rule: same normalized target and name"""
        if not (finding1.get('target') and finding2.get('target') and 
                finding1.get('name') and finding2.get('name')):
            return False
            
        # Check if targets match using normalized form
        # They should already be normalized during the normalization step
        norm_target1 = finding1['target']
        norm_target2 = finding2['target']
        
        # If same normalized target and similar name, they're likely duplicates
        if norm_target1 == norm_target2:
            name1 = self._normalize_text(finding1['name'])
            name2 = self._normalize_text(finding2['name'])
            
            # Check for exact name match or high similarity
            if name1 == name2:
                return True
            
            # Check similarity for close matches
            name_similarity = self._calculate_similarity(name1, name2)
            if name_similarity >= 0.9:  # Higher threshold for name-only matching
                return True
                
        return False

    def _correlation_by_name_and_type(self, finding1: Dict, finding2: Dict) -> bool:
        """Correlation rule: same name and vulnerability type"""
        if finding1.get('name') and finding2.get('name') and finding1.get('type') and finding2.get('type'):
            # Normalize names for comparison
            name1 = self._normalize_text(finding1['name'])
            name2 = self._normalize_text(finding2['name'])
            return name1 == name2 and finding1['type'] == finding2['type']
        return False

    def _correlation_by_fingerprint(self, finding1: Dict, finding2: Dict) -> bool:
        """Correlation rule: match by vulnerability fingerprint"""
        if finding1.get('fingerprint') and finding2.get('fingerprint'):
            return finding1['fingerprint'] == finding2['fingerprint']
        return False

    def _correlation_by_cve(self, finding1: Dict, finding2: Dict) -> bool:
        """Correlation rule: match by CVE references"""
        cves1 = set(finding1.get('cve', []))
        cves2 = set(finding2.get('cve', []))
        
        # If both have CVEs and there's an overlap, they're related
        return bool(cves1 and cves2 and cves1.intersection(cves2))

    def _correlation_by_similarity(self, finding1: Dict, finding2: Dict) -> bool:
        """Correlation rule: match by content similarity"""
        # Compare titles for similarity
        name1 = finding1.get('name', '')
        name2 = finding2.get('name', '')
        
        if name1 and name2:
            name_similarity = self._calculate_similarity(name1, name2)
            if name_similarity >= self.title_similarity_threshold:
                return True
                
        # Compare descriptions for similarity
        desc1 = finding1.get('description', '')
        desc2 = finding2.get('description', '')
        
        if desc1 and desc2:
            desc_similarity = self._calculate_similarity(desc1, desc2)
            if desc_similarity >= self.description_similarity_threshold:
                return True
                
        return False

    def _merge_findings(self, findings: List[Dict]) -> Dict:
        """
        Merge a group of related findings into a single comprehensive finding
        """
        if not findings:
            return {}
            
        if len(findings) == 1:
            return findings[0]
            
        # Start with the highest severity finding as the base
        findings.sort(key=lambda x: self._severity_to_score(x.get('severity', 'LOW')), reverse=True)
        base = findings[0].copy()
        
        # Track merged sources
        sources = set([base['source']])
        references = set(base.get('references', []))
        cves = set(base.get('cve', []))
        
        # Create a comprehensive description
        descriptions = [f"{base['source']}: {base['description']}"]
        evidences = [f"{base['source']}: {base['evidence']}"]
        
        # Merge additional findings
        for finding in findings[1:]:
            sources.add(finding['source'])
            
            # Add references
            for ref in finding.get('references', []):
                references.add(ref)
                
            # Add CVEs
            for cve in finding.get('cve', []):
                cves.add(cve)
                
            # Add description and evidence
            if finding.get('description'):
                descriptions.append(f"{finding['source']}: {finding['description']}")
            
            if finding.get('evidence'):
                evidences.append(f"{finding['source']}: {finding['evidence']}")
                
            # Merge metadata
            for key, value in finding.get('metadata', {}).items():
                if key not in base['metadata']:
                    base['metadata'][key] = value
                    
        # Update merged finding
        base['source'] = ','.join(sources)
        base['references'] = list(references)
        base['cve'] = list(cves)
        base['description'] = '\n\n'.join(descriptions)
        base['evidence'] = '\n\n'.join(evidences)
        
        # Set confidence based on number of sources
        base['confidence'] = 'high' if len(sources) > 1 else base['confidence']
        
        # Calculate highest CVSS score
        base['cvss_score'] = max([f.get('cvss_score', 0.0) for f in findings])
        
        # Add correlation metadata
        base['metadata']['correlated'] = True
        base['metadata']['correlated_count'] = len(findings)
        base['metadata']['correlated_sources'] = list(sources)
        
        return base

    #
    # Helper methods
    #
    
    def _generate_finding_fingerprint(self, finding: Dict) -> str:
        """
        Generate a unique fingerprint for a finding to aid in correlation
        """
        # Extract key attributes for fingerprinting
        name = finding.get('name', '')
        desc = finding.get('description', '')
        
        # Add target to fingerprint to avoid false matches across different targets
        target = finding.get('target', '')
        
        # Create a string representation
        fingerprint_str = f"{target}:{name}:{desc}"
        
        # For specific scanner types, add additional identifiers
        source = finding.get('source', '')
        if source == 'zap' and 'alertItems' in finding:
            fingerprint_str += f":{finding.get('cweid', '')}"
        elif source == 'nuclei':
            fingerprint_str += f":{finding.get('template_id', '')}"
        
        # Generate SHA256 hash
        return hashlib.sha256(fingerprint_str.encode()).hexdigest()

    def _extract_cve_from_text(self, text: str) -> Set[str]:
        """
        Extract CVE IDs from text using regex
        """
        if not text:
            return set()
            
        # CVE pattern: CVE-YYYY-NNNNN (YYYY is year, NNNNN is ID number)
        cve_pattern = r'CVE-\d{4}-\d{4,7}'
        
        # Find all matches
        cves = re.findall(cve_pattern, text, re.IGNORECASE)
        return set(cve.upper() for cve in cves)

    def _extract_cve_references(self, references: List[str]) -> List[str]:
        """
        Extract CVE IDs from reference URLs
        """
        cves = set()
        for ref in references:
            cves.update(self._extract_cve_from_text(ref))
        return list(cves)

    def _normalize_text(self, text: str) -> str:
        """
        Normalize text for more accurate comparison:
        - Convert to lowercase
        - Remove punctuation
        - Remove common words
        """
        if not text:
            return ""
            
        # Convert to lowercase
        text = text.lower()
        
        # Remove punctuation
        text = re.sub(r'[^\w\s]', '', text)
        
        # Remove common words
        stop_words = {'a', 'an', 'the', 'in', 'on', 'at', 'of', 'for', 'with', 'by'}
        words = text.split()
        filtered_words = [word for word in words if word not in stop_words]
        
        return ' '.join(filtered_words)

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """
        Calculate similarity between two strings using Jaccard similarity
        Returns value between 0.0 and 1.0
        """
        if not text1 or not text2:
            return 0.0
            
        # Normalize texts
        text1 = self._normalize_text(text1)
        text2 = self._normalize_text(text2)
        
        # Split into words
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        # Calculate Jaccard similarity: intersection / union
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        if union == 0:
            return 0.0
            
        return intersection / union

    def _severity_to_score(self, severity: str) -> int:
        """
        Convert severity string to numeric score for comparison
        """
        severity = severity.upper()
        scores = {
            'CRITICAL': 4,
            'HIGH': 3,
            'MEDIUM': 2,
            'LOW': 1,
            'INFO': 0
        }
        return scores.get(severity, 0)

    def _map_zap_severity(self, severity: str) -> str:
        """Map ZAP severity to standard severity"""
        if not severity:
            return 'LOW'
            
        mapping = {
            'Informational': 'INFO',
            'Low': 'LOW',
            'Medium': 'MEDIUM',
            'High': 'HIGH',
            'Critical': 'CRITICAL',
            '0': 'INFO',
            '1': 'LOW',
            '2': 'MEDIUM',
            '3': 'HIGH',
            '4': 'CRITICAL'
        }
        return mapping.get(severity, 'LOW')

    def _map_zap_confidence(self, confidence: str) -> str:
        """Map ZAP confidence to standard confidence"""
        if not confidence:
            return 'medium'
            
        mapping = {
            'False Positive': 'low',
            'Low': 'low',
            'Medium': 'medium',
            'High': 'high',
            'Confirmed': 'high',
            '0': 'low',
            '1': 'low',
            '2': 'medium',
            '3': 'high',
            '4': 'high'
        }
        return mapping.get(confidence, 'medium')

    def _map_openvas_severity(self, severity: str) -> str:
        """Map OpenVAS severity to standard severity"""
        try:
            severity_float = float(severity)
            if severity_float >= 9.0:
                return 'CRITICAL'
            elif severity_float >= 7.0:
                return 'HIGH'
            elif severity_float >= 4.0:
                return 'MEDIUM'
            elif severity_float > 0:
                return 'LOW'
            return 'INFO'
        except (ValueError, TypeError):
            return 'LOW'

    def _calculate_cvss_from_severity(self, severity: str) -> float:
        """Calculate estimated CVSS score from severity string"""
        severity = severity.upper()
        cvss_mapping = {
            'CRITICAL': 9.5,
            'HIGH': 7.5,
            'MEDIUM': 5.0,
            'LOW': 2.5,
            'INFO': 0.0
        }
        return cvss_mapping.get(severity, 0.0)

    def _generate_correlation_stats(self, original_findings: List[Dict], 
                                   correlated_findings: List[Dict]) -> Dict:
        """
        Generate statistics about the correlation process
        """
        # Count by source
        original_by_source = {}
        for finding in original_findings:
            source = finding.get('source', 'unknown')
            original_by_source[source] = original_by_source.get(source, 0) + 1
            
        # Count by severity
        severity_distribution = {
            'CRITICAL': 0,
            'HIGH': 0,
            'MEDIUM': 0,
            'LOW': 0,
            'INFO': 0
        }
        
        correlation_groups = []
        for finding in correlated_findings:
            # Update severity count
            severity = finding.get('severity', 'LOW').upper()
            if severity in severity_distribution:
                severity_distribution[severity] += 1
                
            # Track correlation groups
            if ',' in finding.get('source', ''):
                sources = finding.get('source', '').split(',')
                correlation_groups.append({
                    'name': finding.get('name', 'Unknown'),
                    'sources': sources,
                    'severity': severity
                })
        
        return {
            'original_count': len(original_findings),
            'correlated_count': len(correlated_findings),
            'reduction_percentage': round((1 - len(correlated_findings) / len(original_findings)) * 100, 2) if original_findings else 0,
            'original_by_source': original_by_source,
            'severity_distribution': severity_distribution,
            'correlation_groups': correlation_groups[:10]  # Limit to top 10 groups
        }

    @transaction.atomic
    def _save_findings(self, findings: List[Dict], target: str) -> List[Dict]:
        """
        Save correlated findings to database with transaction support
        """
        saved_findings = []
        
        # Ensure target is normalized
        normalized_target = self._normalize_target(target)
        
        for finding in findings:
            try:
                # Check if this vulnerability already exists
                existing_vulns = Vulnerability.objects.filter(
                    target=normalized_target,
                    name=finding['name'],
                    vuln_type=finding.get('type', 'unknown'),
                    severity=finding['severity']
                )
                
                if existing_vulns.exists():
                    # Update the existing vulnerability
                    vuln = existing_vulns.first()
                    
                    # Combine sources
                    sources = set(vuln.source.split(','))
                    sources.add(finding['source'])
                    vuln.source = ','.join(sorted(sources))
                    
                    # Update other fields if needed
                    if finding.get('cvss_score', 0) > (vuln.cvss_score or 0):
                        vuln.cvss_score = finding['cvss_score']
                    
                    # Update metadata
                    if not vuln.metadata:
                        vuln.metadata = {}
                    vuln.metadata.update({
                        **finding.get('metadata', {}),
                        'cve': finding.get('cve', []),
                        'updated_at': datetime.now().isoformat(),
                        'has_updates': True
                    })
                    
                    vuln.save()
                    saved_findings.append(self._serialize_vulnerability(vuln))
                else:
                    # Create a new vulnerability
                    vulnerability = Vulnerability.objects.create(
                        target=normalized_target,  # Use normalized target
                        name=finding['name'],
                        description=finding['description'],
                        severity=finding['severity'],
                        vuln_type=finding.get('type', 'unknown'),
                        evidence=finding['evidence'],
                        source=finding['source'],
                        confidence=finding['confidence'],
                        references=finding.get('references', []),
                        solution=finding.get('metadata', {}).get('solution', ''),
                        cwe=finding.get('cwe', ''),
                        cvss_score=finding.get('cvss_score', 0.0),
                        metadata={
                            **finding.get('metadata', {}),
                            'cve': finding.get('cve', []),
                            'original_target': finding.get('metadata', {}).get('original_target', target)
                        }
                    )
                    saved_findings.append(self._serialize_vulnerability(vulnerability))
            except Exception as e:
                self.logger.error(f"Error saving vulnerability: {str(e)}")
        
        return saved_findings

    def _serialize_vulnerability(self, vuln: Vulnerability) -> Dict:
        """Serialize vulnerability for response"""
        return {
            'id': vuln.id,
            'name': vuln.name,
            'description': vuln.description,
            'severity': vuln.severity,
            'type': vuln.vuln_type,
            'evidence': vuln.evidence,
            'sources': vuln.source.split(','),
            'confidence': vuln.confidence,
            'discovery_date': vuln.discovery_date.isoformat(),
            'references': vuln.references,
            'cvss_score': vuln.cvss_score,
            'cwe': vuln.cwe,
            'metadata': vuln.metadata
        }