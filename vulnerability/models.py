from django.db import models
from django.core.validators import MinValueValidator, MaxValueValidator
import logging
from django.db import transaction

logger = logging.getLogger(__name__)

class Vulnerability(models.Model):
    SEVERITY_CHOICES = [
        ('LOW', 'Low'),
        ('MEDIUM', 'Medium'),
        ('HIGH', 'High'),
        ('CRITICAL', 'Critical'),
    ]

    # Updated source choices to include more options
    SOURCE_CHOICES = [
        ('internal', 'Internal Scanner'),
        ('zap', 'OWASP ZAP'),
        ('nuclei', 'Nuclei Scanner'),
        ('openvas', 'OpenVAS Scanner'),
        ('manual', 'Manual Entry'),
        ('multiple', 'Multiple Sources')  # For correlated findings
    ]

    # Basic Information
    target = models.CharField(max_length=255, db_index=True)
    name = models.CharField(max_length=255)
    description = models.TextField()
    severity = models.CharField(max_length=10, choices=SEVERITY_CHOICES, db_index=True)
    vuln_type = models.CharField(max_length=50, db_index=True)
    
    # Evidence and Details
    evidence = models.TextField()
    solution = models.TextField(blank=True)
    references = models.JSONField(default=list)
    
    # Source and Confidence - increased max_length to accommodate combined sources
    source = models.CharField(max_length=100)  # Removed choices constraint and increased length
    confidence = models.CharField(max_length=50, default='medium')
    
    # Status and Tracking
    discovery_date = models.DateTimeField(auto_now_add=True)
    is_fixed = models.BooleanField(default=False, db_index=True)
    fix_date = models.DateTimeField(null=True, blank=True)
    notes = models.TextField(blank=True)
    
    # Additional Metadata
    cwe = models.CharField(max_length=50, blank=True)
    cvss_score = models.FloatField(
        null=True, 
        blank=True,
        validators=[MinValueValidator(0.0), MaxValueValidator(10.0)]
    )
    metadata = models.JSONField(default=dict)  # Added metadata field
    
    def __str__(self):
        return f"{self.target} - {self.name} ({self.severity})"

# In the Vulnerability model in vulnerability/models.py

# In the Vulnerability model in vulnerability/models.py

    def save(self, *args, **kwargs):
        # Extract our custom parameter before passing to Django save
        skip_deduplication = kwargs.pop('skip_deduplication', False) if 'skip_deduplication' in kwargs else False
        
        # Normalize target before saving
        self.target = self.__class__.normalize_target(self.target)
        
        # Ensure severity is uppercase
        if self.severity:
            self.severity = self.severity.upper()
        
        # Set fix_date when vulnerability is marked as fixed
        if self.is_fixed and not self.fix_date:
            from django.utils import timezone
            self.fix_date = timezone.now()
        
        # Handle source field for multiple sources
        if ',' in self.source:
            # Store original sources in metadata for reference
            if not self.metadata:
                self.metadata = {}
            self.metadata['original_sources'] = self.source.split(',')
            
        # Call save with clean kwargs
        super().save(*args, **kwargs)
        
        # Skip deduplication if explicitly requested (to avoid infinite recursion)
        # or schedule deduplication to run after save
        if not skip_deduplication:
            # Store target to use in lambda
            target = self.target
            # Use raw lambda to avoid potential reference issues
            transaction.on_commit(lambda t=target: self.__class__.deduplicate_vulnerabilities(t))

    @classmethod
    def normalize_target(cls, target_str):
        """Class method to normalize target URLs"""
        if not target_str:
            return target_str
            
        # Remove protocol prefix
        if '://' in target_str:
            target_str = target_str.split('://', 1)[1]
        
        # Remove path, trailing slash, etc.
        if '/' in target_str:
            target_str = target_str.split('/', 1)[0]
            
        # Remove port if present
        if ':' in target_str:
            target_str = target_str.split(':', 1)[0]
            
        # Remove 'www.' prefix if present
        if target_str.startswith('www.'):
            target_str = target_str[4:]
            
        return target_str.lower()

    @classmethod
    def deduplicate_vulnerabilities(cls, target):
        """
        Deduplicate vulnerabilities for a target by merging duplicates
        
        Args:
            target: The target hostname/domain
            
        Returns:
            dict: Statistics about deduplication
        """
        from django.db.models import Count
        from django.db import transaction
        
        # Use the class method to normalize target
        normalized_target = cls.normalize_target(target)
        
        # Find all targets that might be the same after normalization
        potential_targets = []
        for t in cls.objects.values_list('target', flat=True).distinct():
            if cls.normalize_target(t) == normalized_target:
                potential_targets.append(t)
        
        if not potential_targets:
            return {
                'original_count': 0,
                'merged_count': 0,
                'final_count': 0,
                'message': 'No matching targets found'
            }
        
        # Keep track of statistics
        stats = {
            'original_count': cls.objects.filter(target__in=potential_targets).count(),
            'merged_count': 0,
            'final_count': 0
        }
        
        try:
            with transaction.atomic():
                # First normalize all targets
                for pt in potential_targets:
                    if pt != normalized_target:
                        # Update target to normalized version
                        cls.objects.filter(target=pt).update(target=normalized_target)
                
                # Then deduplicate within the normalized target
                duplicate_groups = cls.objects.filter(target=normalized_target).values(
                    'name', 'vuln_type', 'severity'
                ).annotate(
                    count=Count('id')
                ).filter(count__gt=1)
                
                # Process each group
                for group in duplicate_groups:
                    duplicates = cls.objects.filter(
                        target=normalized_target,
                        name=group['name'],
                        vuln_type=group['vuln_type'],
                        severity=group['severity']
                    ).order_by('discovery_date')
                    
                    # Keep the first (oldest) vulnerability as the canonical one
                    if duplicates.count() > 1:
                        primary_vuln = duplicates.first()
                        
                        # Process other duplicates
                        for dup in duplicates[1:]:
                            # Combine sources if different
                            sources = set(primary_vuln.source.split(','))
                            for source in dup.source.split(','):
                                sources.add(source)
                            primary_vuln.source = ','.join(sorted(sources))
                            
                            # Use the higher CVSS score if available
                            if dup.cvss_score and (not primary_vuln.cvss_score or dup.cvss_score > primary_vuln.cvss_score):
                                primary_vuln.cvss_score = dup.cvss_score
                            
                            # Take the most recent evidence if available
                            if dup.evidence and len(dup.evidence) > len(primary_vuln.evidence):
                                primary_vuln.evidence = dup.evidence
                                
                            # Merge references
                            primary_refs = set(primary_vuln.references)
                            for ref in dup.references:
                                primary_refs.add(ref)
                            primary_vuln.references = list(primary_refs)
                            
                            # Track the merged duplicate in metadata
                            if not primary_vuln.metadata:
                                primary_vuln.metadata = {}
                            if 'merged_duplicates' not in primary_vuln.metadata:
                                primary_vuln.metadata['merged_duplicates'] = []
                            primary_vuln.metadata['merged_duplicates'].append({
                                'id': dup.id,
                                'discovery_date': dup.discovery_date.isoformat(),
                                'source': dup.source
                            })
                            
                            # Delete the duplicate
                            dup.delete()
                            stats['merged_count'] += 1
                        
                        # Save the updated primary vulnerability with skip_deduplication
                        # to avoid infinite recursion
                        primary_vuln.save(skip_deduplication=True)
            
            # Calculate final counts
            stats['final_count'] = cls.objects.filter(target=normalized_target).count()
            return stats
            
        except Exception as e:
            logger.error(f"Error deduplicating vulnerabilities: {str(e)}")
            return {'error': str(e)}    

    @classmethod
    @transaction.atomic
    def remove_duplicates(cls, target=None):
        """
        Remove duplicate vulnerabilities for a target
        
        Args:
            target: Optional target to limit deduplication
            
        Returns:
            int: Number of vulnerabilities removed
        """
        from django.db.models import Count
        
        try:
            # Build query based on if target is provided
            if target:
                # Normalize target first
                normalized_target = cls.normalize_target(target)
                # Find duplicates for specific target
                duplicates = cls.objects.filter(target=normalized_target)
            else:
                # Find duplicates across all targets
                duplicates = cls.objects.all()
            
            # Group by name and target to find duplicates
            duplicate_groups = duplicates.values('name', 'target').annotate(
                count=Count('id')
            ).filter(count__gt=1)
            
            removed_count = 0
            
            # Process each duplicate group
            for group in duplicate_groups:
                # Get all duplicates in this group, ordered by ID (oldest first)
                dupes = cls.objects.filter(
                    name=group['name'],
                    target=group['target']
                ).order_by('id')
                
                # Keep the first one, delete the rest
                keep = dupes.first()
                to_delete = dupes.exclude(id=keep.id)
                
                # Count how many we're deleting
                delete_count = to_delete.count()
                
                # Delete duplicates
                to_delete.delete()
                removed_count += delete_count
            
            return removed_count
            
        except Exception as e:
            logger.error(f"Error removing duplicates: {str(e)}")
            return 0
    
    @property
    def age_in_days(self):
        from django.utils import timezone
        return (timezone.now() - self.discovery_date).days
    
    # Add this method to the Vulnerability model


    @property
    def risk_score(self):
        """Calculate risk score based on CVSS and age"""
        base_score = self.cvss_score if self.cvss_score else {
            'CRITICAL': 9.0,
            'HIGH': 7.0,
            'MEDIUM': 5.0,
            'LOW': 3.0
        }.get(self.severity, 1.0)
        
        # Age factor: 1.0 - 2.0 based on age (caps at 90 days)
        age_factor = min(1 + (self.age_in_days / 90), 2.0)
        
        return base_score * age_factor
    
    @property
    def source_list(self):
        """Return the source as a list for easier filtering"""
        return self.source.split(',')

class NucleiFinding(models.Model):
    SEVERITY_CHOICES = [
        ('CRITICAL', 'Critical'),
        ('HIGH', 'High'),
        ('MEDIUM', 'Medium'),
        ('LOW', 'Low'),
        ('INFO', 'Info'),
    ]

    template_id = models.CharField(max_length=255)
    name = models.CharField(max_length=255)
    severity = models.CharField(max_length=10, choices=SEVERITY_CHOICES)
    finding_type = models.CharField(max_length=50)
    host = models.CharField(max_length=255)
    matched_at = models.URLField(max_length=500)
    description = models.TextField()
    tags = models.JSONField(default=list)
    references = models.JSONField(default=list)
    cwe = models.CharField(max_length=50, blank=True, null=True)
    cvss_score = models.FloatField(null=True, blank=True)
    discovery_date = models.DateTimeField(auto_now_add=True)
    scan_id = models.CharField(max_length=100)
    target = models.CharField(max_length=255)
    
    class Meta:
        indexes = [
            models.Index(fields=['template_id']),
            models.Index(fields=['severity']),
            models.Index(fields=['discovery_date']),
            models.Index(fields=['target']),
        ]